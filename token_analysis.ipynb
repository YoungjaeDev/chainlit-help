{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Chainlit Help - 프롬프트 토큰 분석\n",
        "\n",
        "현재 프로젝트에서 사용하는 프롬프트의 토큰 수를 계산합니다.\n",
        "\n",
        "참고: [Gemini API 토큰 계산 문서](https://ai.google.dev/gemini-api/docs/tokens?authuser=1&hl=ko&lang=python)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google AI 클라이언트 초기화 완료\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from google import genai\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 환경변수 로드\n",
        "load_dotenv('app/.env')\n",
        "\n",
        "# Google AI API 키 설정\n",
        "api_key = os.getenv('GOOGLE_API_KEY')\n",
        "if not api_key:\n",
        "    print(\"GOOGLE_API_KEY가 설정되지 않았습니다. app/.env 파일을 확인해주세요.\")\n",
        "else:\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    print(\"Google AI 클라이언트 초기화 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "codebase: 62,648 문자\n",
            "documentation: 257,654 문자\n",
            "cookbook: 44,054 문자\n"
          ]
        }
      ],
      "source": [
        "# 컨텍스트 파일들 로드\n",
        "def load_context_files():\n",
        "    context_files = {\n",
        "        'codebase': 'app/context/codebase.txt',\n",
        "        'documentation': 'app/context/documentation.txt', \n",
        "        'cookbook': 'app/context/cookbook.txt'\n",
        "    }\n",
        "    \n",
        "    context_content = {}\n",
        "    \n",
        "    for name, path in context_files.items():\n",
        "        try:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                context_content[name] = content\n",
        "                print(f\"{name}: {len(content):,} 문자\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️ {path} 파일을 찾을 수 없습니다.\")\n",
        "            context_content[name] = \"\"\n",
        "    \n",
        "    return context_content\n",
        "\n",
        "context_content = load_context_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG 프롬프트 설정:\n",
            "모델: gemini-2.0-flash\n",
            "온도: 0.2\n",
            "\n",
            "시스템 프롬프트 템플릿:\n",
            "You are a helpful assistant who answers questions related to Chainlit in valid markdown. Keep it short, and if available prefer responding with code. For documentation links, always use fully formed u...\n"
          ]
        }
      ],
      "source": [
        "# RAG 프롬프트 템플릿 로드\n",
        "with open('app/prompts/rag.json', 'r', encoding='utf-8') as f:\n",
        "    rag_prompt = json.load(f)\n",
        "\n",
        "print(\"RAG 프롬프트 설정:\")\n",
        "print(f\"모델: {rag_prompt['settings']['model']}\")\n",
        "print(f\"온도: {rag_prompt['settings']['temperature']}\")\n",
        "print(\"\\n시스템 프롬프트 템플릿:\")\n",
        "print(rag_prompt['template_messages'][0]['content'][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "langchain_template = rag_prompt['template_messages'][0]['content'].replace('{{', '{').replace('}}', '}')\n",
        "\n",
        "# 실제 시스템 프롬프트 구성 (app.py와 동일하게)\n",
        "system_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"codebase\", \"documentation\", \"cookbook\"],\n",
        "    template=langchain_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['codebase', 'cookbook', 'documentation'], input_types={}, partial_variables={}, template='You are a helpful assistant who answers questions related to Chainlit in valid markdown. Keep it short, and if available prefer responding with code. For documentation links, always use fully formed urls using this domain: `https://docs.chainlit.io`. Do NOT create APIs, methods or anything else outside of provided context. NEVER hallucinate or extrapolate on the context.\\n\\nContext:\\n<codebase>\\n{codebase}\\n</codebase>\\n<documentation>\\n{documentation}\\n</documentation>\\n<cookbook>\\n{cookbook}\\n</cookbook>\\n\\n')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "system_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 템플릿에 컨텍스트 내용 삽입\n",
        "formatted_system_prompt = system_prompt_template.format_prompt(\n",
        "    codebase=context_content['codebase'],\n",
        "    documentation=context_content['documentation'],\n",
        "    cookbook=context_content['cookbook']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text='You are a helpful assistant who answers questions related to Chainlit in valid markdown. Keep it short, and if available prefer responding with code. For documentation links, always use fully formed urls using this domain: `https://docs.chainlit.io`. Do NOT create APIs, methods or anything else outside of provided context. NEVER hallucinate or extrapolate on the context.\\n\\nContext:\\n<codebase>\\nDirectory structure:\\n└── chainlit-chainlit/\\n    └── libs/\\n        └── react-client/\\n            └── src/\\n                ├── context.ts\\n                ├── index.ts\\n                ├── state.ts\\n                ├── useAudio.ts\\n                ├── useChatData.ts\\n                ├── useChatInteract.ts\\n                ├── useChatMessages.ts\\n                ├── useChatSession.ts\\n                ├── useConfig.ts\\n                ├── api/\\n                │   ├── index.tsx\\n                │   └── hooks/\\n                │       ├── api.ts\\n                │       └── auth/\\n                │           ├── config.ts\\n                │           ├── index.ts\\n                │           ├── sessionManagement.ts\\n                │           ├── state.ts\\n                │           ├── types.ts\\n                │           └── userManagement.ts\\n                ├── types/\\n                │   ├── action.ts\\n                │   ├── audio.ts\\n                │   ├── command.ts\\n                │   ├── config.ts\\n                │   ├── element.ts\\n                │   ├── feedback.ts\\n                │   ├── file.ts\\n                │   ├── history.ts\\n                │   ├── index.ts\\n                │   ├── mcp.ts\\n                │   ├── step.ts\\n                │   ├── thread.ts\\n                │   └── user.ts\\n                └── utils/\\n                    ├── group.ts\\n                    └── message.ts\\n\\n\\nFiles Content:\\n\\n================================================\\nFile: libs/react-client/src/context.ts\\n================================================\\nimport { createContext } from \\'react\\';\\n\\nimport { ChainlitAPI } from \\'./api\\';\\n\\nconst defaultChainlitContext = undefined;\\n\\nconst ChainlitContext = createContext<ChainlitAPI>(\\n  new ChainlitAPI(\\'http://localhost:8000\\', \\'webapp\\')\\n);\\n\\nexport { ChainlitContext, defaultChainlitContext };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/index.ts\\n================================================\\nexport * from \\'./useChatData\\';\\nexport * from \\'./useChatInteract\\';\\nexport * from \\'./useChatMessages\\';\\nexport * from \\'./useChatSession\\';\\nexport * from \\'./useAudio\\';\\nexport * from \\'./useConfig\\';\\nexport * from \\'./api\\';\\nexport * from \\'./types\\';\\nexport * from \\'./context\\';\\nexport * from \\'./state\\';\\nexport * from \\'./utils/message\\';\\n\\nexport { Socket } from \\'socket.io-client\\';\\n\\nexport { WavRenderer } from \\'./wavtools/wav_renderer\\';\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/state.ts\\n================================================\\nimport { isEqual } from \\'lodash\\';\\nimport { AtomEffect, DefaultValue, atom, selector } from \\'recoil\\';\\nimport { Socket } from \\'socket.io-client\\';\\nimport { v4 as uuidv4 } from \\'uuid\\';\\n\\nimport { ICommand } from \\'./types/command\\';\\n\\nimport {\\n  IAction,\\n  IAsk,\\n  IAuthConfig,\\n  ICallFn,\\n  IChainlitConfig,\\n  IMcp,\\n  IMessageElement,\\n  IStep,\\n  ITasklistElement,\\n  IUser,\\n  ThreadHistory\\n} from \\'./types\\';\\nimport { groupByDate } from \\'./utils/group\\';\\nimport { WavRecorder, WavStreamPlayer } from \\'./wavtools\\';\\n\\nexport interface ISession {\\n  socket: Socket;\\n  error?: boolean;\\n}\\n\\nexport const threadIdToResumeState = atom<string | undefined>({\\n  key: \\'ThreadIdToResume\\',\\n  default: undefined\\n});\\n\\nexport const resumeThreadErrorState = atom<string | undefined>({\\n  key: \\'ResumeThreadErrorState\\',\\n  default: undefined\\n});\\n\\nexport const chatProfileState = atom<string | undefined>({\\n  key: \\'ChatProfile\\',\\n  default: undefined\\n});\\n\\nconst sessionIdAtom = atom<string>({\\n  key: \\'SessionId\\',\\n  default: uuidv4()\\n});\\n\\nexport const sessionIdState = selector({\\n  key: \\'SessionIdSelector\\',\\n  get: ({ get }) => get(sessionIdAtom),\\n  set: ({ set }, newValue) =>\\n    set(sessionIdAtom, newValue instanceof DefaultValue ? uuidv4() : newValue)\\n});\\n\\nexport const sessionState = atom<ISession | undefined>({\\n  key: \\'Session\\',\\n  dangerouslyAllowMutability: true,\\n  default: undefined\\n});\\n\\nexport const actionState = atom<IAction[]>({\\n  key: \\'Actions\\',\\n  default: []\\n});\\n\\nexport const messagesState = atom<IStep[]>({\\n  key: \\'Messages\\',\\n  dangerouslyAllowMutability: true,\\n  default: []\\n});\\n\\nexport const commandsState = atom<ICommand[]>({\\n  key: \\'Commands\\',\\n  default: []\\n});\\n\\nexport const tokenCountState = atom<number>({\\n  key: \\'TokenCount\\',\\n  default: 0\\n});\\n\\nexport const loadingState = atom<boolean>({\\n  key: \\'Loading\\',\\n  default: false\\n});\\n\\nexport const askUserState = atom<IAsk | undefined>({\\n  key: \\'AskUser\\',\\n  default: undefined\\n});\\n\\nexport const wavRecorderState = atom({\\n  key: \\'WavRecorder\\',\\n  dangerouslyAllowMutability: true,\\n  default: new WavRecorder()\\n});\\n\\nexport const wavStreamPlayerState = atom({\\n  key: \\'WavStreamPlayer\\',\\n  dangerouslyAllowMutability: true,\\n  default: new WavStreamPlayer()\\n});\\n\\nexport const audioConnectionState = atom<\\'connecting\\' | \\'on\\' | \\'off\\'>({\\n  key: \\'AudioConnection\\',\\n  default: \\'off\\'\\n});\\n\\nexport const isAiSpeakingState = atom({\\n  key: \\'isAiSpeaking\\',\\n  default: false\\n});\\n\\nexport const callFnState = atom<ICallFn | undefined>({\\n  key: \\'CallFn\\',\\n  default: undefined\\n});\\n\\nexport const chatSettingsInputsState = atom<any>({\\n  key: \\'ChatSettings\\',\\n  default: []\\n});\\n\\nexport const chatSettingsDefaultValueSelector = selector({\\n  key: \\'ChatSettingsValue/Default\\',\\n  get: ({ get }) => {\\n    const chatSettings = get(chatSettingsInputsState);\\n    return chatSettings.reduce(\\n      (form: { [key: string]: any }, input: any) => (\\n        (form[input.id] = input.initial), form\\n      ),\\n      {}\\n    );\\n  }\\n});\\n\\nexport const chatSettingsValueState = atom({\\n  key: \\'ChatSettingsValue\\',\\n  default: chatSettingsDefaultValueSelector\\n});\\n\\nexport const elementState = atom<IMessageElement[]>({\\n  key: \\'DisplayElements\\',\\n  default: []\\n});\\n\\nexport const tasklistState = atom<ITasklistElement[]>({\\n  key: \\'TasklistElements\\',\\n  default: []\\n});\\n\\nexport const firstUserInteraction = atom<string | undefined>({\\n  key: \\'FirstUserInteraction\\',\\n  default: undefined\\n});\\n\\nexport const userState = atom<IUser | undefined | null>({\\n  key: \\'User\\',\\n  default: undefined\\n});\\n\\nexport const configState = atom<IChainlitConfig | undefined>({\\n  key: \\'ChainlitConfig\\',\\n  default: undefined\\n});\\n\\nexport const authState = atom<IAuthConfig | undefined>({\\n  key: \\'AuthConfig\\',\\n  default: undefined\\n});\\n\\nexport const threadHistoryState = atom<ThreadHistory | undefined>({\\n  key: \\'ThreadHistory\\',\\n  default: {\\n    threads: undefined,\\n    currentThreadId: undefined,\\n    timeGroupedThreads: undefined,\\n    pageInfo: undefined\\n  },\\n  effects: [\\n    ({ setSelf, onSet }: { setSelf: any; onSet: any }) => {\\n      onSet(\\n        (\\n          newValue: ThreadHistory | undefined,\\n          oldValue: ThreadHistory | undefined\\n        ) => {\\n          let timeGroupedThreads = newValue?.timeGroupedThreads;\\n          if (\\n            newValue?.threads &&\\n            !isEqual(newValue.threads, oldValue?.timeGroupedThreads)\\n          ) {\\n            timeGroupedThreads = groupByDate(newValue.threads);\\n          }\\n\\n          setSelf({\\n            ...newValue,\\n            timeGroupedThreads\\n          });\\n        }\\n      );\\n    }\\n  ]\\n});\\n\\nexport const sideViewState = atom<\\n  { title: string; elements: IMessageElement[]; key?: string } | undefined\\n>({\\n  key: \\'SideView\\',\\n  default: undefined\\n});\\n\\nexport const currentThreadIdState = atom<string | undefined>({\\n  key: \\'CurrentThreadId\\',\\n  default: undefined\\n});\\n\\nconst localStorageEffect =\\n  <T>(key: string): AtomEffect<T> =>\\n  ({ setSelf, onSet }) => {\\n    // When the atom is first initialized, try to get its value from localStorage\\n    const savedValue = localStorage.getItem(key);\\n    if (savedValue != null) {\\n      try {\\n        setSelf(JSON.parse(savedValue));\\n      } catch (error) {\\n        console.error(\\n          `Error parsing localStorage value for key \"${key}\":`,\\n          error\\n        );\\n      }\\n    }\\n\\n    // Subscribe to state changes and update localStorage\\n    onSet((newValue, _, isReset) => {\\n      if (isReset) {\\n        localStorage.removeItem(key);\\n      } else {\\n        localStorage.setItem(key, JSON.stringify(newValue));\\n      }\\n    });\\n  };\\n\\nexport const mcpState = atom<IMcp[]>({\\n  key: \\'Mcp\\',\\n  default: [],\\n  effects: [localStorageEffect<IMcp[]>(\\'mcp_storage_key\\')]\\n});\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useAudio.ts\\n================================================\\nimport { useCallback } from \\'react\\';\\nimport { useRecoilState, useRecoilValue } from \\'recoil\\';\\n\\nimport {\\n  audioConnectionState,\\n  isAiSpeakingState,\\n  wavRecorderState,\\n  wavStreamPlayerState\\n} from \\'./state\\';\\nimport { useChatInteract } from \\'./useChatInteract\\';\\n\\nconst useAudio = () => {\\n  const [audioConnection, setAudioConnection] =\\n    useRecoilState(audioConnectionState);\\n  const wavRecorder = useRecoilValue(wavRecorderState);\\n  const wavStreamPlayer = useRecoilValue(wavStreamPlayerState);\\n  const isAiSpeaking = useRecoilValue(isAiSpeakingState);\\n\\n  const { startAudioStream, endAudioStream } = useChatInteract();\\n\\n  const startConversation = useCallback(async () => {\\n    setAudioConnection(\\'connecting\\');\\n    await startAudioStream();\\n  }, [startAudioStream]);\\n\\n  const endConversation = useCallback(async () => {\\n    setAudioConnection(\\'off\\');\\n    await wavRecorder.end();\\n    await wavStreamPlayer.interrupt();\\n    await endAudioStream();\\n  }, [endAudioStream, wavRecorder, wavStreamPlayer]);\\n\\n  return {\\n    startConversation,\\n    endConversation,\\n    audioConnection,\\n    isAiSpeaking,\\n    wavRecorder,\\n    wavStreamPlayer\\n  };\\n};\\n\\nexport { useAudio };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useChatData.ts\\n================================================\\nimport { useRecoilValue } from \\'recoil\\';\\n\\nimport {\\n  actionState,\\n  askUserState,\\n  callFnState,\\n  chatSettingsDefaultValueSelector,\\n  chatSettingsInputsState,\\n  chatSettingsValueState,\\n  elementState,\\n  loadingState,\\n  sessionState,\\n  tasklistState\\n} from \\'./state\\';\\n\\nexport interface IToken {\\n  id: number | string;\\n  token: string;\\n  isSequence: boolean;\\n  isInput: boolean;\\n}\\n\\nconst useChatData = () => {\\n  const loading = useRecoilValue(loadingState);\\n  const elements = useRecoilValue(elementState);\\n  const tasklists = useRecoilValue(tasklistState);\\n  const actions = useRecoilValue(actionState);\\n  const session = useRecoilValue(sessionState);\\n  const askUser = useRecoilValue(askUserState);\\n  const callFn = useRecoilValue(callFnState);\\n  const chatSettingsInputs = useRecoilValue(chatSettingsInputsState);\\n  const chatSettingsValue = useRecoilValue(chatSettingsValueState);\\n  const chatSettingsDefaultValue = useRecoilValue(\\n    chatSettingsDefaultValueSelector\\n  );\\n\\n  const connected = session?.socket.connected && !session?.error;\\n  const disabled =\\n    !connected ||\\n    loading ||\\n    askUser?.spec.type === \\'file\\' ||\\n    askUser?.spec.type === \\'action\\';\\n\\n  return {\\n    actions,\\n    askUser,\\n    callFn,\\n    chatSettingsDefaultValue,\\n    chatSettingsInputs,\\n    chatSettingsValue,\\n    connected,\\n    disabled,\\n    elements,\\n    error: session?.error,\\n    loading,\\n    tasklists\\n  };\\n};\\n\\nexport { useChatData };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useChatInteract.ts\\n================================================\\nimport { useCallback, useContext } from \\'react\\';\\nimport { useRecoilValue, useResetRecoilState, useSetRecoilState } from \\'recoil\\';\\nimport {\\n  actionState,\\n  askUserState,\\n  chatSettingsInputsState,\\n  chatSettingsValueState,\\n  currentThreadIdState,\\n  elementState,\\n  firstUserInteraction,\\n  loadingState,\\n  messagesState,\\n  sessionIdState,\\n  sessionState,\\n  sideViewState,\\n  tasklistState,\\n  threadIdToResumeState,\\n  tokenCountState\\n} from \\'src/state\\';\\nimport { IFileRef, IStep } from \\'src/types\\';\\nimport { addMessage } from \\'src/utils/message\\';\\nimport { v4 as uuidv4 } from \\'uuid\\';\\n\\nimport { ChainlitContext } from \\'./context\\';\\n\\ntype PartialBy<T, K extends keyof T> = Omit<T, K> & Partial<Pick<T, K>>;\\n\\nconst useChatInteract = () => {\\n  const client = useContext(ChainlitContext);\\n  const session = useRecoilValue(sessionState);\\n  const askUser = useRecoilValue(askUserState);\\n  const sessionId = useRecoilValue(sessionIdState);\\n\\n  const resetChatSettings = useResetRecoilState(chatSettingsInputsState);\\n  const resetSessionId = useResetRecoilState(sessionIdState);\\n  const resetChatSettingsValue = useResetRecoilState(chatSettingsValueState);\\n\\n  const setFirstUserInteraction = useSetRecoilState(firstUserInteraction);\\n  const setLoading = useSetRecoilState(loadingState);\\n  const setMessages = useSetRecoilState(messagesState);\\n  const setElements = useSetRecoilState(elementState);\\n  const setTasklists = useSetRecoilState(tasklistState);\\n  const setActions = useSetRecoilState(actionState);\\n  const setTokenCount = useSetRecoilState(tokenCountState);\\n  const setIdToResume = useSetRecoilState(threadIdToResumeState);\\n  const setSideView = useSetRecoilState(sideViewState);\\n  const setCurrentThreadId = useSetRecoilState(currentThreadIdState);\\n\\n  const clear = useCallback(() => {\\n    session?.socket.emit(\\'clear_session\\');\\n    session?.socket.disconnect();\\n    setIdToResume(undefined);\\n    resetSessionId();\\n    setFirstUserInteraction(undefined);\\n    setMessages([]);\\n    setElements([]);\\n    setTasklists([]);\\n    setActions([]);\\n    setTokenCount(0);\\n    resetChatSettings();\\n    resetChatSettingsValue();\\n    setSideView(undefined);\\n    setCurrentThreadId(undefined);\\n  }, [session]);\\n\\n  const sendMessage = useCallback(\\n    (\\n      message: PartialBy<IStep, \\'createdAt\\' | \\'id\\'>,\\n      fileReferences: IFileRef[] = []\\n    ) => {\\n      if (!message.id) {\\n        message.id = uuidv4();\\n      }\\n      if (!message.createdAt) {\\n        message.createdAt = new Date().toISOString();\\n      }\\n      setMessages((oldMessages) => addMessage(oldMessages, message as IStep));\\n\\n      session?.socket.emit(\\'client_message\\', { message, fileReferences });\\n    },\\n    [session?.socket]\\n  );\\n\\n  const editMessage = useCallback(\\n    (message: IStep) => {\\n      session?.socket.emit(\\'edit_message\\', { message });\\n    },\\n    [session?.socket]\\n  );\\n\\n  const windowMessage = useCallback(\\n    (data: any) => {\\n      session?.socket.emit(\\'window_message\\', data);\\n    },\\n    [session?.socket]\\n  );\\n\\n  const startAudioStream = useCallback(() => {\\n    session?.socket.emit(\\'audio_start\\');\\n  }, [session?.socket]);\\n\\n  const sendAudioChunk = useCallback(\\n    (\\n      isStart: boolean,\\n      mimeType: string,\\n      elapsedTime: number,\\n      data: Int16Array\\n    ) => {\\n      session?.socket.emit(\\'audio_chunk\\', {\\n        isStart,\\n        mimeType,\\n        elapsedTime,\\n        data\\n      });\\n    },\\n    [session?.socket]\\n  );\\n\\n  const endAudioStream = useCallback(() => {\\n    session?.socket.emit(\\'audio_end\\');\\n  }, [session?.socket]);\\n\\n  const replyMessage = useCallback(\\n    (message: IStep) => {\\n      if (askUser) {\\n        if (askUser.parentId) message.parentId = askUser.parentId;\\n        setMessages((oldMessages) => addMessage(oldMessages, message));\\n        askUser.callback(message);\\n      }\\n    },\\n    [askUser]\\n  );\\n\\n  const updateChatSettings = useCallback(\\n    (values: object) => {\\n      session?.socket.emit(\\'chat_settings_change\\', values);\\n    },\\n    [session?.socket]\\n  );\\n\\n  const stopTask = useCallback(() => {\\n    setMessages((oldMessages) =>\\n      oldMessages.map((m) => {\\n        m.streaming = false;\\n        return m;\\n      })\\n    );\\n\\n    setLoading(false);\\n\\n    session?.socket.emit(\\'stop\\');\\n  }, [session?.socket]);\\n\\n  const uploadFile = useCallback(\\n    (file: File, onProgress: (progress: number) => void) => {\\n      return client.uploadFile(file, onProgress, sessionId);\\n    },\\n    [sessionId]\\n  );\\n\\n  return {\\n    uploadFile,\\n    clear,\\n    replyMessage,\\n    sendMessage,\\n    editMessage,\\n    windowMessage,\\n    startAudioStream,\\n    sendAudioChunk,\\n    endAudioStream,\\n    stopTask,\\n    setIdToResume,\\n    updateChatSettings\\n  };\\n};\\n\\nexport { useChatInteract };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useChatMessages.ts\\n================================================\\nimport { useRecoilValue } from \\'recoil\\';\\n\\nimport {\\n  currentThreadIdState,\\n  firstUserInteraction,\\n  messagesState\\n} from \\'./state\\';\\n\\nconst useChatMessages = () => {\\n  const messages = useRecoilValue(messagesState);\\n  const firstInteraction = useRecoilValue(firstUserInteraction);\\n  const threadId = useRecoilValue(currentThreadIdState);\\n\\n  return {\\n    threadId,\\n    messages,\\n    firstInteraction\\n  };\\n};\\n\\nexport { useChatMessages };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useChatSession.ts\\n================================================\\nimport { debounce } from \\'lodash\\';\\nimport { useCallback, useContext, useEffect } from \\'react\\';\\nimport {\\n  useRecoilState,\\n  useRecoilValue,\\n  useResetRecoilState,\\n  useSetRecoilState\\n} from \\'recoil\\';\\nimport io from \\'socket.io-client\\';\\nimport { toast } from \\'sonner\\';\\nimport {\\n  actionState,\\n  askUserState,\\n  audioConnectionState,\\n  callFnState,\\n  chatProfileState,\\n  chatSettingsInputsState,\\n  chatSettingsValueState,\\n  commandsState,\\n  currentThreadIdState,\\n  elementState,\\n  firstUserInteraction,\\n  isAiSpeakingState,\\n  loadingState,\\n  mcpState,\\n  messagesState,\\n  resumeThreadErrorState,\\n  sessionIdState,\\n  sessionState,\\n  sideViewState,\\n  tasklistState,\\n  threadIdToResumeState,\\n  tokenCountState,\\n  wavRecorderState,\\n  wavStreamPlayerState\\n} from \\'src/state\\';\\nimport {\\n  IAction,\\n  ICommand,\\n  IElement,\\n  IMessageElement,\\n  IStep,\\n  ITasklistElement,\\n  IThread\\n} from \\'src/types\\';\\nimport {\\n  addMessage,\\n  deleteMessageById,\\n  updateMessageById,\\n  updateMessageContentById\\n} from \\'src/utils/message\\';\\n\\nimport { OutputAudioChunk } from \\'./types/audio\\';\\n\\nimport { ChainlitContext } from \\'./context\\';\\nimport type { IToken } from \\'./useChatData\\';\\n\\nconst useChatSession = () => {\\n  const client = useContext(ChainlitContext);\\n  const sessionId = useRecoilValue(sessionIdState);\\n\\n  const [session, setSession] = useRecoilState(sessionState);\\n  const setIsAiSpeaking = useSetRecoilState(isAiSpeakingState);\\n  const setAudioConnection = useSetRecoilState(audioConnectionState);\\n  const resetChatSettingsValue = useResetRecoilState(chatSettingsValueState);\\n  const setChatSettingsValue = useSetRecoilState(chatSettingsValueState);\\n  const setFirstUserInteraction = useSetRecoilState(firstUserInteraction);\\n  const setLoading = useSetRecoilState(loadingState);\\n  const setMcps = useSetRecoilState(mcpState);\\n  const wavStreamPlayer = useRecoilValue(wavStreamPlayerState);\\n  const wavRecorder = useRecoilValue(wavRecorderState);\\n  const setMessages = useSetRecoilState(messagesState);\\n  const setAskUser = useSetRecoilState(askUserState);\\n  const setCallFn = useSetRecoilState(callFnState);\\n  const setCommands = useSetRecoilState(commandsState);\\n  const setSideView = useSetRecoilState(sideViewState);\\n  const setElements = useSetRecoilState(elementState);\\n  const setTasklists = useSetRecoilState(tasklistState);\\n  const setActions = useSetRecoilState(actionState);\\n  const setChatSettingsInputs = useSetRecoilState(chatSettingsInputsState);\\n  const setTokenCount = useSetRecoilState(tokenCountState);\\n  const [chatProfile, setChatProfile] = useRecoilState(chatProfileState);\\n  const idToResume = useRecoilValue(threadIdToResumeState);\\n  const setThreadResumeError = useSetRecoilState(resumeThreadErrorState);\\n\\n  const [currentThreadId, setCurrentThreadId] =\\n    useRecoilState(currentThreadIdState);\\n\\n  // Use currentThreadId as thread id in websocket header\\n  useEffect(() => {\\n    if (session?.socket) {\\n      session.socket.auth[\\'threadId\\'] = currentThreadId || \\'\\';\\n    }\\n  }, [currentThreadId]);\\n\\n  const _connect = useCallback(\\n    ({\\n      transports,\\n      userEnv\\n    }: {\\n      transports?: string[];\\n      userEnv: Record<string, string>;\\n    }) => {\\n      const { protocol, host, pathname } = new URL(client.httpEndpoint);\\n      const uri = `${protocol}//${host}`;\\n      const path =\\n        pathname && pathname !== \\'/\\'\\n          ? `${pathname}/ws/socket.io`\\n          : \\'/ws/socket.io\\';\\n\\n      const socket = io(uri, {\\n        path,\\n        withCredentials: true,\\n        transports,\\n        auth: {\\n          clientType: client.type,\\n          sessionId,\\n          threadId: idToResume || \\'\\',\\n          userEnv: JSON.stringify(userEnv),\\n          chatProfile: chatProfile ? encodeURIComponent(chatProfile) : \\'\\'\\n        }\\n      });\\n      setSession((old) => {\\n        old?.socket?.removeAllListeners();\\n        old?.socket?.close();\\n        return {\\n          socket\\n        };\\n      });\\n\\n      socket.on(\\'connect\\', () => {\\n        socket.emit(\\'connection_successful\\');\\n        setSession((s) => ({ ...s!, error: false }));\\n        setMcps((prev) =>\\n          prev.map((mcp) => {\\n            const promise =\\n              mcp.clientType === \\'sse\\'\\n                ? client.connectSseMCP(sessionId, mcp.name, mcp.url!)\\n                : client.connectStdioMCP(sessionId, mcp.name, mcp.command!);\\n            promise\\n              .then(async ({ success, mcp }) => {\\n                setMcps((prev) =>\\n                  prev.map((existingMcp) => {\\n                    if (existingMcp.name === mcp.name) {\\n                      return {\\n                        ...existingMcp,\\n                        status: success ? \\'connected\\' : \\'failed\\',\\n                        tools: mcp ? mcp.tools : existingMcp.tools\\n                      };\\n                    }\\n                    return existingMcp;\\n                  })\\n                );\\n              })\\n              .catch(() => {\\n                setMcps((prev) =>\\n                  prev.map((existingMcp) => {\\n                    if (existingMcp.name === mcp.name) {\\n                      return {\\n                        ...existingMcp,\\n                        status: \\'failed\\'\\n                      };\\n                    }\\n                    return existingMcp;\\n                  })\\n                );\\n              });\\n            return { ...mcp, status: \\'connecting\\' };\\n          })\\n        );\\n      });\\n\\n      socket.on(\\'connect_error\\', (_) => {\\n        setSession((s) => ({ ...s!, error: true }));\\n      });\\n\\n      socket.on(\\'task_start\\', () => {\\n        setLoading(true);\\n      });\\n\\n      socket.on(\\'task_end\\', () => {\\n        setLoading(false);\\n      });\\n\\n      socket.on(\\'reload\\', () => {\\n        socket.emit(\\'clear_session\\');\\n        window.location.reload();\\n      });\\n\\n      socket.on(\\'audio_connection\\', async (state: \\'on\\' | \\'off\\') => {\\n        if (state === \\'on\\') {\\n          let isFirstChunk = true;\\n          const startTime = Date.now();\\n          const mimeType = \\'pcm16\\';\\n          // Connect to microphone\\n          await wavRecorder.begin();\\n          await wavStreamPlayer.connect();\\n          await wavRecorder.record(async (data) => {\\n            const elapsedTime = Date.now() - startTime;\\n            socket.emit(\\'audio_chunk\\', {\\n              isStart: isFirstChunk,\\n              mimeType,\\n              elapsedTime,\\n              data: data.mono\\n            });\\n            isFirstChunk = false;\\n          });\\n          wavStreamPlayer.onStop = () => setIsAiSpeaking(false);\\n        } else {\\n          await wavRecorder.end();\\n          await wavStreamPlayer.interrupt();\\n        }\\n        setAudioConnection(state);\\n      });\\n\\n      socket.on(\\'audio_chunk\\', (chunk: OutputAudioChunk) => {\\n        wavStreamPlayer.add16BitPCM(chunk.data, chunk.track);\\n        setIsAiSpeaking(true);\\n      });\\n\\n      socket.on(\\'audio_interrupt\\', () => {\\n        wavStreamPlayer.interrupt();\\n      });\\n\\n      socket.on(\\'resume_thread\\', (thread: IThread) => {\\n        let messages: IStep[] = [];\\n        for (const step of thread.steps) {\\n          messages = addMessage(messages, step);\\n        }\\n        if (thread.metadata?.chat_profile) {\\n          setChatProfile(thread.metadata?.chat_profile);\\n        }\\n        if (thread.metadata?.chat_settings) {\\n          setChatSettingsValue(thread.metadata?.chat_settings);\\n        }\\n        setMessages(messages);\\n        const elements = thread.elements || [];\\n        setTasklists(\\n          (elements as ITasklistElement[]).filter((e) => e.type === \\'tasklist\\')\\n        );\\n        setElements(\\n          (elements as IMessageElement[]).filter(\\n            (e) => [\\'avatar\\', \\'tasklist\\'].indexOf(e.type) === -1\\n          )\\n        );\\n      });\\n\\n      socket.on(\\'resume_thread_error\\', (error?: string) => {\\n        setThreadResumeError(error);\\n      });\\n\\n      socket.on(\\'new_message\\', (message: IStep) => {\\n        setMessages((oldMessages) => addMessage(oldMessages, message));\\n      });\\n\\n      socket.on(\\n        \\'first_interaction\\',\\n        (event: { interaction: string; thread_id: string }) => {\\n          setFirstUserInteraction(event.interaction);\\n          setCurrentThreadId(event.thread_id);\\n        }\\n      );\\n\\n      socket.on(\\'update_message\\', (message: IStep) => {\\n        setMessages((oldMessages) =>\\n          updateMessageById(oldMessages, message.id, message)\\n        );\\n      });\\n\\n      socket.on(\\'delete_message\\', (message: IStep) => {\\n        setMessages((oldMessages) =>\\n          deleteMessageById(oldMessages, message.id)\\n        );\\n      });\\n\\n      socket.on(\\'stream_start\\', (message: IStep) => {\\n        setMessages((oldMessages) => addMessage(oldMessages, message));\\n      });\\n\\n      socket.on(\\n        \\'stream_token\\',\\n        ({ id, token, isSequence, isInput }: IToken) => {\\n          setMessages((oldMessages) =>\\n            updateMessageContentById(\\n              oldMessages,\\n              id,\\n              token,\\n              isSequence,\\n              isInput\\n            )\\n          );\\n        }\\n      );\\n\\n      socket.on(\\'ask\\', ({ msg, spec }, callback) => {\\n        setAskUser({ spec, callback, parentId: msg.parentId });\\n        setMessages((oldMessages) => addMessage(oldMessages, msg));\\n\\n        setLoading(false);\\n      });\\n\\n      socket.on(\\'ask_timeout\\', () => {\\n        setAskUser(undefined);\\n        setLoading(false);\\n      });\\n\\n      socket.on(\\'clear_ask\\', () => {\\n        setAskUser(undefined);\\n      });\\n\\n      socket.on(\\'call_fn\\', ({ name, args }, callback) => {\\n        setCallFn({ name, args, callback });\\n      });\\n\\n      socket.on(\\'clear_call_fn\\', () => {\\n        setCallFn(undefined);\\n      });\\n\\n      socket.on(\\'call_fn_timeout\\', () => {\\n        setCallFn(undefined);\\n      });\\n\\n      socket.on(\\'chat_settings\\', (inputs: any) => {\\n        setChatSettingsInputs(inputs);\\n        resetChatSettingsValue();\\n      });\\n\\n      socket.on(\\'set_commands\\', (commands: ICommand[]) => {\\n        setCommands(commands);\\n      });\\n\\n      socket.on(\\'set_sidebar_title\\', (title: string) => {\\n        setSideView((prev) => {\\n          if (prev?.title === title) return prev;\\n          return { title, elements: prev?.elements || [] };\\n        });\\n      });\\n\\n      socket.on(\\n        \\'set_sidebar_elements\\',\\n        ({ elements, key }: { elements: IMessageElement[]; key?: string }) => {\\n          if (!elements.length) {\\n            setSideView(undefined);\\n          } else {\\n            elements.forEach((element) => {\\n              if (!element.url && element.chainlitKey) {\\n                element.url = client.getElementUrl(\\n                  element.chainlitKey,\\n                  sessionId\\n                );\\n              }\\n            });\\n            setSideView((prev) => {\\n              if (prev?.key === key) return prev;\\n              return { title: prev?.title || \\'\\', elements: elements, key };\\n            });\\n          }\\n        }\\n      );\\n\\n      socket.on(\\'element\\', (element: IElement) => {\\n        if (!element.url && element.chainlitKey) {\\n          element.url = client.getElementUrl(element.chainlitKey, sessionId);\\n        }\\n\\n        if (element.type === \\'tasklist\\') {\\n          setTasklists((old) => {\\n            const index = old.findIndex((e) => e.id === element.id);\\n            if (index === -1) {\\n              return [...old, element];\\n            } else {\\n              return [...old.slice(0, index), element, ...old.slice(index + 1)];\\n            }\\n          });\\n        } else {\\n          setElements((old) => {\\n            const index = old.findIndex((e) => e.id === element.id);\\n            if (index === -1) {\\n              return [...old, element];\\n            } else {\\n              return [...old.slice(0, index), element, ...old.slice(index + 1)];\\n            }\\n          });\\n        }\\n      });\\n\\n      socket.on(\\'remove_element\\', (remove: { id: string }) => {\\n        setElements((old) => {\\n          return old.filter((e) => e.id !== remove.id);\\n        });\\n        setTasklists((old) => {\\n          return old.filter((e) => e.id !== remove.id);\\n        });\\n      });\\n\\n      socket.on(\\'action\\', (action: IAction) => {\\n        setActions((old) => [...old, action]);\\n      });\\n\\n      socket.on(\\'remove_action\\', (action: IAction) => {\\n        setActions((old) => {\\n          const index = old.findIndex((a) => a.id === action.id);\\n          if (index === -1) return old;\\n          return [...old.slice(0, index), ...old.slice(index + 1)];\\n        });\\n      });\\n\\n      socket.on(\\'token_usage\\', (count: number) => {\\n        setTokenCount((old) => old + count);\\n      });\\n\\n      socket.on(\\'window_message\\', (data: any) => {\\n        if (window.parent) {\\n          window.parent.postMessage(data, \\'*\\');\\n        }\\n      });\\n\\n      socket.on(\\'toast\\', (data: { message: string; type: string }) => {\\n        if (!data.message) {\\n          console.warn(\\'No message received for toast.\\');\\n          return;\\n        }\\n\\n        switch (data.type) {\\n          case \\'info\\':\\n            toast.info(data.message);\\n            break;\\n          case \\'error\\':\\n            toast.error(data.message);\\n            break;\\n          case \\'success\\':\\n            toast.success(data.message);\\n            break;\\n          case \\'warning\\':\\n            toast.warning(data.message);\\n            break;\\n          default:\\n            toast(data.message);\\n            break;\\n        }\\n      });\\n    },\\n    [setSession, sessionId, idToResume, chatProfile]\\n  );\\n\\n  const connect = useCallback(debounce(_connect, 200), [_connect]);\\n\\n  const disconnect = useCallback(() => {\\n    if (session?.socket) {\\n      session.socket.removeAllListeners();\\n      session.socket.close();\\n    }\\n  }, [session]);\\n\\n  return {\\n    connect,\\n    disconnect,\\n    session,\\n    sessionId,\\n    chatProfile,\\n    idToResume,\\n    setChatProfile\\n  };\\n};\\n\\nexport { useChatSession };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/useConfig.ts\\n================================================\\nimport { useEffect } from \\'react\\';\\nimport { useRecoilState } from \\'recoil\\';\\n\\nimport { useApi, useAuth } from \\'./api\\';\\nimport { configState } from \\'./state\\';\\nimport { IChainlitConfig } from \\'./types\\';\\n\\nconst useConfig = () => {\\n  const [config, setConfig] = useRecoilState(configState);\\n  const { isAuthenticated } = useAuth();\\n  const language = navigator.language || \\'en-US\\';\\n\\n  const { data, error, isLoading } = useApi<IChainlitConfig>(\\n    !config && isAuthenticated ? `/project/settings?language=${language}` : null\\n  );\\n\\n  useEffect(() => {\\n    if (!data) return;\\n    setConfig(data);\\n  }, [data, setConfig]);\\n\\n  return { config, error, isLoading, language };\\n};\\n\\nexport { useConfig };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/index.tsx\\n================================================\\nimport { IElement, IThread, IUser } from \\'src/types\\';\\n\\nimport { IAction } from \\'src/types/action\\';\\nimport { IFeedback } from \\'src/types/feedback\\';\\n\\nexport * from \\'./hooks/auth\\';\\nexport * from \\'./hooks/api\\';\\n\\nexport interface IThreadFilters {\\n  search?: string;\\n  feedback?: number;\\n}\\n\\nexport interface IPageInfo {\\n  hasNextPage: boolean;\\n  endCursor?: string;\\n}\\n\\nexport interface IPagination {\\n  first: number;\\n  cursor?: string | number;\\n}\\n\\nexport class ClientError extends Error {\\n  status: number;\\n  detail?: string;\\n\\n  constructor(message: string, status: number, detail?: string) {\\n    super(message);\\n    this.status = status;\\n    this.detail = detail;\\n  }\\n\\n  toString() {\\n    if (this.detail) {\\n      return `${this.message}: ${this.detail}`;\\n    } else {\\n      return this.message;\\n    }\\n  }\\n}\\n\\ntype Payload = FormData | any;\\n\\nexport class APIBase {\\n  constructor(\\n    public httpEndpoint: string,\\n    public type: \\'webapp\\' | \\'copilot\\' | \\'teams\\' | \\'slack\\' | \\'discord\\',\\n    public on401?: () => void,\\n    public onError?: (error: ClientError) => void\\n  ) {}\\n\\n  buildEndpoint(path: string) {\\n    if (this.httpEndpoint.endsWith(\\'/\\')) {\\n      // remove trailing slash on httpEndpoint\\n      return `${this.httpEndpoint.slice(0, -1)}${path}`;\\n    } else {\\n      return `${this.httpEndpoint}${path}`;\\n    }\\n  }\\n\\n  private async getDetailFromErrorResponse(\\n    res: Response\\n  ): Promise<string | undefined> {\\n    try {\\n      const body = await res.json();\\n      return body?.detail;\\n    } catch (error: any) {\\n      console.error(\\'Unable to parse error response\\', error);\\n    }\\n    return undefined;\\n  }\\n\\n  private handleRequestError(error: any) {\\n    if (error instanceof ClientError) {\\n      if (error.status === 401 && this.on401) {\\n        this.on401();\\n      }\\n      if (this.onError) {\\n        this.onError(error);\\n      }\\n    }\\n    console.error(error);\\n  }\\n\\n  /**\\n   * Low-level HTTP request handler for direct API interactions.\\n   * Provides full control over HTTP methods, request configuration, and error handling.\\n   *\\n   * Key features:\\n   * - Supports all HTTP methods (GET, POST, PUT, PATCH, DELETE)\\n   * - Handles both FormData and JSON payloads\\n   * - Manages authentication headers\\n   * - Custom error handling with ClientError class\\n   * - Support for request cancellation via AbortSignal\\n   *\\n   * @param method - HTTP method to use (GET, POST, etc.)\\n   * @param path - API endpoint path\\n   * @param data - Optional request payload (FormData or JSON-serializable data)\\n   * @param signal - Optional AbortSignal for request cancellation\\n   * @returns Promise<Response>\\n   * @throws ClientError for HTTP errors, including 401 unauthorized\\n   */\\n  async fetch(\\n    method: string,\\n    path: string,\\n    data?: Payload,\\n    signal?: AbortSignal,\\n    headers: { Authorization?: string; \\'Content-Type\\'?: string } = {}\\n  ): Promise<Response> {\\n    try {\\n      let body;\\n\\n      if (data instanceof FormData) {\\n        body = data;\\n      } else {\\n        headers[\\'Content-Type\\'] = \\'application/json\\';\\n        body = data ? JSON.stringify(data) : null;\\n      }\\n\\n      const res = await fetch(this.buildEndpoint(path), {\\n        method,\\n        credentials: \\'include\\',\\n        headers,\\n        signal,\\n        body\\n      });\\n\\n      if (!res.ok) {\\n        const detail = await this.getDetailFromErrorResponse(res);\\n\\n        throw new ClientError(res.statusText, res.status, detail);\\n      }\\n\\n      return res;\\n    } catch (error: any) {\\n      this.handleRequestError(error);\\n      throw error;\\n    }\\n  }\\n\\n  async get(endpoint: string) {\\n    return await this.fetch(\\'GET\\', endpoint);\\n  }\\n\\n  async post(endpoint: string, data: Payload, signal?: AbortSignal) {\\n    return await this.fetch(\\'POST\\', endpoint, data, signal);\\n  }\\n\\n  async put(endpoint: string, data: Payload) {\\n    return await this.fetch(\\'PUT\\', endpoint, data);\\n  }\\n\\n  async patch(endpoint: string, data: Payload) {\\n    return await this.fetch(\\'PATCH\\', endpoint, data);\\n  }\\n\\n  async delete(endpoint: string, data: Payload) {\\n    return await this.fetch(\\'DELETE\\', endpoint, data);\\n  }\\n}\\n\\nexport class ChainlitAPI extends APIBase {\\n  async headerAuth() {\\n    const res = await this.post(`/auth/header`, {});\\n    return res.json();\\n  }\\n\\n  async jwtAuth(token: string) {\\n    const res = await this.fetch(\\'POST\\', \\'/auth/jwt\\', undefined, undefined, {\\n      Authorization: `Bearer ${token}`\\n    });\\n    return res.json();\\n  }\\n\\n  async passwordAuth(data: FormData) {\\n    const res = await this.post(`/login`, data);\\n    return res.json();\\n  }\\n\\n  async getUser(): Promise<IUser> {\\n    const res = await this.get(`/user`);\\n    return res.json();\\n  }\\n\\n  async logout() {\\n    const res = await this.post(`/logout`, {});\\n    return res.json();\\n  }\\n\\n  async setFeedback(\\n    feedback: IFeedback\\n  ): Promise<{ success: boolean; feedbackId: string }> {\\n    const res = await this.put(`/feedback`, { feedback });\\n    return res.json();\\n  }\\n\\n  async deleteFeedback(feedbackId: string): Promise<{ success: boolean }> {\\n    const res = await this.delete(`/feedback`, { feedbackId });\\n    return res.json();\\n  }\\n\\n  async listThreads(\\n    pagination: IPagination,\\n    filter: IThreadFilters\\n  ): Promise<{\\n    pageInfo: IPageInfo;\\n    data: IThread[];\\n  }> {\\n    const res = await this.post(`/project/threads`, { pagination, filter });\\n\\n    return res.json();\\n  }\\n\\n  async renameThread(threadId: string, name: string) {\\n    const res = await this.put(`/project/thread`, { threadId, name });\\n\\n    return res.json();\\n  }\\n\\n  async deleteThread(threadId: string) {\\n    const res = await this.delete(`/project/thread`, { threadId });\\n\\n    return res.json();\\n  }\\n\\n  uploadFile(\\n    file: File,\\n    onProgress: (progress: number) => void,\\n    sessionId: string\\n  ) {\\n    const xhr = new XMLHttpRequest();\\n    xhr.withCredentials = true;\\n\\n    const promise = new Promise<{ id: string }>((resolve, reject) => {\\n      const formData = new FormData();\\n      formData.append(\\'file\\', file);\\n\\n      xhr.open(\\n        \\'POST\\',\\n        this.buildEndpoint(`/project/file?session_id=${sessionId}`),\\n        true\\n      );\\n\\n      // Track the progress of the upload\\n      xhr.upload.onprogress = function (event) {\\n        if (event.lengthComputable) {\\n          const percentage = (event.loaded / event.total) * 100;\\n          onProgress(percentage);\\n        }\\n      };\\n\\n      xhr.onload = function () {\\n        if (xhr.status === 200) {\\n          const response = JSON.parse(xhr.responseText);\\n          resolve(response);\\n          return;\\n        }\\n        const contentType = xhr.getResponseHeader(\\'Content-Type\\');\\n        if (contentType && contentType.includes(\\'application/json\\')) {\\n          const response = JSON.parse(xhr.responseText);\\n          reject(response.detail);\\n        } else {\\n          reject(\\'Upload failed\\');\\n        }\\n      };\\n\\n      xhr.onerror = function () {\\n        reject(\\'Upload error\\');\\n      };\\n\\n      xhr.send(formData);\\n    });\\n\\n    return { xhr, promise };\\n  }\\n\\n  async callAction(action: IAction, sessionId: string) {\\n    const res = await this.post(`/project/action`, { sessionId, action });\\n\\n    return res.json();\\n  }\\n\\n  async updateElement(element: IElement, sessionId: string) {\\n    const res = await this.put(`/project/element`, { sessionId, element });\\n\\n    return res.json();\\n  }\\n\\n  async deleteElement(element: IElement, sessionId: string) {\\n    const res = await this.delete(`/project/element`, { sessionId, element });\\n\\n    return res.json();\\n  }\\n\\n  async connectStdioMCP(sessionId: string, name: string, fullCommand: string) {\\n    const res = await this.post(`/mcp`, {\\n      sessionId,\\n      name,\\n      fullCommand,\\n      clientType: \\'stdio\\'\\n    });\\n    return res.json();\\n  }\\n\\n  async connectSseMCP(sessionId: string, name: string, url: string) {\\n    const res = await this.post(`/mcp`, {\\n      sessionId,\\n      name,\\n      url,\\n      clientType: \\'sse\\'\\n    });\\n    return res.json();\\n  }\\n\\n  async disconnectMcp(sessionId: string, name: string) {\\n    const res = await this.delete(`/mcp`, { sessionId, name });\\n    return res.json();\\n  }\\n\\n  getElementUrl(id: string, sessionId: string) {\\n    const queryParams = `?session_id=${sessionId}`;\\n    return this.buildEndpoint(`/project/file/${id}${queryParams}`);\\n  }\\n\\n  getLogoEndpoint(theme: string) {\\n    return this.buildEndpoint(`/logo?theme=${theme}`);\\n  }\\n\\n  getOAuthEndpoint(provider: string) {\\n    return this.buildEndpoint(`/auth/oauth/${provider}`);\\n  }\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/api.ts\\n================================================\\nimport { useContext, useMemo } from \\'react\\';\\nimport { ChainlitAPI } from \\'src/api\\';\\nimport { ChainlitContext } from \\'src/context\\';\\nimport useSWR, { SWRConfig, SWRConfiguration } from \\'swr\\';\\n\\nimport { useAuthState } from \\'./auth/state\\';\\n\\nconst fetcher = async (client: ChainlitAPI, endpoint: string) => {\\n  const res = await client.get(endpoint);\\n  return res?.json();\\n};\\n\\nconst cloneClient = (client: ChainlitAPI): ChainlitAPI => {\\n  // Shallow clone API client.\\n  // TODO: Move me to core API.\\n\\n  // Create new client\\n  const newClient = new ChainlitAPI(\\'\\', \\'webapp\\');\\n\\n  // Assign old properties to new client\\n  Object.assign(newClient, client);\\n\\n  return newClient;\\n};\\n\\n/**\\n * React hook for cached API data fetching using SWR (stale-while-revalidate).\\n * Optimized for GET requests with automatic caching and revalidation.\\n *\\n * Key features:\\n * - Automatic data caching and revalidation\\n * - Integration with React component lifecycle\\n * - Loading state management\\n * - Recoil state integration for global state\\n * - Memoized fetcher function to prevent unnecessary rerenders\\n *\\n * @param path - API endpoint path or null to disable the request\\n * @param config - Optional SWR configuration\\n * @returns SWR response object containing:\\n *          - data: The fetched data\\n *          - error: Any error that occurred\\n *          - isValidating: Whether a request is in progress\\n *          - mutate: Function to mutate the cached data\\n *\\n * @example\\n * const { data, error, isValidating } = useApi<UserData>(\\'/user\\');\\n */\\nfunction useApi<T>(\\n  path?: string | null,\\n  { ...swrConfig }: SWRConfiguration = {}\\n) {\\n  const client = useContext(ChainlitContext);\\n  const { setUser } = useAuthState();\\n\\n  // Memoize the fetcher function to avoid recreating it on every render\\n  const memoizedFetcher = useMemo(\\n    () =>\\n      ([url]: [url: string]) => {\\n        if (!swrConfig.onErrorRetry) {\\n          swrConfig.onErrorRetry = (...args) => {\\n            const [err] = args;\\n\\n            // Don\\'t do automatic retry for 401 - it just means we\\'re not logged in (yet).\\n            if (err.status === 401) {\\n              setUser(null);\\n              return;\\n            }\\n\\n            // Fall back to default behavior.\\n            return SWRConfig.defaultValue.onErrorRetry(...args);\\n          };\\n        }\\n\\n        const useApiClient = cloneClient(client);\\n        useApiClient.on401 = useApiClient.onError = undefined;\\n        return fetcher(useApiClient, url);\\n      },\\n    [client]\\n  );\\n\\n  // Use a stable key for useSWR\\n  const swrKey = useMemo(() => {\\n    return path ? [path] : null;\\n  }, [path]);\\n\\n  return useSWR<T, Error>(swrKey, memoizedFetcher, swrConfig);\\n}\\n\\nexport { useApi, fetcher };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/config.ts\\n================================================\\nimport { useEffect } from \\'react\\';\\nimport { IAuthConfig } from \\'src/index\\';\\n\\nimport { useApi } from \\'../api\\';\\nimport { useAuthState } from \\'./state\\';\\n\\nexport const useAuthConfig = () => {\\n  const { authConfig, setAuthConfig } = useAuthState();\\n  const { data: authConfigData, isLoading } = useApi<IAuthConfig>(\\n    authConfig ? null : \\'/auth/config\\'\\n  );\\n\\n  useEffect(() => {\\n    if (authConfigData) {\\n      setAuthConfig(authConfigData);\\n    }\\n  }, [authConfigData, setAuthConfig]);\\n\\n  return { authConfig, isLoading };\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/index.ts\\n================================================\\nimport { IAuthConfig, IUser } from \\'src/types\\';\\n\\nimport { useAuthConfig } from \\'./config\\';\\nimport { useSessionManagement } from \\'./sessionManagement\\';\\nimport { useUserManagement } from \\'./userManagement\\';\\n\\nexport const useAuth = () => {\\n  const { authConfig } = useAuthConfig();\\n  const { logout } = useSessionManagement();\\n  const { user, setUserFromAPI } = useUserManagement();\\n\\n  const isReady =\\n    !!authConfig && (!authConfig.requireLogin || user !== undefined);\\n\\n  if (authConfig && !authConfig.requireLogin) {\\n    return {\\n      data: authConfig,\\n      user: null,\\n      isReady,\\n      isAuthenticated: true,\\n      logout: () => Promise.resolve(),\\n      setUserFromAPI: () => Promise.resolve()\\n    };\\n  }\\n\\n  return {\\n    data: authConfig,\\n    user,\\n    isReady,\\n    isAuthenticated: !!user,\\n    logout,\\n    setUserFromAPI\\n  };\\n};\\n\\nexport type { IAuthConfig, IUser };\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/sessionManagement.ts\\n================================================\\nimport { useContext } from \\'react\\';\\nimport { ChainlitContext } from \\'src/index\\';\\n\\nimport { useAuthState } from \\'./state\\';\\n\\nexport const useSessionManagement = () => {\\n  const apiClient = useContext(ChainlitContext);\\n  const { setUser, setThreadHistory } = useAuthState();\\n\\n  const logout = async (reload = false): Promise<void> => {\\n    await apiClient.logout();\\n    setUser(undefined);\\n    setThreadHistory(undefined);\\n\\n    if (reload) {\\n      window.location.reload();\\n    }\\n  };\\n\\n  return { logout };\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/state.ts\\n================================================\\nimport { useRecoilState, useSetRecoilState } from \\'recoil\\';\\nimport { authState, threadHistoryState, userState } from \\'src/state\\';\\n\\nexport const useAuthState = () => {\\n  const [authConfig, setAuthConfig] = useRecoilState(authState);\\n  const [user, setUser] = useRecoilState(userState);\\n  const setThreadHistory = useSetRecoilState(threadHistoryState);\\n\\n  return {\\n    authConfig,\\n    setAuthConfig,\\n    user,\\n    setUser,\\n    setThreadHistory\\n  };\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/types.ts\\n================================================\\nimport { IAuthConfig, IUser } from \\'src/types\\';\\n\\nexport interface JWTPayload extends IUser {\\n  exp: number;\\n}\\n\\nexport interface AuthState {\\n  data: IAuthConfig | undefined;\\n  user: IUser | null;\\n  isAuthenticated: boolean;\\n  isReady: boolean;\\n}\\n\\nexport interface AuthActions {\\n  logout: (reload?: boolean) => Promise<void>;\\n  setUserFromAPI: () => Promise<void>;\\n}\\n\\nexport type IUseAuth = AuthState & AuthActions;\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/api/hooks/auth/userManagement.ts\\n================================================\\nimport { useEffect } from \\'react\\';\\nimport { IUser } from \\'src/types\\';\\n\\nimport { useApi } from \\'../api\\';\\nimport { useAuthState } from \\'./state\\';\\n\\nexport const useUserManagement = () => {\\n  const { user, setUser } = useAuthState();\\n\\n  const {\\n    data: userData,\\n    error,\\n    isLoading,\\n    mutate: setUserFromAPI\\n  } = useApi<IUser>(\\'/user\\');\\n\\n  useEffect(() => {\\n    if (userData) {\\n      setUser(userData);\\n    } else if (isLoading) {\\n      setUser(undefined);\\n    }\\n  }, [userData, isLoading, setUser]);\\n\\n  useEffect(() => {\\n    if (error) {\\n      setUser(null);\\n    }\\n  }, [error]);\\n\\n  return { user, setUserFromAPI };\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/action.ts\\n================================================\\nexport interface IAction {\\n  label: string;\\n  forId: string;\\n  id: string;\\n  payload: Record<string, unknown>;\\n  name: string;\\n  onClick: () => void;\\n  tooltip: string;\\n  icon?: string;\\n}\\n\\nexport interface ICallFn {\\n  callback: (payload: Record<string, any>) => void;\\n  name: string;\\n  args: Record<string, any>;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/audio.ts\\n================================================\\nexport interface OutputAudioChunk {\\n  track: string;\\n  mimeType: string;\\n  data: Int16Array;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/command.ts\\n================================================\\nexport interface ICommand {\\n  id: string;\\n  icon: string;\\n  description: string;\\n  button?: boolean;\\n  persistent?: boolean;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/config.ts\\n================================================\\nexport interface IStarter {\\n  label: string;\\n  message: string;\\n  icon?: string;\\n}\\n\\nexport interface ChatProfile {\\n  default: boolean;\\n  icon?: string;\\n  name: string;\\n  markdown_description: string;\\n  starters?: IStarter[];\\n}\\n\\nexport interface IAudioConfig {\\n  enabled: boolean;\\n  sample_rate: number;\\n}\\n\\nexport interface IAuthConfig {\\n  requireLogin: boolean;\\n  passwordAuth: boolean;\\n  headerAuth: boolean;\\n  oauthProviders: string[];\\n  default_theme?: \\'light\\' | \\'dark\\';\\n  ui?: IChainlitConfig[\\'ui\\'];\\n}\\n\\nexport interface IChainlitConfig {\\n  markdown?: string;\\n  ui: {\\n    name: string;\\n    description?: string;\\n    font_family?: string;\\n    default_theme?: \\'light\\' | \\'dark\\';\\n    layout?: \\'default\\' | \\'wide\\';\\n    default_sidebar_state?: \\'open\\' | \\'closed\\';\\n    cot: \\'hidden\\' | \\'tool_call\\' | \\'full\\';\\n    github?: string;\\n    custom_css?: string;\\n    custom_js?: string;\\n    custom_font?: string;\\n    login_page_image?: string;\\n    login_page_image_filter?: string;\\n    login_page_image_dark_filter?: string;\\n    custom_meta_image_url?: string;\\n    header_links?: { name: string; icon_url: string; url: string }[];\\n  };\\n  features: {\\n    spontaneous_file_upload?: {\\n      enabled?: boolean;\\n      max_size_mb?: number;\\n      max_files?: number;\\n      accept?: string[] | Record<string, string[]>;\\n    };\\n    audio: IAudioConfig;\\n    unsafe_allow_html?: boolean;\\n    user_message_autoscroll?: boolean;\\n    latex?: boolean;\\n    edit_message?: boolean;\\n    mcp?: {\\n      enabled?: boolean;\\n      sse?: {\\n        enabled?: boolean;\\n      };\\n      stdio?: {\\n        enabled?: boolean;\\n      };\\n    };\\n  };\\n  debugUrl?: string;\\n  userEnv: string[];\\n  dataPersistence: boolean;\\n  threadResumable: boolean;\\n  chatProfiles: ChatProfile[];\\n  starters?: IStarter[];\\n  translation: object;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/element.ts\\n================================================\\nexport type IElement =\\n  | IImageElement\\n  | ITextElement\\n  | IPdfElement\\n  | ITasklistElement\\n  | IAudioElement\\n  | IVideoElement\\n  | IFileElement\\n  | IPlotlyElement\\n  | IDataframeElement\\n  | ICustomElement;\\n\\nexport type IMessageElement =\\n  | IImageElement\\n  | ITextElement\\n  | IPdfElement\\n  | IAudioElement\\n  | IVideoElement\\n  | IFileElement\\n  | IPlotlyElement\\n  | IDataframeElement\\n  | ICustomElement;\\n\\nexport type ElementType = IElement[\\'type\\'];\\nexport type IElementSize = \\'small\\' | \\'medium\\' | \\'large\\';\\n\\ninterface TElement<T> {\\n  id: string;\\n  type: T;\\n  threadId?: string;\\n  forId: string;\\n  mime?: string;\\n  url?: string;\\n  chainlitKey?: string;\\n}\\n\\ninterface TMessageElement<T> extends TElement<T> {\\n  name: string;\\n  display: \\'inline\\' | \\'side\\' | \\'page\\';\\n}\\n\\nexport interface IImageElement extends TMessageElement<\\'image\\'> {\\n  size?: IElementSize;\\n}\\n\\nexport interface ITextElement extends TMessageElement<\\'text\\'> {\\n  language?: string;\\n}\\n\\nexport interface IPdfElement extends TMessageElement<\\'pdf\\'> {\\n  page?: number;\\n}\\n\\nexport interface IAudioElement extends TMessageElement<\\'audio\\'> {\\n  autoPlay?: boolean;\\n}\\n\\nexport interface IVideoElement extends TMessageElement<\\'video\\'> {\\n  size?: IElementSize;\\n\\n  /**\\n   * Override settings for each type of player in ReactPlayer\\n   * https://github.com/cookpete/react-player?tab=readme-ov-file#config-prop\\n   * @type {object}\\n   */\\n  playerConfig?: object;\\n}\\n\\nexport interface IFileElement extends TMessageElement<\\'file\\'> {\\n  type: \\'file\\';\\n}\\n\\nexport type IPlotlyElement = TMessageElement<\\'plotly\\'>;\\n\\nexport type ITasklistElement = TElement<\\'tasklist\\'>;\\n\\nexport type IDataframeElement = TMessageElement<\\'dataframe\\'>;\\n\\nexport interface ICustomElement extends TMessageElement<\\'custom\\'> {\\n  props: Record<string, unknown>;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/feedback.ts\\n================================================\\nexport interface IFeedback {\\n  id?: string;\\n  forId?: string;\\n  threadId?: string;\\n  comment?: string;\\n  value: number;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/file.ts\\n================================================\\nimport { IAction } from \\'./action\\';\\nimport { IStep } from \\'./step\\';\\n\\nexport interface FileSpec {\\n  accept?: string[] | Record<string, string[]>;\\n  max_size_mb?: number;\\n  max_files?: number;\\n}\\n\\nexport interface ActionSpec {\\n  keys?: string[];\\n}\\n\\nexport interface IFileRef {\\n  id: string;\\n}\\n\\nexport interface IAsk {\\n  callback: (payload: IStep | IFileRef[] | IAction) => void;\\n  spec: {\\n    type: \\'text\\' | \\'file\\' | \\'action\\';\\n    step_id: string;\\n    timeout: number;\\n  } & FileSpec &\\n    ActionSpec;\\n  parentId?: string;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/history.ts\\n================================================\\nimport { IThread } from \\'src/types\\';\\n\\nimport { IPageInfo } from \\'..\\';\\n\\nexport type UserInput = {\\n  content: string;\\n  createdAt: number;\\n};\\n\\nexport type ThreadHistory = {\\n  threads?: IThread[];\\n  currentThreadId?: string;\\n  timeGroupedThreads?: { [key: string]: IThread[] };\\n  pageInfo?: IPageInfo;\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/index.ts\\n================================================\\nexport * from \\'./action\\';\\nexport * from \\'./element\\';\\nexport * from \\'./command\\';\\nexport * from \\'./file\\';\\nexport * from \\'./feedback\\';\\nexport * from \\'./step\\';\\nexport * from \\'./user\\';\\nexport * from \\'./thread\\';\\nexport * from \\'./history\\';\\nexport * from \\'./config\\';\\nexport * from \\'./mcp\\';\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/mcp.ts\\n================================================\\nexport interface IMcp {\\n  name: string;\\n  tools: [{ name: string }];\\n  status: \\'connected\\' | \\'connecting\\' | \\'failed\\';\\n  clientType: \\'sse\\' | \\'stdio\\';\\n  command?: string;\\n  url?: string;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/step.ts\\n================================================\\nimport { IFeedback } from \\'./feedback\\';\\n\\ntype StepType =\\n  | \\'assistant_message\\'\\n  | \\'user_message\\'\\n  | \\'system_message\\'\\n  | \\'run\\'\\n  | \\'tool\\'\\n  | \\'llm\\'\\n  | \\'embedding\\'\\n  | \\'retrieval\\'\\n  | \\'rerank\\'\\n  | \\'undefined\\';\\n\\nexport interface IStep {\\n  id: string;\\n  name: string;\\n  type: StepType;\\n  threadId?: string;\\n  parentId?: string;\\n  isError?: boolean;\\n  command?: string;\\n  showInput?: boolean | string;\\n  waitForAnswer?: boolean;\\n  input?: string;\\n  output: string;\\n  createdAt: number | string;\\n  start?: number | string;\\n  end?: number | string;\\n  feedback?: IFeedback;\\n  language?: string;\\n  defaultOpen?: boolean;\\n  streaming?: boolean;\\n  steps?: IStep[];\\n  metadata?: Record<string, any>;\\n  //legacy\\n  indent?: number;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/thread.ts\\n================================================\\nimport { IElement } from \\'./element\\';\\nimport { IStep } from \\'./step\\';\\n\\nexport interface IThread {\\n  id: string;\\n  createdAt: number | string;\\n  name?: string;\\n  userId?: string;\\n  userIdentifier?: string;\\n  metadata?: Record<string, any>;\\n  steps: IStep[];\\n  elements?: IElement[];\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/types/user.ts\\n================================================\\nexport type AuthProvider =\\n  | \\'credentials\\'\\n  | \\'header\\'\\n  | \\'github\\'\\n  | \\'google\\'\\n  | \\'azure-ad\\'\\n  | \\'azure-ad-hybrid\\';\\n\\nexport interface IUserMetadata extends Record<string, any> {\\n  tags?: string[];\\n  image?: string;\\n  provider?: AuthProvider;\\n}\\n\\nexport interface IUser {\\n  id: string;\\n  identifier: string;\\n  display_name?: string;\\n  metadata: IUserMetadata;\\n}\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/utils/group.ts\\n================================================\\nimport { IThread } from \\'src/types\\';\\n\\nexport const groupByDate = (data: IThread[]) => {\\n  const groupedData: { [key: string]: IThread[] } = {};\\n\\n  const today = new Date();\\n  today.setHours(0, 0, 0, 0);\\n\\n  [...data]\\n    .sort(\\n      (a, b) =>\\n        new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()\\n    )\\n    .forEach((item) => {\\n      const threadDate = new Date(item.createdAt);\\n      threadDate.setHours(0, 0, 0, 0);\\n\\n      const daysDiff = Math.floor(\\n        (today.getTime() - threadDate.getTime()) / 86400000\\n      );\\n\\n      let category: string;\\n      if (daysDiff === 0) {\\n        category = \\'Today\\';\\n      } else if (daysDiff === 1) {\\n        category = \\'Yesterday\\';\\n      } else if (daysDiff <= 7) {\\n        category = \\'Previous 7 days\\';\\n      } else if (daysDiff <= 30) {\\n        category = \\'Previous 30 days\\';\\n      } else {\\n        category = threadDate.toLocaleString(\\'default\\', {\\n          month: \\'long\\',\\n          year: \\'numeric\\'\\n        });\\n      }\\n\\n      groupedData[category] ??= [];\\n      groupedData[category].push(item);\\n    });\\n\\n  return groupedData;\\n};\\n\\n\\n\\n================================================\\nFile: libs/react-client/src/utils/message.ts\\n================================================\\nimport { isEqual } from \\'lodash\\';\\n\\nimport { IStep } from \\'..\\';\\n\\nconst nestMessages = (messages: IStep[]): IStep[] => {\\n  let nestedMessages: IStep[] = [];\\n\\n  for (const message of messages) {\\n    nestedMessages = addMessage(nestedMessages, message);\\n  }\\n\\n  return nestedMessages;\\n};\\n\\nconst isLastMessage = (messages: IStep[], index: number) => {\\n  if (messages.length - 1 === index) {\\n    return true;\\n  }\\n\\n  for (let i = index + 1; i < messages.length; i++) {\\n    if (messages[i].streaming) {\\n      continue;\\n    } else {\\n      return false;\\n    }\\n  }\\n\\n  return true;\\n};\\n\\n// Nested messages utils\\n\\nconst addMessage = (messages: IStep[], message: IStep): IStep[] => {\\n  if (hasMessageById(messages, message.id)) {\\n    return updateMessageById(messages, message.id, message);\\n  } else if (\\'parentId\\' in message && message.parentId) {\\n    return addMessageToParent(messages, message.parentId, message);\\n  } else if (\\'indent\\' in message && message.indent && message.indent > 0) {\\n    return addIndentMessage(messages, message.indent, message);\\n  } else {\\n    return [...messages, message];\\n  }\\n};\\n\\nconst addIndentMessage = (\\n  messages: IStep[],\\n  indent: number,\\n  newMessage: IStep,\\n  currentIndentation: number = 0\\n): IStep[] => {\\n  const nextMessages = [...messages];\\n\\n  if (nextMessages.length === 0) {\\n    return [...nextMessages, newMessage];\\n  } else {\\n    const index = nextMessages.length - 1;\\n    const msg = nextMessages[index];\\n    msg.steps = msg.steps || [];\\n\\n    if (currentIndentation + 1 === indent) {\\n      msg.steps = [...msg.steps, newMessage];\\n      nextMessages[index] = { ...msg };\\n\\n      return nextMessages;\\n    } else {\\n      msg.steps = addIndentMessage(\\n        msg.steps,\\n        indent,\\n        newMessage,\\n        currentIndentation + 1\\n      );\\n\\n      nextMessages[index] = { ...msg };\\n      return nextMessages;\\n    }\\n  }\\n};\\n\\nconst addMessageToParent = (\\n  messages: IStep[],\\n  parentId: string,\\n  newMessage: IStep\\n): IStep[] => {\\n  const nextMessages = [...messages];\\n\\n  for (let index = 0; index < nextMessages.length; index++) {\\n    const msg = nextMessages[index];\\n\\n    if (isEqual(msg.id, parentId)) {\\n      msg.steps = msg.steps ? [...msg.steps, newMessage] : [newMessage];\\n      nextMessages[index] = { ...msg };\\n    } else if (hasMessageById(nextMessages, parentId) && msg.steps) {\\n      msg.steps = addMessageToParent(msg.steps, parentId, newMessage);\\n      nextMessages[index] = { ...msg };\\n    }\\n  }\\n\\n  return nextMessages;\\n};\\n\\nconst findMessageById = (\\n  messages: IStep[],\\n  messageId: string\\n): IStep | undefined => {\\n  for (const message of messages) {\\n    if (isEqual(message.id, messageId)) {\\n      return message;\\n    } else if (message.steps && message.steps.length > 0) {\\n      const foundMessage = findMessageById(message.steps, messageId);\\n      if (foundMessage) {\\n        return foundMessage;\\n      }\\n    }\\n  }\\n  return undefined;\\n};\\n\\nconst hasMessageById = (messages: IStep[], messageId: string): boolean => {\\n  return findMessageById(messages, messageId) !== undefined;\\n};\\n\\nconst updateMessageById = (\\n  messages: IStep[],\\n  messageId: string,\\n  updatedMessage: IStep\\n): IStep[] => {\\n  const nextMessages = [...messages];\\n\\n  for (let index = 0; index < nextMessages.length; index++) {\\n    const msg = nextMessages[index];\\n\\n    if (isEqual(msg.id, messageId)) {\\n      nextMessages[index] = { steps: msg.steps, ...updatedMessage };\\n    } else if (hasMessageById(nextMessages, messageId) && msg.steps) {\\n      msg.steps = updateMessageById(msg.steps, messageId, updatedMessage);\\n      nextMessages[index] = { ...msg };\\n    }\\n  }\\n\\n  return nextMessages;\\n};\\n\\nconst deleteMessageById = (messages: IStep[], messageId: string) => {\\n  let nextMessages = [...messages];\\n\\n  for (let index = 0; index < nextMessages.length; index++) {\\n    const msg = nextMessages[index];\\n\\n    if (msg.id === messageId) {\\n      nextMessages = [\\n        ...nextMessages.slice(0, index),\\n        ...nextMessages.slice(index + 1)\\n      ];\\n    } else if (hasMessageById(nextMessages, messageId) && msg.steps) {\\n      msg.steps = deleteMessageById(msg.steps, messageId);\\n      nextMessages[index] = { ...msg };\\n    }\\n  }\\n\\n  return nextMessages;\\n};\\n\\nconst updateMessageContentById = (\\n  messages: IStep[],\\n  messageId: number | string,\\n  updatedContent: string,\\n  isSequence: boolean,\\n  isInput: boolean\\n): IStep[] => {\\n  const nextMessages = [...messages];\\n  for (let index = 0; index < nextMessages.length; index++) {\\n    const msg = nextMessages[index];\\n\\n    if (isEqual(msg.id, messageId)) {\\n      if (\\'content\\' in msg && msg.content !== undefined) {\\n        if (isSequence) {\\n          msg.content = updatedContent;\\n        } else {\\n          msg.content += updatedContent;\\n        }\\n      } else if (isInput) {\\n        if (\\'input\\' in msg && msg.input !== undefined) {\\n          if (isSequence) {\\n            msg.input = updatedContent;\\n          } else {\\n            msg.input += updatedContent;\\n          }\\n        }\\n      } else {\\n        if (\\'output\\' in msg && msg.output !== undefined) {\\n          if (isSequence) {\\n            msg.output = updatedContent;\\n          } else {\\n            msg.output += updatedContent;\\n          }\\n        }\\n      }\\n\\n      nextMessages[index] = { ...msg };\\n    } else if (msg.steps) {\\n      msg.steps = updateMessageContentById(\\n        msg.steps,\\n        messageId,\\n        updatedContent,\\n        isSequence,\\n        isInput\\n      );\\n      nextMessages[index] = { ...msg };\\n    }\\n  }\\n\\n  return nextMessages;\\n};\\n\\nexport {\\n  addMessageToParent,\\n  addMessage,\\n  deleteMessageById,\\n  hasMessageById,\\n  isLastMessage,\\n  nestMessages,\\n  updateMessageById,\\n  updateMessageContentById\\n};\\n\\n\\n\\n</codebase>\\n<documentation>\\nDirectory structure:\\n└── chainlit-docs/\\n    ├── advanced-features/\\n    │   ├── ask-user.mdx\\n    │   ├── chat-profiles.mdx\\n    │   ├── chat-settings.mdx\\n    │   ├── mcp.mdx\\n    │   ├── multi-modal.mdx\\n    │   ├── streaming.mdx\\n    │   └── test-debug.mdx\\n    ├── api-reference/\\n    │   ├── action.mdx\\n    │   ├── author-rename.mdx\\n    │   ├── cache.mdx\\n    │   ├── chat-profiles.mdx\\n    │   ├── chat-settings.mdx\\n    │   ├── make-async.mdx\\n    │   ├── message.mdx\\n    │   ├── step-class.mdx\\n    │   ├── step-decorator.mdx\\n    │   ├── window-message.mdx\\n    │   ├── ask/\\n    │   │   ├── ask-for-action.mdx\\n    │   │   ├── ask-for-file.mdx\\n    │   │   └── ask-for-input.mdx\\n    │   ├── data-persistence/\\n    │   │   └── custom-data-layer.mdx\\n    │   ├── elements/\\n    │   │   ├── audio.mdx\\n    │   │   ├── custom.mdx\\n    │   │   ├── dataframe.mdx\\n    │   │   ├── file.mdx\\n    │   │   ├── image.mdx\\n    │   │   ├── pdf.mdx\\n    │   │   ├── plotly.mdx\\n    │   │   ├── pyplot.mdx\\n    │   │   ├── tasklist.mdx\\n    │   │   ├── text.mdx\\n    │   │   └── video.mdx\\n    │   ├── input-widgets/\\n    │   │   ├── select.mdx\\n    │   │   ├── slider.mdx\\n    │   │   ├── switch.mdx\\n    │   │   ├── tags.mdx\\n    │   │   └── textinput.mdx\\n    │   ├── integrations/\\n    │   │   ├── haystack.mdx\\n    │   │   ├── langchain.mdx\\n    │   │   └── llamaindex.mdx\\n    │   └── lifecycle-hooks/\\n    │       ├── on-audio-chunk.mdx\\n    │       ├── on-audio-end.mdx\\n    │       ├── on-chat-end.mdx\\n    │       ├── on-chat-resume.mdx\\n    │       ├── on-chat-start.mdx\\n    │       ├── on-logout.mdx\\n    │       └── on-message.mdx\\n    ├── authentication/\\n    │   ├── header.mdx\\n    │   ├── oauth.mdx\\n    │   ├── overview.mdx\\n    │   └── password.mdx\\n    ├── backend/\\n    │   ├── command-line.mdx\\n    │   ├── env-variables.mdx\\n    │   └── config/\\n    │       ├── features.mdx\\n    │       ├── overview.mdx\\n    │       ├── project.mdx\\n    │       └── ui.mdx\\n    ├── concepts/\\n    │   ├── action.mdx\\n    │   ├── chat-lifecycle.mdx\\n    │   ├── command.mdx\\n    │   ├── element.mdx\\n    │   ├── message.mdx\\n    │   ├── starters.mdx\\n    │   ├── step.mdx\\n    │   └── user-session.mdx\\n    ├── customisation/\\n    │   ├── avatars.mdx\\n    │   ├── custom-css.mdx\\n    │   ├── custom-js.mdx\\n    │   ├── custom-logo-and-favicon.mdx\\n    │   ├── overview.mdx\\n    │   ├── theme.mdx\\n    │   └── translation.mdx\\n    ├── data-layers/\\n    │   ├── official.mdx\\n    │   ├── overview.mdx\\n    ├── data-persistence/\\n    │   ├── feedback.mdx\\n    │   ├── history.mdx\\n    │   ├── overview.mdx\\n    │   └── tags-metadata.mdx\\n    ├── deploy/\\n    │   ├── copilot.mdx\\n    │   ├── discord.mdx\\n    │   ├── overview.mdx\\n    │   ├── slack.mdx\\n    │   ├── teams.mdx\\n    │   ├── webapp.mdx\\n    │   └── react/\\n    │       ├── additional-resources.mdx\\n    │       ├── installation-and-setup.mdx\\n    │       ├── overview.mdx\\n    │       └── usage.mdx\\n    ├── examples/\\n    │   ├── community.mdx\\n    │   ├── cookbook.mdx\\n    │   ├── openai-sql.mdx\\n    │   ├── qa.mdx\\n    │   └── security.mdx\\n    ├── get-started/\\n    │   ├── installation.mdx\\n    │   ├── overview.mdx\\n    │   └── pure-python.mdx\\n    ├── guides/\\n    │   ├── iframe.mdx\\n    │   ├── sync-async.mdx\\n    │   └── migration/\\n    │       ├── 1.0.500.mdx\\n    │       ├── 1.1.0.mdx\\n    │       ├── 1.1.300.mdx\\n    │       ├── 1.1.400.mdx\\n    │       ├── 1.1.404.mdx\\n    │       └── 2.0.0.mdx\\n    ├── images/\\n    │   ├── discord/\\n    │   ├── slack/\\n    │   └── teams/\\n    ├── integrations/\\n    │   ├── embedchain.mdx\\n    │   ├── fastapi.mdx\\n    │   ├── haystack.mdx\\n    │   ├── langchain.mdx\\n    │   ├── litellm.mdx\\n    │   ├── llama-index.mdx\\n    │   ├── message-based.mdx\\n    │   ├── mistralai.mdx\\n    │   └── openai.mdx\\n    ├── llmops/\\n    │   ├── enterprise.mdx\\n    │   └── literalai.mdx\\n    └── logo/\\n\\n\\nFiles Content:\\n\\n================================================\\nFile: advanced-features/ask-user.mdx\\n================================================\\n---\\ntitle: \"Ask User\"\\n---\\n\\nThe ask APIs prompt the user for input. Depending on the API, the user input can be a string, a file, or pick an action.\\n\\nUntil the user provides an input, both the UI and your code will be blocked.\\n\\n<Frame caption=\"Ask File example\">\\n  <img src=\"/images/ask-file.png\" />\\n</Frame>\\n\\n## Available Ask APIs\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Text Input\"\\n    icon=\"text\"\\n    color=\"#ea5a0c\"\\n    href=\"/api-reference/ask/ask-for-input\"\\n  >\\n    Ask the user for a string input.\\n  </Card>\\n  <Card\\n    title=\"File\"\\n    icon=\"file\"\\n    color=\"#0285c7\"\\n    href=\"/api-reference/ask/ask-for-file\"\\n  >\\n    Ask the user to upload a file.\\n  </Card>\\n  <Card\\n    title=\"Action\"\\n    icon=\"bolt\"\\n    color=\"#16a34a\"\\n    href=\"/api-reference/ask/ask-for-action\"\\n  >\\n    Ask the user to pick an action.\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: advanced-features/chat-profiles.mdx\\n================================================\\n---\\ntitle: \"Chat Profiles\"\\n---\\n\\nChat Profiles are useful if you want to let your users choose from a list of predefined configured assistants. For example, you can define a chat profile for a support chat, a sales chat, or a chat for a specific product.\\n\\n<Card\\n  title=\"Chat Profiles API\"\\n  color=\"F80061\"\\n  icon=\"comments\"\\n  href=\"/api-reference/chat-profiles\"\\n>\\n  Learn how to define chat profiles.\\n</Card>\\n\\n<Frame caption=\"Example of Chat Profiles\">\\n  <img src=\"/images/chat-profiles.png\" />\\n</Frame>\\n\\n\\n\\n================================================\\nFile: advanced-features/chat-settings.mdx\\n================================================\\n---\\ntitle: Chat Settings\\n---\\n\\nChat settings are useful to let each user configure their chat experience given a set of options.\\n\\n## How it works\\n\\nCheck the chat settings [API reference](/api-reference/chat-settings) to learn how to configure it.\\n\\n## Preview\\n\\nIf chat settings are set, a new button will appear in the chat bar.\\n\\n![OpenChatSettings](/images/open-chat-settings.png)\\n\\nClicking on this button will open the settings panel. All settings are editable by the user. Once settings are updated, an event is sent to the Chainlit server so the application can react to the update.\\n\\n<Frame caption=\"Chat Settings in Chainlit\">\\n  <img src=\"/images/chat-settings.png\" />\\n</Frame>\\n\\n## Example\\n\\nCheck out this example from the cookbook that uses this feature: https://github.com/Chainlit/cookbook/tree/main/image-gen\\n\\n\\n\\n================================================\\nFile: advanced-features/mcp.mdx\\n================================================\\n---\\ntitle: \"MCP Servers\"\\ndescription: Model Control Protocol (MCP) allows you to integrate external tool providers with your Chainlit application. This enables your AI models to use tools through standardized interfaces.\\n---\\n\\n## Overview\\n\\nMCP provides a mechanism for Chainlit applications to connect to either server-sent events (SSE) based services or command-line (stdio) based tools. Once connected, your application can discover available tools, execute them, and integrate their responses into your application\\'s flow.\\n\\n <Card\\n    title=\"Chainlit MCP Cookbook\"\\n    icon=\"github\"\\n    href=\"https://github.com/Chainlit/cookbook/tree/main/mcp\"\\n  >\\n    End to end cookbook example showcasing MCP tool calling with Claude.\\n  </Card>\\n\\n<Frame caption=\"Connect to an MCP server\">\\n  <video\\n    controls\\n    autoPlay\\n    loop\\n    muted\\n    src=\"https://github.com/user-attachments/assets/6119341f-fb5d-4c3f-9f10-735a74841fd6\"\\n  />\\n</Frame>\\n\\n### Contact us for Enterprise Ready MCP\\n\\nWe\\'re working with companies to create their MCP stacks, enabling AI agents to consume their data and context in standardized ways. Fill out this [form](https://docs.google.com/forms/d/e/1FAIpQLSdObSIeIFt4nHppZ6r2rIoEe-jZRo4CqxbmRKKgb-ZsSPONnQ/viewform?usp=dialog).\\n\\n## Connections Types\\n\\nChainlit supports two types of MCP connections:\\n\\n1. **SSE (Server-Sent Events)**: Connect to a remote service via HTTP\\n2. **stdio**: Execute a local command and communicate via standard I/O\\n\\n> ⚠️ **Security Warning**: The stdio connection type spawns actual subprocesses on the Chainlit server. Only use this with trusted commands in controlled environments. Ensure proper validation of user inputs to prevent command injection vulnerabilities.\\n\\n<Note>**Command Availability Warning**: When using the stdio connection type with commands like `npx` or `uvx`, these commands must be available on the Chainlit server where the application is running. The subprocess is executed on the server, not on the client machine.</Note>\\n\\n## Setup\\n\\n### 1. Register Connection Handlers\\n\\nTo use MCP in your Chainlit application, you need to implement the `on_mcp_connect` handler. The `on_mcp_disconnect` handler is optional but recommended for proper cleanup.\\n\\n```python\\nimport chainlit as cl\\nfrom mcp import ClientSession\\n\\n@cl.on_mcp_connect\\nasync def on_mcp_connect(connection, session: ClientSession):\\n    \"\"\"Called when an MCP connection is established\"\"\"\\n    # Your connection initialization code here\\n    # This handler is required for MCP to work\\n    \\n@cl.on_mcp_disconnect\\nasync def on_mcp_disconnect(name: str, session: ClientSession):\\n    \"\"\"Called when an MCP connection is terminated\"\"\"\\n    # Your cleanup code here\\n    # This handler is optional\\n```\\n\\n### 2. Client Configuration\\n\\nThe client needs to provide the connection details through the Chainlit interface. This includes:\\n\\n- Connection name (unique identifier)\\n- Client type (`sse` or `stdio`)\\n- For SSE: URL endpoint\\n- For stdio: Full command (e.g., `npx your-tool-package` or `uvx your-tool-package`)\\n\\n<Frame caption=\"Adding an MCP\">\\n  <img src=\"/images/add-mcp.png\" />\\n</Frame>\\n\\n## Working with MCP Connections\\n\\n### Retrieving Available Tools\\n\\nUpon connection, you can discover the available tools provided by the MCP service:\\n\\n```python\\n@cl.on_mcp_connect\\nasync def on_mcp(connection, session: ClientSession):\\n    # List available tools\\n    result = await session.list_tools()\\n    \\n    # Process tool metadata\\n    tools = [{\\n        \"name\": t.name,\\n        \"description\": t.description,\\n        \"input_schema\": t.inputSchema,\\n    } for t in result.tools]\\n    \\n    # Store tools for later use\\n    mcp_tools = cl.user_session.get(\"mcp_tools\", {})\\n    mcp_tools[connection.name] = tools\\n    cl.user_session.set(\"mcp_tools\", mcp_tools)\\n```\\n\\n### Executing Tools\\n\\nYou can execute tools using the MCP session:\\n\\n```python\\n@cl.step(type=\"tool\") \\nasync def call_tool(tool_use):\\n    tool_name = tool_use.name\\n    tool_input = tool_use.input\\n    \\n    # Find appropriate MCP connection for this tool\\n    mcp_name = find_mcp_for_tool(tool_name)\\n    \\n    # Get the MCP session\\n    mcp_session, _ = cl.context.session.mcp_sessions.get(mcp_name)\\n    \\n    # Call the tool\\n    result = await mcp_session.call_tool(tool_name, tool_input)\\n    \\n    return result\\n```\\n\\n## Integrating with LLMs\\n\\nMCP tools can be seamlessly integrated with LLMs that support tool calling:\\n\\n```python\\nasync def call_model_with_tools():\\n    # Get tools from all MCP connections\\n    mcp_tools = cl.user_session.get(\"mcp_tools\", {})\\n    all_tools = [tool for connection_tools in mcp_tools.values() for tool in connection_tools]\\n    \\n    # Call your LLM with the tools\\n    response = await your_llm_client.call(\\n        messages=messages,\\n        tools=all_tools\\n    )\\n    \\n    # Handle tool calls if needed\\n    if response.has_tool_calls():\\n        # Process tool calls\\n        pass\\n        \\n    return response\\n```\\n\\n## Session Management\\n\\nMCP connections are managed at the session level. Each WebSocket session can have multiple named MCP connections. The connections are cleaned up when:\\n\\n1. The user explicitly disconnects\\n2. The same connection name is reused (old connection is replaced)\\n3. The WebSocket session ends\\n\\n\\n================================================\\nFile: advanced-features/multi-modal.mdx\\n================================================\\n---\\ntitle: \"Multi-Modality\"\\n---\\n\\nThe term \\'Multi-Modal\\' refers to the ability to support more than just text, encompassing images, videos, audio and files.\\n\\n## Voice Assistant\\n\\nChainlit let\\'s you access the user\\'s microphone audio stream and process it in real-time. This can be used to create voice assistants, transcribe audio, or even process audio in real-time.\\n\\n<Note>\\n  The user will only be able to use the microphone if you implemented the\\n  [@cl.on_audio_chunk](/api-reference/lifecycle-hooks/on-audio-chunk) decorator.\\n</Note>\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"OpenAI Realtime\"\\n    icon=\"microphone-lines\"\\n    color=\"#0285c7\"\\n    href=\"https://github.com/Chainlit/cookbook/tree/main/realtime-assistant\"\\n  >\\n    Cookbook example showcasing how to use Chainlit with realtime audio APIs.\\n  </Card>\\n  <Card\\n    title=\"Text To Speech -> Speech to Text\"\\n    color=\"F80061\"\\n    icon=\"microphone\"\\n    href=\"https://github.com/Chainlit/cookbook/blob/main/openai-whisper/app.py\"\\n  >\\n    Cookbook example showcasing speech to text -> answer generation -> text to speech.\\n  </Card>\\n</CardGroup>\\n\\n<Frame caption=\"OpenAI Realtime Example\">\\n  <video\\n    controls\\n    loop\\n    src=\"/images/openai-realtime.mp4\"\\n  />\\n</Frame>\\n\\n## Spontaneous File Uploads\\n\\nWithin the Chainlit application, users have the flexibility to attach any file to their messages. This can be achieved either by utilizing the drag and drop feature or by clicking on the `attach` button located in the chat bar.\\n\\n<Frame caption=\"Attach files to a message\">\\n  <img src=\"/images/multi-modal.gif\" />\\n</Frame>\\n\\nAs a developer, you have the capability to access these attached files through the [cl.on_message](/api-reference/lifecycle-hooks/on-message) decorated function.\\n\\n```py\\nimport chainlit as cl\\n\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    if not msg.elements:\\n        await cl.Message(content=\"No file attached\").send()\\n        return\\n\\n    # Processing images exclusively\\n    images = [file for file in msg.elements if \"image\" in file.mime]\\n\\n    # Read the first image\\n    with open(images[0].path, \"r\") as f:\\n        pass\\n\\n    await cl.Message(content=f\"Received {len(images)} image(s)\").send()\\n\\n```\\n\\n### Disabling Spontaneous File Uploads\\n\\nIf you wish to disable this feature (which would prevent users from attaching files to their messages), you can do so by setting `features.spontaneous_file_upload.enabled=false` in your Chainlit [config](/backend/config/features) file.\\n\\n\\n\\n================================================\\nFile: advanced-features/streaming.mdx\\n================================================\\n---\\ntitle: Streaming\\n---\\n\\nChainlit supports streaming for both [Message](/concepts/message) and [Step](/concepts/step). Here is an example with `openai`.\\n\\n<Frame caption=\"Streaming OpenAI response\">\\n  <img src=\"/images/streaming.gif\" />\\n</Frame>\\n\\n```python\\nfrom openai import AsyncOpenAI\\nimport chainlit as cl\\n\\nclient = AsyncOpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\\n\\n\\nsettings = {\\n    \"model\": \"gpt-3.5-turbo\",\\n    \"temperature\": 0.7,\\n    \"max_tokens\": 500,\\n    \"top_p\": 1,\\n    \"frequency_penalty\": 0,\\n    \"presence_penalty\": 0,\\n}\\n\\n\\n@cl.on_chat_start\\ndef start_chat():\\n    cl.user_session.set(\\n        \"message_history\",\\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}],\\n    )\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    message_history = cl.user_session.get(\"message_history\")\\n    message_history.append({\"role\": \"user\", \"content\": message.content})\\n\\n    msg = cl.Message(content=\"\")\\n\\n    stream = await client.chat.completions.create(\\n        messages=message_history, stream=True, **settings\\n    )\\n\\n    async for part in stream:\\n        if token := part.choices[0].delta.content or \"\":\\n            await msg.stream_token(token)\\n\\n    message_history.append({\"role\": \"assistant\", \"content\": msg.content})\\n    await msg.update()\\n```\\n\\n## Integrations\\n\\nStreaming is also supported at a higher level for some integrations.\\n\\nFor example, to use streaming with Langchain just pass `streaming=True` when instantiating the LLM:\\n\\n```python\\nllm = OpenAI(temperature=0, streaming=True)\\n```\\n\\nAlso make sure to pass a [callback handler](/api-reference/integrations/langchain) to your chain or agent run.\\n\\nSee [here](/api-reference/integrations/langchain#final-answer-streaming) for final answer streaming.\\n\\n\\n\\n================================================\\nFile: advanced-features/test-debug.mdx\\n================================================\\n---\\ntitle: \"Testing & Debugging\"\\n---\\n\\nTo test or debug your application files and decorated functions, you will need to provide the Chainlit context to your test suite. \\n\\nIn your main application script or test files add:\\n\\n```\\nif __name__ == \"__main__\":\\n    from chainlit.cli import run_chainlit\\n    run_chainlit(__file__)\\n```\\n\\nThen run the script from your IDE in debug mode. \\n\\n\\n\\n\\n================================================\\nFile: api-reference/action.mdx\\n================================================\\n---\\ntitle: \"Action\"\\n---\\n\\nThe `Action` class is designed to create and manage actions to be sent and displayed in the chatbot user interface. Actions consist of buttons that the user can interact with, and these interactions trigger specific functionalities within your app.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  Name of the action, this should match the action callback.\\n</ParamField>\\n\\n<ParamField path=\"payload\" type=\"Dict\">\\n  The payload associated with the action.\\n</ParamField>\\n\\n<ParamField path=\"icon\" type=\"str\" optional>\\n  The lucide icon name for the action button. See https://lucide.dev/icons/.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\" optional>\\n  The label of the action. This is what the user will see. If no label and no icon is provided, the name is display as a fallback.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The description of the action. This is what the user will see when they hover\\n  the action.\\n</ParamField>\\n\\n## Usage\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.action_callback(\"action_button\")\\nasync def on_action(action):\\n    await cl.Message(content=f\"Executed {action.name}\").send()\\n    # Optionally remove the action button from the chatbot user interface\\n    await action.remove()\\n\\n@cl.on_chat_start\\nasync def start():\\n    # Sending an action button within a chatbot message\\n    actions = [\\n        cl.Action(name=\"action_button\", payload={\"value\": \"example_value\"}, label=\"Click me!\")\\n    ]\\n\\n    await cl.Message(content=\"Interact with this action button:\", actions=actions).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/author-rename.mdx\\n================================================\\n---\\ntitle: \"author_rename and Message author\"\\n---\\n\\nThis documentation covers two methods for setting or renaming the author of a message to display more friendly author names in the UI: the `author_rename` decorator and the Message author specification at message creation.\\n\\n## Method 1: author_rename\\n\\nUseful for renaming the author of a message dynamically during the message handling process.\\n\\n## Parameters\\n\\n<ParamField path=\"orig_author\" type=\"str\" required>\\n  The original author name.\\n</ParamField>\\n\\n## Returns\\n\\n<ResponseField name=\"author\" type=\"str\" required>\\n  The renamed author\\n</ResponseField>\\n\\n## Usage\\n\\n```python\\nfrom langchain import OpenAI, LLMMathChain\\nimport chainlit as cl\\n\\n\\n@cl.author_rename\\ndef rename(orig_author: str):\\n    rename_dict = {\"LLMMathChain\": \"Albert Einstein\", \"Chatbot\": \"Assistant\"}\\n    return rename_dict.get(orig_author, orig_author)\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    llm = OpenAI(temperature=0)\\n    llm_math = LLMMathChain.from_llm(llm=llm)\\n    res = await llm_math.acall(message.content, callbacks=[cl.AsyncLangchainCallbackHandler()])\\n\\n    await cl.Message(content=\"Hello\").send()\\n```\\n\\n\\n## Method 2: Message author\\n\\nAllows for naming the author of a message at the moment of the message creation.\\n\\n### Usage\\n\\nYou can specify the author directly when creating a new message object:\\n\\n```python\\nfrom langchain import OpenAI, LLMMathChain\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    llm = OpenAI(temperature=0)\\n    llm_math = LLMMathChain.from_llm(llm=llm)\\n    res = await llm_math.acall(message.content, callbacks=[cl.AsyncLangchainCallbackHandler()])\\n\\n    # Specify the author at message creation\\n    response_message = cl.Message(content=\"Hello\", author=\"NewChatBotName\")\\n    await response_message.send()\\n```\\n\\n\\n\\n\\n\\n\\n================================================\\nFile: api-reference/cache.mdx\\n================================================\\n---\\ntitle: \"cache\"\\n---\\n\\nThe `cache` decorator is a tool for caching results of resource-intensive calculations or loading processes. It can be conveniently combined with the [file watcher](/backend/command-line) to prevent resource reloading each time the application restarts. This not only saves time, but also enhances overall efficiency.\\n\\n## Parameters\\n\\n<ParamField path=\"func\" type=\"Callable\">\\n  The target function whose results need to be cached.\\n</ParamField>\\n\\n## Returns\\n\\n<ResponseField name=\"cached_value\" type=\"Any\" required>\\n  The computed value that is stored in the cache after its initial calculation.\\n</ResponseField>\\n\\n## Usage\\n\\n```python\\nimport time\\nimport chainlit as cl\\n\\n@cl.cache\\ndef to_cache():\\n    time.sleep(5)  # Simulate a time-consuming process\\n    return \"Hello!\"\\n\\nvalue = to_cache()\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    await cl.Message(\\n        content=value,\\n    ).send()\\n```\\n\\nIn this example, the `to_cache` function simulates a time-consuming process that returns a value. By using the `cl.cache` decorator, the result of the function is cached after its first execution. Future calls to the `to_cache` function return the cached value without running the time-consuming process again.\\n\\n\\n\\n================================================\\nFile: api-reference/chat-profiles.mdx\\n================================================\\n---\\ntitle: \"Chat Profiles\"\\n---\\n\\nDecorator to define the list of chat profiles.\\n\\nIf authentication is enabled, you can access the user details to create the list of chat profiles conditionally.\\n\\nThe icon is optional.\\n\\n## Parameters\\n\\n<ParamField path=\"current_user\" type=\"User\">\\n  The message coming from the UI.\\n</ParamField>\\n\\n## Usage\\n\\n```python Simple example\\nimport chainlit as cl\\n\\n\\n@cl.set_chat_profiles\\nasync def chat_profile():\\n    return [\\n        cl.ChatProfile(\\n            name=\"GPT-3.5\",\\n            markdown_description=\"The underlying LLM model is **GPT-3.5**.\",\\n            icon=\"https://picsum.photos/200\",\\n        ),\\n        cl.ChatProfile(\\n            name=\"GPT-4\",\\n            markdown_description=\"The underlying LLM model is **GPT-4**.\",\\n            icon=\"https://picsum.photos/250\",\\n        ),\\n    ]\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    chat_profile = cl.user_session.get(\"chat_profile\")\\n    await cl.Message(\\n        content=f\"starting chat using the {chat_profile} chat profile\"\\n    ).send()\\n```\\n\\n```python With authentication\\nfrom typing import Optional\\n\\nimport chainlit as cl\\n\\n\\n@cl.set_chat_profiles\\nasync def chat_profile(current_user: cl.User):\\n    if current_user.metadata[\"role\"] != \"ADMIN\":\\n        return None\\n\\n    return [\\n        cl.ChatProfile(\\n            name=\"GPT-3.5\",\\n            markdown_description=\"The underlying LLM model is **GPT-3.5**, a *175B parameter model* trained on 410GB of text data.\",\\n        ),\\n        cl.ChatProfile(\\n            name=\"GPT-4\",\\n            markdown_description=\"The underlying LLM model is **GPT-4**, a *1.5T parameter model* trained on 3.5TB of text data.\",\\n            icon=\"https://picsum.photos/250\",\\n        ),\\n        cl.ChatProfile(\\n            name=\"GPT-5\",\\n            markdown_description=\"The underlying LLM model is **GPT-5**.\",\\n            icon=\"https://picsum.photos/200\",\\n        ),\\n    ]\\n\\n\\n@cl.password_auth_callback\\ndef auth_callback(username: str, password: str) -> Optional[cl.User]:\\n    if (username, password) == (\"admin\", \"admin\"):\\n        return cl.User(identifier=\"admin\", metadata={\"role\": \"ADMIN\"})\\n    else:\\n        return None\\n\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    user = cl.user_session.get(\"user\")\\n    chat_profile = cl.user_session.get(\"chat_profile\")\\n    await cl.Message(\\n        content=f\"starting chat with {user.identifier} using the {chat_profile} chat profile\"\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/chat-settings.mdx\\n================================================\\n---\\ntitle: \"Chat Settings\"\\n---\\n\\nThe `ChatSettings` class is designed to create and send a dynamic form to the UI. This form can be updated by the user.\\n\\n## Attributes\\n\\n<ParamField path=\"inputs\" type=\"List[InputWidget]\">\\n  The fields of the form\\n</ParamField>\\n\\n## Usage\\n\\n```python\\nimport chainlit as cl\\nfrom chainlit.input_widget import Select, Switch, Slider\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            Select(\\n                id=\"Model\",\\n                label=\"OpenAI - Model\",\\n                values=[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-4\", \"gpt-4-32k\"],\\n                initial_index=0,\\n            ),\\n            Switch(id=\"Streaming\", label=\"OpenAI - Stream Tokens\", initial=True),\\n            Slider(\\n                id=\"Temperature\",\\n                label=\"OpenAI - Temperature\",\\n                initial=1,\\n                min=0,\\n                max=2,\\n                step=0.1,\\n            ),\\n            Slider(\\n                id=\"SAI_Steps\",\\n                label=\"Stability AI - Steps\",\\n                initial=30,\\n                min=10,\\n                max=150,\\n                step=1,\\n                description=\"Amount of inference steps performed on image generation.\",\\n            ),\\n            Slider(\\n                id=\"SAI_Cfg_Scale\",\\n                label=\"Stability AI - Cfg_Scale\",\\n                initial=7,\\n                min=1,\\n                max=35,\\n                step=0.1,\\n                description=\"Influences how strongly your generation is guided to match your prompt.\",\\n            ),\\n            Slider(\\n                id=\"SAI_Width\",\\n                label=\"Stability AI - Image Width\",\\n                initial=512,\\n                min=256,\\n                max=2048,\\n                step=64,\\n                tooltip=\"Measured in pixels\",\\n            ),\\n            Slider(\\n                id=\"SAI_Height\",\\n                label=\"Stability AI - Image Height\",\\n                initial=512,\\n                min=256,\\n                max=2048,\\n                step=64,\\n                tooltip=\"Measured in pixels\",\\n            ),\\n        ]\\n    ).send()\\n\\n\\n@cl.on_settings_update\\nasync def setup_agent(settings):\\n    print(\"on_settings_update\", settings)\\n\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/make-async.mdx\\n================================================\\n---\\ntitle: \"make_async\"\\n---\\n\\nThe `make_async` function takes a synchronous function (for instance a LangChain agent) and returns an asynchronous function that will run the original function in a separate thread.\\nThis is useful to run long running synchronous tasks without blocking the event loop.\\n\\n## Parameters\\n\\n<ParamField path=\"func\" type=\"Callable\">\\n  The synchronous function to run in a separate thread.\\n</ParamField>\\n\\n## Returns\\n\\n<ResponseField name=\"async_function\" type=\"Coroutine\" required>\\n  The asynchronous function that will run the synchronous function in a separate\\n  thread.\\n</ResponseField>\\n\\n## Usage\\n\\n```python\\nimport time\\nimport chainlit as cl\\n\\ndef sync_func():\\n    time.sleep(5)\\n    return \"Hello!\"\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    answer = await cl.make_async(sync_func)()\\n    await cl.Message(\\n        content=answer,\\n    ).send()\\n```\\n\\n```python LangChain agent\\nimport chainlit as cl\\n\\nres = await cl.make_async(agent)(input_str, callbacks=[cl.LangchainCallbackHandler()])\\nawait cl.Message(content=res[\"text\"]).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/message.mdx\\n================================================\\n---\\ntitle: \"Message\"\\n---\\n\\nThe `Message` class is designed to send, stream, update or remove messages.\\n\\n## Parameters\\n\\n<ParamField path=\"content\" type=\"str\">\\n  The content of the message.\\n</ParamField>\\n<ParamField path=\"author\" type=\"str\" optional>\\n  The author of the message, defaults to the chatbot name defined in your config\\n  file.\\n</ParamField>\\n<ParamField path=\"elements\" type=\"Element[]\" optional>\\n  Elements to attach to the message.\\n</ParamField>\\n<ParamField path=\"actions\" type=\"Action[]\" optional>\\n  Actions to attach to the message.\\n</ParamField>\\n<ParamField path=\"language\" type=\"str\" optional>\\n  Language of the code if the content is code. See\\n  https://react-code-blocks-rajinwonderland.vercel.app/?path=/story/codeblock--supported-languages\\n  for a list of supported languages.\\n</ParamField>\\n\\n## Send a message\\n\\nSend a new message to the UI.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    await cl.Message(\\n        content=f\"Received: {message.content}\",\\n    ).send()\\n```\\n\\n## Stream a message\\n\\nSend a message token by token to the UI.\\n\\n```python\\nimport chainlit as cl\\n\\ntoken_list = [\"the\", \"quick\", \"brown\", \"fox\"]\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    msg = cl.Message(content=\"\")\\n    for token in token_list:\\n        await msg.stream_token(token)\\n\\n    await msg.send()\\n```\\n\\n## Update a message\\n\\nUpdate a message that already has been sent.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    msg = cl.Message(content=\"Hello!\")\\n    await msg.send()\\n\\n    await cl.sleep(2)\\n\\n    msg.content = \"Hello again!\"\\n    await msg.update()\\n```\\n\\n## Remove a message\\n\\nRemove a message from the UI.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    msg = cl.Message(content=\"Message 1\")\\n    await msg.send()\\n    await cl.sleep(2)\\n    await msg.remove()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/step-class.mdx\\n================================================\\n---\\ntitle: \"Step Class\"\\n---\\n\\nThe `Step` class is a Python Context Manager that can be used to create steps in your chainlit app. The step is created when the context manager is entered and is updated to the client when the context manager is exited.\\n\\n## Parameters\\n\\n<ParamField path=\"name\" type=\"str\" optional>\\n  The name of the step. Default to the name of the decorated function.\\n</ParamField>\\n<ParamField path=\"type\" type=\"Enum\" default=\"undefined\" optional>\\n  The type of the step, useful for monitoring and debugging.\\n</ParamField>\\n<ParamField path=\"elements\" type=\"List[Element]\" optional>\\n  Elements to attach to the step.\\n</ParamField>\\n<ParamField path=\"language\" type=\"str\" optional>\\n  Language of the output. See\\n  https://react-code-blocks-rajinwonderland.vercel.app/?path=/story/codeblock--supported-languages\\n  for a list of supported languages.\\n</ParamField>\\n<ParamField path=\"show_input\" type=\"Union[bool, str]\" default=\"False\" optional>\\n  By default only the output of the step is shown. Set this to `True` to also\\n  show the input. You can also set this to a language like `json` or `python` to\\n  syntax highlight the input.\\n</ParamField>\\n\\n## Send a Step\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def main():\\n    async with cl.Step(name=\"Test\") as step:\\n        # Step is sent as soon as the context manager is entered\\n        step.input = \"hello\"\\n        step.output = \"world\"\\n\\n    # Step is updated when the context manager is exited\\n```\\n\\n## Stream the Output\\n\\n```python\\nfrom openai import AsyncOpenAI\\n\\nimport chainlit as cl\\n\\nclient = AsyncOpenAI()\\n\\n@cl.on_message\\nasync def main(msg: cl.Message):\\n\\n    async with cl.Step(name=\"gpt4\", type=\"llm\") as step:\\n        step.input = msg.content\\n\\n        stream = await client.chat.completions.create(\\n            messages=[{\"role\": \"user\", \"content\": msg.content}],\\n            stream=True,\\n            model=\"gpt-4\",\\n            temperature=0,\\n        )\\n\\n        async for part in stream:\\n            delta = part.choices[0].delta\\n            if delta.content:\\n                # Stream the output of the step\\n                await step.stream_token(delta.content)\\n```\\n\\n## Nest Steps\\n\\nTo nest steps, simply create a step inside another step.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    async with cl.Step(name=\"Parent step\") as parent_step:\\n        parent_step.input = \"Parent step input\"\\n\\n        async with cl.Step(name=\"Child step\") as child_step:\\n            child_step.input = \"Child step input\"\\n            child_step.output = \"Child step output\"\\n\\n        parent_step.output = \"Parent step output\"\\n```\\n\\n## Update a Step\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    async with cl.Step(name=\"Parent step\") as step:\\n        step.input = \"Parent step input\"\\n        step.output = \"Parent step output\"\\n\\n    await cl.sleep(2)\\n\\n    step.output = \"Parent step output updated\"\\n    await step.update()\\n```\\n\\n## Remove a Step\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    async with cl.Step(name=\"Parent step\") as step:\\n        step.input = \"Parent step input\"\\n        step.output = \"Parent step output\"\\n\\n    await cl.sleep(2)\\n\\n    await step.remove()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/step-decorator.mdx\\n================================================\\n---\\ntitle: \"Step Decorator\"\\n---\\n\\nThe step decorator will log steps based on the decorated function. By default, the arguments of the function will be used as the input of the step and the return value will be used as the output.\\n\\nUnder the hood, the step decorator is using the [cl.Step](/api-reference/step-class) class.\\n\\n## Parameters\\n\\n<ParamField path=\"name\" type=\"str\" optional>\\n  The name of the step. Default to the name of the decorated function.\\n</ParamField>\\n<ParamField path=\"type\" type=\"str\" default=\"undefined\" optional>\\n  The type of the step, useful for monitoring and debugging.\\n</ParamField>\\n<ParamField path=\"language\" type=\"str\" optional>\\n  Language of the output. See\\n  https://react-code-blocks-rajinwonderland.vercel.app/?path=/story/codeblock--supported-languages\\n  for a list of supported languages.\\n</ParamField>\\n<ParamField path=\"show_input\" type=\"Union[bool, str]\" default=\"False\" optional>\\n  By default only the output of the step is shown. Set this to `True` to also\\n  show the input. You can also set this to a language like `json` or `python` to\\n  syntax highlight the input.\\n</ParamField>\\n\\n## Access the Current step\\n\\nYou can access the current step object using `cl.context.current_step` and override values.\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.step\\nasync def my_step():\\n    current_step = cl.context.current_step\\n\\n    # Override the input of the step\\n    current_step.input = \"My custom input\"\\n\\n    # Override the output of the step\\n    current_step.output = \"My custom output\"\\n```\\n\\n## Stream the Output\\n\\n```python\\nfrom openai import AsyncOpenAI\\n\\nimport chainlit as cl\\n\\nclient = AsyncOpenAI(api_key=\"YOUR_API_KEY\")\\n\\n@cl.step(type=\"llm\")\\nasync def gpt4():\\n    settings = {\\n        \"model\": \"gpt-4\",\\n        \"temperature\": 0,\\n    }\\n\\n    stream = await client.chat.completions.create(\\n        messages=message_history, stream=True, **settings\\n    )\\n\\n    current_step = cl.context.current_step\\n\\n    async for part in stream:\\n        delta = part.choices[0].delta\\n\\n        if delta.content:\\n            # Stream the output of the step\\n            await current_step.stream_token(delta.content)\\n```\\n\\n## Nest Steps\\n\\nIf another step decorated function is called inside the decorated function, the child step will be nested under the parent step.\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.step\\nasync def parent_step():\\n    await child_step()\\n    return \"Parent step output\"\\n\\n@cl.step\\nasync def child_step():\\n    return \"Child step output\"\\n\\n@cl.on_chat_start\\nasync def main():\\n    await parent_step()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/window-message.mdx\\n================================================\\n---\\ntitle: \"Window Messaging\"\\n---\\n\\n# on_window_message\\n\\nDecorator to react to messages coming from the Web App\\'s parent window.\\nThe decorated function is called every time a new window message is received.\\n\\n## Parameters\\n\\n<ParamField path=\"message\" type=\"str\">\\n  The message coming from the Web App\\'s parent window.\\n</ParamField>\\n\\n## Usage\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_window_message\\ndef main(message: str):\\n  # do something\\n```\\n\\n# send_window_message\\n\\nFunction to send messages to the Web App\\'s parent window.\\n\\n## Parameters\\n\\n<ParamField path=\"message\" type=\"str\">\\n  The message to send to the Web App\\'s parent window.\\n</ParamField>\\n\\n## Usage\\n\\n```python\\n@cl.on_message\\nasync def message():\\n  await cl.send_window_message(\"Server: Hello from Chainlit\")\\n```\\n\\n\\n================================================\\nFile: api-reference/ask/ask-for-action.mdx\\n================================================\\n---\\ntitle: \"AskUserAction\"\\n---\\n\\nAsk for the user to take an action before continuing.\\nIf the user does not answer in time (see timeout), a `TimeoutError` will be raised or `None` will be returned depending on `raise_on_timeout` parameter.\\nIf a project ID is configured, the messages will be uploaded to the cloud storage.\\n\\n### Attributes\\n\\n<ParamField path=\"content\" type=\"str\">\\n  The content of the message.\\n</ParamField>\\n<ParamField path=\"actions\" type=\"List[Action]\">\\n  The list of [Action](/api-reference/action) to prompt the user.\\n</ParamField>\\n<ParamField path=\"author\" type=\"str\" optional>\\n  The author of the message, defaults to the chatbot name defined in your\\n  config.\\n</ParamField>\\n<ParamField path=\"timeout\" type=\"int\" optional default={90}>\\n  The number of seconds to wait for an answer before raising a TimeoutError.\\n</ParamField>\\n<ParamField path=\"raise_on_timeout\" type=\"bool\" optional default=\"False\">\\n  Whether to raise a socketio TimeoutError if the user does not answer in time.\\n</ParamField>\\n\\n### Returns\\n\\n<ResponseField name=\"response\" type=\"AskActionResponse | None\" required>\\n  The response of the user.\\n</ResponseField>\\n\\n### Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    res = await cl.AskActionMessage(\\n        content=\"Pick an action!\",\\n        actions=[\\n            cl.Action(name=\"continue\", payload={\"value\": \"continue\"}, label=\"✅ Continue\"),\\n            cl.Action(name=\"cancel\", payload={\"value\": \"cancel\"}, label=\"❌ Cancel\"),\\n        ],\\n    ).send()\\n\\n    if res and res.get(\"payload\").get(\"value\") == \"continue\":\\n        await cl.Message(\\n            content=\"Continue!\",\\n        ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/ask/ask-for-file.mdx\\n================================================\\n---\\ntitle: \"AskFileMessage\"\\n---\\n\\nAsk the user to upload a file before continuing.\\nIf the user does not answer in time (see timeout), a TimeoutError will be raised or None will be returned depending on raise_on_timeout.\\nIf a project ID is configured, the messages will be uploaded to the cloud storage.\\n\\n### Attributes\\n\\n<ParamField path=\"content\" type=\"str\">\\n  Text displayed above the upload button.\\n</ParamField>\\n<ParamField path=\"accept\" type=\"Union[List[str], Dict[str, List[str]]]\">\\n  List of mime type to accept like `[\"text/csv\", \"application/pdf\"]` or a dict like `{\"text/plain\": [\".txt\", \".py\"]}`.\\n  More infos here https://react-dropzone.org/#!/Accepting%20specific%20file%20types.\\n</ParamField>\\n<ParamField path=\"max_size_mb\" type=\"int\" optional>\\n  Maximum file size in MB. Defaults to 2.\\n</ParamField>\\n<ParamField path=\"max_files\" type=\"int\" optional>\\n  Maximum number of files to upload. Defaults to 1. Maximum value is 10.\\n</ParamField>\\n<ParamField path=\"timeout\" type=\"int\" optional>\\n  The number of seconds to wait for an answer before raising a TimeoutError.\\n</ParamField>\\n<ParamField path=\"raise_on_timeout\" type=\"bool\" optional>\\n  Whether to raise a socketio TimeoutError if the user does not answer in time.\\n</ParamField>\\n\\n### Returns\\n\\n<ResponseField name=\"response\" type=\"List[AskFileResponse]\" required>\\n  The files uploaded by the user.\\n</ResponseField>\\n\\n### Example\\n\\n```python Ask for a text file\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    files = None\\n\\n    # Wait for the user to upload a file\\n    while files == None:\\n        files = await cl.AskFileMessage(\\n            content=\"Please upload a text file to begin!\", accept=[\"text/plain\"]\\n        ).send()\\n\\n    text_file = files[0]\\n\\n    with open(text_file.path, \"r\", encoding=\"utf-8\") as f:\\n        text = f.read()\\n\\n    # Let the user know that the system is ready\\n    await cl.Message(\\n        content=f\"`{text_file.name}` uploaded, it contains {len(text)} characters!\"\\n    ).send()\\n\\n```\\n\\nYou can also pass a dict to the `accept` parameter to precise the file extension for each mime type:\\n\\n```python Ask for a python file\\nimport chainlit as cl\\n\\nfile = await cl.AskFileMessage(\\n        content=\"Please upload a python file to begin!\", accept={\"text/plain\": [\".py\"]}\\n      ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/ask/ask-for-input.mdx\\n================================================\\n---\\ntitle: \"AskUserMessage\"\\n---\\n\\nAsk for the user input before continuing.\\nIf the user does not answer in time (see timeout), a TimeoutError will be raised or None will be returned depending on raise_on_timeout.\\nIf a project ID is configured, the messages will be uploaded to the cloud storage.\\n\\n### Attributes\\n\\n<ParamField path=\"content\" type=\"str\">\\n  The content of the message.\\n</ParamField>\\n<ParamField path=\"author\" type=\"str\" optional>\\n  The author of the message, defaults to the chatbot name defined in your\\n  config.\\n</ParamField>\\n<ParamField path=\"timeout\" type=\"int\" optional>\\n  The number of seconds to wait for an answer before raising a TimeoutError.\\n</ParamField>\\n<ParamField path=\"raise_on_timeout\" type=\"bool\" optional>\\n  Whether to raise a socketio TimeoutError if the user does not answer in time.\\n</ParamField>\\n\\n### Returns\\n\\n<ResponseField name=\"response\" type=\"Step\" required>\\n  The response of the user.\\n</ResponseField>\\n\\n### Usage\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    res = await cl.AskUserMessage(content=\"What is your name?\", timeout=10).send()\\n    if res:\\n        await cl.Message(\\n            content=f\"Your name is: {res[\\'output\\']}\",\\n        ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/data-persistence/custom-data-layer.mdx\\n================================================\\n---\\ntitle: Custom Data Layer\\n---\\n\\nThe `BaseDataLayer` class serves as an abstract foundation for data persistence operations within the Chainlit framework.\\nThis class outlines methods for managing users, feedback, elements, steps, and threads in a chatbot application.\\n\\n## Methods\\n\\n<ParamField path=\"async get_user(self, identifier: str)\" type=\"Coroutine\">\\n  Fetches a user by their identifier. Return type is optionally a `PersistedUser`.\\n</ParamField>\\n\\n<ParamField path=\"async create_user(self, user: User)\" type=\"Coroutine\">\\n  Creates a new user based on the `User` instance provided. Return type is optionally a `PersistedUser`.\\n</ParamField>\\n\\n<ParamField path=\"async upsert_feedback(self, feedback: Feedback)\" type=\"Coroutine\">\\n  Inserts or updates feedback. Accepts a `Feedback` instance and returns a string as an identifier of the persisted feedback.\\n</ParamField>\\n\\n<ParamField path=\"async delete_feedback(self, feedback_id: str)\" type=\"Coroutine\">\\n  Deletes a feedback by `feedback_id`. Return `True` if it was successful.\\n</ParamField>\\n\\n<ParamField path=\"async create_element(self, element_dict: ElementDict)\" type=\"Coroutine\" optional>\\n  Adds a new element to the data layer. Accepts `ElementDict` as an argument.\\n</ParamField>\\n\\n<ParamField path=\"async get_element(self, thread_id: str, element_id: str)\" type=\"Coroutine\">\\n  Retrieves an element by `thread_id` and `element_id`. Return type is optionally an `ElementDict`.\\n</ParamField>\\n\\n<ParamField path=\"async delete_element(self, element_id: str)\" type=\"Coroutine\" optional>\\n  Deletes an element given its identifier `element_id`.\\n</ParamField>\\n\\n<ParamField path=\"async create_step(self, step_dict: StepDict)\" type=\"Coroutine\" optional>\\n  Creates a new step in the data layer. Accepts `StepDict` as an argument.\\n</ParamField>\\n\\n<ParamField path=\"async update_step(self, step_dict: StepDict)\" type=\"Coroutine\" optional>\\n  Updates an existing step. Accepts `StepDict` as an argument.\\n</ParamField>\\n\\n<ParamField path=\"async delete_step(self, step_id: str)\" type=\"Coroutine\" optional>\\n  Deletes a step given its identifier `step_id`.\\n</ParamField>\\n\\n<ParamField path=\"async get_thread_author(self, thread_id: str)\" type=\"Coroutine\">\\n  Fetches the author of a given thread by `thread_id`. Returns a string representing the author identifier.\\n</ParamField>\\n\\n<ParamField path=\"async delete_thread(self, thread_id: str)\" type=\"Coroutine\">\\n  Deletes a thread given its identifier `thread_id`.\\n</ParamField>\\n\\n<ParamField path=\"async list_threads(self, pagination: Pagination, filters: ThreadFilter)\" type=\"Coroutine\">\\n  Lists threads based on `pagination` and `filters` arguments. Returns a `PaginatedResponse[ThreadDict]`.\\n</ParamField>\\n\\n<ParamField path=\"async get_thread(self, thread_id: str)\" type=\"Coroutine\">\\n  Retrieves a thread by its identifier `thread_id`. Return type is optionally a `ThreadDict`.\\n</ParamField>\\n\\n<ParamField path=\"async update_thread(self, thread_id: str, name: Optional[str] = None, user_id: Optional[str] = None, metadata: Optional[Dict] = None, tags: Optional[List[str]] = None)\" type=\"Coroutine\">\\n  Updates a thread\\'s details like name, user_id, metadata, and tags. Arguments are mostly optional.\\n</ParamField>\\n\\n<ParamField path=\"async delete_user_session(self, id: str)\" type=\"Coroutine\">\\n  Deletes a user session given its identifier `id`. Returns a boolean value indicating success.\\n</ParamField>\\n\\n## Decorators\\n\\n<ParamField path=\"queue_until_user_message()\" type=\"Decorator\">\\n  Queues certain methods to execute only after the first user message is received, especially useful for `WebsocketSessions`.\\n</ParamField>\\n\\n## Example\\n\\nDue to the abstract nature of `BaseDataLayer`, direct instantiation and usage are not practical without subclassing and implementing the abstract methods.\\n\\nYou can refer to the [guide for custom data layer implementation](/data-layers/overview).\\n\\n\\n================================================\\nFile: api-reference/elements/audio.mdx\\n================================================\\n---\\ntitle: \"Audio\"\\n---\\n\\nThe `Audio` class allows you to display an audio player for a specific audio file in the chatbot user interface.\\n\\nYou must provide either an url or a path or content bytes.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the audio file to be displayed in the UI. This is shown to users.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines where the element should be displayed in the UI. Choices are \"side\"\\n  (default), \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\" optional>\\n  The remote URL of the audio.\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the audio.\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"bytes\" optional>\\n  The file content of the audio in bytes format.\\n</ParamField>\\n\\n<ParamField path=\"auto_play\" type=\"bool\" optional>\\n  Whether the audio should start playing automatically.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    elements = [\\n        cl.Audio(name=\"example.mp3\", path=\"./example.mp3\", display=\"inline\"),\\n    ]\\n    await cl.Message(\\n        content=\"Here is an audio file\",\\n        elements=elements,\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/custom.mdx\\n================================================\\n---\\ntitle: \"Custom\"\\n---\\n\\nThe `CustomElement` class allows you to render a custom `.jsx` snippet. The `.jsx` file should be placed in `public/elements/ELEMENT_NAME.jsx`.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the custom Element. It should match the name of your JSX file (without the `.jsx` extension).\\n</ParamField>\\n\\n<ParamField path=\"props\" type=\"Dict\">\\n  The props to pass to the JSX.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" default=\"inline\" optional>\\n  Determines how the text element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n## How to Write the JSX file\\n\\n<Note>If you are not familiar with UI development, you can pass these instructions to an LLM to ask it to generate the `.jsx` for you!</Note>\\n\\nTo implement the `jsx` file for your Chainlit custom element, follow these instructions.\\n\\n### Component definition\\n\\nOnly write JSX code, no TSX. Each `.jsx` file should export default one component like:\\n\\n```jsx\\nexport default function MyComponent() {\\n    return <div>Hello World</div>\\n}\\n```\\n\\nThe component `props` are globally injected (not as a function argument). **NEVER** pass them as function argument.\\n\\n### Use Tailwind for Styling\\n\\nUnder the hood, the code will be rendered in a shadcn + tailwind environment.\\nThe theme is relying on CSS variables.\\n\\nHere is an example rendering a `div` with a primary color background and round border:\\n\\n```jsx\\nexport default function TailwindExample() {\\n    return <div className=\"bg-primary rounded-md h-4 w-full\" />\\n}\\n```\\n\\n### Only Use Allowed Imports\\n\\nOnly use available packages for imports. Here is the full list:\\n\\n- `react`\\n- `sonner`\\n- `zod`\\n- `recoil`\\n- `react-hook-form`\\n- `lucide-react`\\n- `@/components/ui/accordion`\\n- `@/components/ui/aspect-ratio`\\n- `@/components/ui/avatar`\\n- `@/components/ui/badge`\\n- `@/components/ui/button`\\n- `@/components/ui/card`\\n- `@/components/ui/carousel`\\n- `@/components/ui/checkbox`\\n- `@/components/ui/command`\\n- `@/components/ui/dialog`\\n- `@/components/ui/dropdown-menu`\\n- `@/components/ui/form`\\n- `@/components/ui/hover-card`\\n- `@/components/ui/input`\\n- `@/components/ui/label`\\n- `@/components/ui/pagination`\\n- `@/components/ui/popover`\\n- `@/components/ui/progress`\\n- `@/components/ui/scroll-area`\\n- `@/components/ui/separator`\\n- `@/components/ui/select`\\n- `@/components/ui/sheet`\\n- `@/components/ui/skeleton`\\n- `@/components/ui/switch`\\n- `@/components/ui/table`\\n- `@/components/ui/textarea`\\n- `@/components/ui/tooltip`\\n\\n<Note>The `@/components/ui` imports are from Shadcn.</Note>\\n\\n### Available APIs\\n\\nChainlit exposes the following APIs globally to make the custom element interactive.\\n\\n```ts\\ninterface APIs {\\n    // Update the element props. This will re-render the element.\\n    updateElement: (nextProps: Record<string, any>) => Promise<{success: boolean}>;\\n    // Delete the element entirely.\\n    deleteElement: () => Promise<{success: boolean}>;\\n    // Call an action defined in the Chainlit app\\n    callAction: (action: {name: string, payload: Record<string, unknown>}) =>Promise<{success: boolean}>;\\n    // Send a user message\\n    sendUserMessage: (message: string) => void;\\n}\\n```\\n\\n### Example of a Counter Element\\n\\n```jsx\\nimport { Button } from \"@/components/ui/button\"\\nimport { X, Plus } from \\'lucide-react\\';\\n\\nexport default function Counter() {\\n    return (\\n        <div id=\"custom-counter\" className=\"mt-4 flex flex-col gap-2\">\\n                <div>Count: {props.count}</div>\\n                <Button id=\"increment\" onClick={() => updateElement(Object.assign(props, {count: props.count + 1}))}><Plus /> Increment</Button>\\n                <Button id=\"remove\" onClick={deleteElement}><X /> Remove</Button>\\n        </div>\\n    );\\n}\\n```\\n\\n## Full Example\\n\\nLet\\'s build a custom element to render the status of a Linear ticket.\\n\\nFirst, we write a small Chainlit application faking fetching data from linear:\\n\\n```python app.py\\nimport chainlit as cl\\n\\nasync def get_ticket():\\n    \"\"\"Pretending to fetch data from linear\"\"\"\\n    return {\\n        \"title\": \"Fix Authentication Bug\",\\n        \"status\": \"in-progress\",\\n        \"assignee\": \"Sarah Chen\",\\n        \"deadline\": \"2025-01-15\",\\n        \"tags\": [\"security\", \"high-priority\", \"backend\"]\\n    }\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    # Let\\'s pretend the user is asking about a linear ticket.\\n    # Usually an LLM with tool calling would be used to decide to render the component or not.\\n    \\n    props = await get_ticket()\\n    \\n    ticket_element = cl.CustomElement(name=\"LinearTicket\", props=props)\\n    # Store the element if we want to update it server side at a later stage.\\n    cl.user_session.set(\"ticket_el\", ticket_element)\\n    \\n    await cl.Message(content=\"Here is the ticket information!\", elements=[ticket_element]).send()\\n```\\n\\nSecond we implement the custom element we reference in the Python code:\\n\\n```jsx public/elements/LinearTicket.jsx\\nimport { Card, CardHeader, CardTitle, CardContent } from \"@/components/ui/card\"\\nimport { Badge } from \"@/components/ui/badge\"\\nimport { Progress } from \"@/components/ui/progress\"\\nimport { Clock, User, Tag } from \"lucide-react\"\\n\\nexport default function TicketStatusCard() {\\n  const getProgressValue = (status) => {\\n    const progress = {\\n      \\'open\\': 25,\\n      \\'in-progress\\': 50,\\n      \\'resolved\\': 75,\\n      \\'closed\\': 100\\n    }\\n    return progress[status] || 0\\n  }\\n\\n  return (\\n    <Card className=\"w-full max-w-md\">\\n      <CardHeader className=\"pb-2\">\\n        <div className=\"flex justify-between items-center\">\\n          <CardTitle className=\"text-lg font-medium\">\\n            {props.title || \\'Untitled Ticket\\'}\\n          </CardTitle>\\n          <Badge \\n            variant=\"outline\" \\n          >\\n            {props.status || \\'Unknown\\'}\\n          </Badge>\\n        </div>\\n      </CardHeader>\\n      <CardContent>\\n        <div className=\"space-y-4\">\\n          <Progress value={getProgressValue(props.status)} className=\"h-2\" />\\n          \\n          <div className=\"grid grid-cols-2 gap-4 text-sm\">\\n            <div className=\"flex items-center gap-2\">\\n              <User className=\"h-4 w-4 opacity-70\" />\\n              <span>{props.assignee || \\'Unassigned\\'}</span>\\n            </div>\\n            <div className=\"flex items-center gap-2\">\\n              <Clock className=\"h-4 w-4 opacity-70\" />\\n              <span>{props.deadline || \\'No deadline\\'}</span>\\n            </div>\\n            <div className=\"flex items-center gap-2 col-span-2\">\\n              <Tag className=\"h-4 w-4 opacity-70\" />\\n              <span>{props.tags?.join(\\', \\') || \\'No tags\\'}</span>\\n            </div>\\n          </div>\\n        </div>\\n      </CardContent>\\n    </Card>\\n  )\\n}\\n```\\n\\nFinally, we start the application with `chainlit run app.py` and send a first message in the UI.\\n\\n<Frame caption=\"The LinearTicket custom element rendered.\">\\n  <img src=\"/images/custom-element.png\" />\\n</Frame>\\n\\n## Advanced\\n\\n### Update Props from Python\\n\\nTo update a custom element props from the python code, you can store the element instance in the user session and call `.update()` on it.\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_chat_start\\nasync def start():\\n    element = cl.CustomElement(name=\"Foo\", props={\"foo\": \"bar\"})\\n    cl.user_session.set(\"element\", element)\\n\\n@cl.on_message\\nasync def on_message():\\n    element = cl.user_session.get(\"element\")\\n    element.props[\"foo\"] = \"baz\"\\n    await element.update()\\n```\\n\\n### Call a Function from Python\\n\\nIf you need to call a function directly from the python code, you can use `cl.CopilotFunction`.\\n\\n```python call_func.py\\nimport chainlit as cl\\n\\n@cl.on_chat_start\\nasync def start():\\n    element = cl.CustomElement(name=\"CallFn\")\\n    await cl.Message(content=\"Hello\", elements=[element]).send()\\n    \\n@cl.on_message\\nasync def on_msg(msg: cl.Message):\\n    fn = cl.CopilotFunction(name=\"test\", args={\"content\": msg.content})\\n    res = await fn.acall()\\n```\\n\\n```jsx CallFn.jsx\\nimport { useEffect } from \\'react\\';\\nimport { useRecoilValue } from \\'recoil\\';\\nimport { callFnState } from \\'@chainlit/react-client\\';\\n\\nexport default function CallFnExample() {\\n    const callFn = useRecoilValue(callFnState);\\n\\n    useEffect(() => {\\n        if (callFn?.name === \"test\") {\\n          // Replace the console log with your actual function\\n          console.log(\"Function called with\", callFn.args.content)\\n          callFn.callback()\\n        }\\n      }, [callFn]);\\n\\n      return null\\n}\\n```\\n\\n\\n================================================\\nFile: api-reference/elements/dataframe.mdx\\n================================================\\n---\\ntitle: \"Dataframe\"\\n---\\n\\nThe `Dataframe` class is designed to send a pandas dataframe to the chatbot user interface.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the dataframe to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the dataframe element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"data\" type=\"pd.DataFrame\" optional>\\n  The pandas dataframe instance.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport pandas as pd\\n\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    # Create a sample DataFrame with more than 10 rows to test pagination functionality\\n    data = {\\n        \"Name\": [\\n            \"Alice\",\\n            \"David\",\\n            \"Charlie\",\\n            \"Bob\",\\n            \"Eva\",\\n            \"Grace\",\\n            \"Hannah\",\\n            \"Jack\",\\n            \"Frank\",\\n            \"Kara\",\\n            \"Liam\",\\n            \"Ivy\",\\n            \"Mia\",\\n            \"Noah\",\\n            \"Olivia\",\\n        ],\\n        \"Age\": [25, 40, 35, 30, 45, 55, 60, 70, 50, 75, 80, 65, 85, 90, 95],\\n        \"City\": [\\n            \"New York\",\\n            \"Houston\",\\n            \"Chicago\",\\n            \"Los Angeles\",\\n            \"Phoenix\",\\n            \"San Antonio\",\\n            \"San Diego\",\\n            \"San Jose\",\\n            \"Philadelphia\",\\n            \"Austin\",\\n            \"Fort Worth\",\\n            \"Dallas\",\\n            \"Jacksonville\",\\n            \"Columbus\",\\n            \"Charlotte\",\\n        ],\\n        \"Salary\": [\\n            70000,\\n            100000,\\n            90000,\\n            80000,\\n            110000,\\n            130000,\\n            140000,\\n            160000,\\n            120000,\\n            170000,\\n            180000,\\n            150000,\\n            190000,\\n            200000,\\n            210000,\\n        ],\\n    }\\n\\n    df = pd.DataFrame(data)\\n\\n    elements = [cl.Dataframe(data=df, display=\"inline\", name=\"Dataframe\")]\\n\\n    await cl.Message(content=\"This message has a Dataframe\", elements=elements).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/file.mdx\\n================================================\\n---\\ntitle: \"File\"\\n---\\n\\nThe `File` class allows you to display a button that lets users download the content of the file.\\n\\nYou must provide either an url or a path or content bytes.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the file. This will be shown to users.\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\" optional>\\n  The remote URL of the file image source.\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the file image.\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"bytes\" optional>\\n  The file content of the file image in bytes format.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    elements = [\\n        cl.File(\\n            name=\"hello.py\",\\n            path=\"./hello.py\",\\n            display=\"inline\",\\n        ),\\n    ]\\n\\n    await cl.Message(\\n        content=\"This message has a file element\", elements=elements\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/image.mdx\\n================================================\\n---\\ntitle: \"Image\"\\n---\\n\\nThe `Image` class is designed to create and handle image elements to be sent and displayed in the chatbot user interface.\\n\\nYou must provide either an url or a path or content bytes.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the image to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the image element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"size\" type=\"ElementSize\" optional>\\n  Determines the size of the image. Only works with display=\"inline\". Choices\\n  are \"small\", \"medium\" (default), or \"large\".\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\" optional>\\n  The remote URL of the image source.\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the image.\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"bytes\" optional>\\n  The file content of the image in bytes format.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    image = cl.Image(path=\"./cat.jpeg\", name=\"image1\", display=\"inline\")\\n\\n    # Attach the image to the message\\n    await cl.Message(\\n        content=\"This message has an image!\",\\n        elements=[image],\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/pdf.mdx\\n================================================\\n---\\ntitle: \"PDF viewer\"\\n---\\n\\nThe `Pdf` class allows you to display a PDF hosted remotely or locally in the chatbot UI. This class either takes a URL of a PDF hosted online, or the path of a local PDF.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the PDF to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the PDF element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\">\\n  The remote URL of the PDF file. Must provide url for a remote PDF (or either\\n  path or content for a local PDF).\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the PDF. Must provide either path or content for a\\n  local PDF (or url for a remote PDF).\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"bytes\" optional>\\n  The file content of the PDF in bytes format. Must provide either path or\\n  content for a local PDF (or url for a remote PDF).\\n</ParamField>\\n\\n<ParamField path=\"page\" type=\"int\" optional>\\n  The default rendered page. Must be an integer greater than 0 and less than or\\n  equal to the total number of pages in the PDF. The default value is 1.\\n</ParamField>\\n\\n## Example\\n\\n### Inline\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    # Sending a pdf with the local file path\\n    elements = [\\n      cl.Pdf(name=\"pdf1\", display=\"inline\", path=\"./pdf1.pdf\", page=1)\\n    ]\\n\\n    cl.Message(content=\"Look at this local pdf!\", elements=elements).send()\\n```\\n\\n### Side and Page\\nYou must have the name of the pdf in the content of the message for the link to be created.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    # Sending a pdf with the local file path\\n    elements = [\\n      cl.Pdf(name=\"pdf1\", display=\"side\", path=\"./pdf1.pdf\", page=1)\\n    ]\\n    # Reminder: The name of the pdf must be in the content of the message\\n    await cl.Message(content=\"Look at this local pdf1!\", elements=elements).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/plotly.mdx\\n================================================\\n---\\ntitle: \"Plotly\"\\n---\\n\\nThe `Plotly` class allows you to display a Plotly chart in the chatbot UI. This class takes a Plotly figure.\\n\\nThe advantage of the `Plotly` element over the `Pyplot` element is that it\\'s interactive (the user can zoom on the chart for example).\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the chart to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the chart element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"size\" type=\"ElementSize\" optional>\\n  Determines the size of the chart. Only works with display=\"inline\". Choices\\n  are \"small\", \"medium\" (default), or \"large\".\\n</ParamField>\\n\\n<ParamField path=\"figure\" type=\"str\">\\n  The `plotly.graph_objects.Figure` instance that you want to display.\\n</ParamField>\\n\\n### Example\\n\\n```python\\nimport plotly.graph_objects as go\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    fig = go.Figure(\\n        data=[go.Bar(y=[2, 1, 3])],\\n        layout_title_text=\"An example figure\",\\n    )\\n    elements = [cl.Plotly(name=\"chart\", figure=fig, display=\"inline\")]\\n\\n    await cl.Message(content=\"This message has a chart\", elements=elements).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/pyplot.mdx\\n================================================\\n---\\ntitle: \"Pyplot\"\\n---\\n\\nThe `Pyplot` class allows you to display a Matplotlib pyplot chart in the chatbot UI. This class takes a pyplot figure.\\n\\nThe difference of between this element and the `Plotly` element is that the user is shown a static image of the chart when using `Pyplot`.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the chart to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the chart element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"size\" type=\"ElementSize\" optional>\\n  Determines the size of the chart. Only works with display=\"inline\". Choices\\n  are \"small\", \"medium\" (default), or \"large\".\\n</ParamField>\\n\\n<ParamField path=\"figure\" type=\"str\">\\n  The `matplotlib.figure.Figure` instance that you want to display.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport matplotlib.pyplot as plt\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    fig, ax = plt.subplots()\\n    ax.plot([1, 2, 3, 4], [1, 4, 2, 3])\\n\\n    elements = [\\n        cl.Pyplot(name=\"plot\", figure=fig, display=\"inline\"),\\n    ]\\n    await cl.Message(\\n        content=\"Here is a simple plot\",\\n        elements=elements,\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/tasklist.mdx\\n================================================\\n---\\ntitle: \"TaskList\"\\n---\\n\\nThe `TaskList` class allows you to display a task list next to the chatbot UI.\\n\\n## Attributes\\n\\n<ParamField path=\"status\" type=\"str\">\\n  The status of the TaskList. We suggest using something short like \"Ready\",\\n  \"Running...\", \"Failed\", \"Done\".\\n</ParamField>\\n\\n<ParamField path=\"tasks\" type=\"Task\">\\n  The list of tasks to be displayed in the UI.\\n</ParamField>\\n\\n## Usage\\n\\nThe TaskList element is slightly different from other elements in that it is not attached to a Message or Step but can be sent directly to the chat interface.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    # Create the TaskList\\n    task_list = cl.TaskList()\\n    task_list.status = \"Running...\"\\n\\n    # Create a task and put it in the running state\\n    task1 = cl.Task(title=\"Processing data\", status=cl.TaskStatus.RUNNING)\\n    await task_list.add_task(task1)\\n    # Create another task that is in the ready state\\n    task2 = cl.Task(title=\"Performing calculations\")\\n    await task_list.add_task(task2)\\n\\n    # Optional: link a message to each task to allow task navigation in the chat history\\n    message = await cl.Message(content=\"Started processing data\").send()\\n    task1.forId = message.id\\n\\n    # Update the task list in the interface\\n    await task_list.send()\\n\\n    # Perform some action on your end\\n    await cl.sleep(1)\\n\\n    # Update the task statuses\\n    task1.status = cl.TaskStatus.DONE\\n    task2.status = cl.TaskStatus.FAILED\\n    task_list.status = \"Failed\"\\n    await task_list.send()\\n\\n```\\n\\n<Frame caption=\"Task List in action\">\\n  <video\\n    controls\\n    autoPlay\\n    loop\\n    src=\"https://user-images.githubusercontent.com/13104895/251525854-57d1d9af-62a0-4f67-a530-1f0a3fc93488.mp4\"\\n  />\\n</Frame>\\n\\n\\n\\n================================================\\nFile: api-reference/elements/text.mdx\\n================================================\\n---\\ntitle: \"Text\"\\n---\\n\\nThe `Text` class allows you to display a text element in the chatbot UI. This class takes a string and creates a text element that can be sent to the UI.\\nIt supports the markdown syntax for formatting text.\\n\\nYou must provide either an url or a path or content bytes.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the text element to be displayed in the UI.\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"Union[str, bytes]\" optional>\\n  The text string or bytes that should be displayed as the content of the text\\n  element.\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\" optional>\\n  The remote URL of the text source.\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the text file.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines how the text element should be displayed in the UI. Choices are\\n  \"side\", \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"language\" type=\"str\" optional>\\n  Language of the code if the text is a piece of code. See\\n  https://react-code-blocks-rajinwonderland.vercel.app/?path=/story/codeblock--supported-languages\\n  for a list of supported languages.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    text_content = \"Hello, this is a text element.\"\\n    elements = [\\n        cl.Text(name=\"simple_text\", content=text_content, display=\"inline\")\\n    ]\\n\\n    await cl.Message(\\n        content=\"Check out this text element!\",\\n        elements=elements,\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/elements/video.mdx\\n================================================\\n---\\ntitle: \"Video\"\\n---\\n\\nThe `Video` class allows you to display an video player for a specific video file in the chatbot user interface.\\n\\nYou must provide either an url or a path or content bytes.\\n\\n## Attributes\\n\\n<ParamField path=\"name\" type=\"str\">\\n  The name of the video file to be displayed in the UI. This is shown to users.\\n</ParamField>\\n\\n<ParamField path=\"display\" type=\"ElementDisplay\" optional>\\n  Determines where the element should be displayed in the UI. Choices are \"side\"\\n  (default), \"inline\", or \"page\".\\n</ParamField>\\n\\n<ParamField path=\"url\" type=\"str\" optional>\\n  The remote URL of the video.\\n</ParamField>\\n\\n<ParamField path=\"path\" type=\"str\" optional>\\n  The local file path of the video.\\n</ParamField>\\n\\n<ParamField path=\"content\" type=\"bytes\" optional>\\n  The file content of the video in bytes format.\\n</ParamField>\\n\\n## Example\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def main():\\n    elements = [\\n        cl.Video(name=\"example.mp4\", path=\"./example.mp4\", display=\"inline\"),\\n    ]\\n    await cl.Message(\\n        content=\"Here is an video file\",\\n        elements=elements,\\n    ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/input-widgets/select.mdx\\n================================================\\n---\\ntitle: \"Select\"\\n---\\n\\n### Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The identifier used to retrieve the widget value from the settings.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\">\\n  The label of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"values\" type=\"List[str]\" optional>\\n  Labels for the select options.\\n</ParamField>\\n\\n<ParamField path=\"items\" type=\"Dict[str, str]\" optional>\\n  Labels with corresponding values for the select options.\\n</ParamField>\\n\\n<ParamField path=\"initial_value\" type=\"int\" optional>\\n  The initial value of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"initial_index\" type=\"int\" optional>\\n  Index of the initial value of the input widget.\\n  Can only be used in combination with \\'values\\'.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The tooltip text shown when hovering over the tooltip icon next to the label.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The text displayed underneath the input widget.\\n</ParamField>\\n\\n### Usage\\n\\n```python Code Example\\nimport chainlit as cl\\nfrom chainlit.input_widget import Select\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            Select(\\n                id=\"Model\",\\n                label=\"OpenAI - Model\",\\n                values=[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-4\", \"gpt-4-32k\"],\\n                initial_index=0,\\n            )\\n        ]\\n    ).send()\\n    value = settings[\"Model\"]\\n\\n```\\n\\n\\n================================================\\nFile: api-reference/input-widgets/slider.mdx\\n================================================\\n---\\ntitle: \"Slider\"\\n---\\n\\n### Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The identifier used to retrieve the widget value from the settings.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\">\\n  The label of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"initial\" type=\"int\">\\n  The initial value of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"min\" type=\"int\" optional>\\n  The minimum permitted slider value. Defaults to 0.\\n</ParamField>\\n\\n<ParamField path=\"max\" type=\"int\" optional>\\n  The maximum permitted slider value. Defaults to 10.\\n</ParamField>\\n\\n<ParamField path=\"step\" type=\"int\" optional>\\n  The stepping interval of the slider. Defaults to 1.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The tooltip text shown when hovering over the tooltip icon next to the label.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The text displayed underneath the input widget.\\n</ParamField>\\n\\n### Usage\\n\\n```python Code Example\\nimport chainlit as cl\\nfrom chainlit.input_widget import Slider\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            Slider(\\n                id=\"Temperature\",\\n                label=\"OpenAI - Temperature\",\\n                initial=1,\\n                min=0,\\n                max=2,\\n                step=0.1,\\n            ),\\n        ]\\n    ).send()\\n    value = settings[\"Temperature\"]\\n\\n```\\n\\n\\n================================================\\nFile: api-reference/input-widgets/switch.mdx\\n================================================\\n---\\ntitle: \"Switch\"\\n---\\n\\n### Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The identifier used to retrieve the widget value from the settings.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\">\\n  The label of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"initial\" type=\"int\">\\n  The initial value of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The tooltip text shown when hovering over the tooltip icon next to the label.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The text displayed underneath the input widget.\\n</ParamField>\\n\\n### Usage\\n\\n```python Code Example\\nimport chainlit as cl\\nfrom chainlit.input_widget import Switch\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            Switch(id=\"Streaming\", label=\"OpenAI - Stream Tokens\", initial=True),\\n        ]\\n    ).send()\\n    value = settings[\"Streaming\"]\\n\\n```\\n\\n\\n================================================\\nFile: api-reference/input-widgets/tags.mdx\\n================================================\\n---\\ntitle: \"Tags\"\\n---\\n\\n### Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The identifier used to retrieve the widget value from the settings.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\">\\n  The label of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"initial\" type=\"List[str]\" optional>\\n  The initial values of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The tooltip text shown when hovering over the tooltip icon next to the label.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The text displayed underneath the input widget.\\n</ParamField>\\n\\n### Usage\\n\\n```python Code Example\\nimport chainlit as cl\\nfrom chainlit.input_widget import Tags\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            Tags(id=\"StopSequence\", label=\"OpenAI - StopSequence\", initial=[\"Answer:\"]),\\n        ]\\n    ).send()\\n    value = settings[\"StopSequence\"]\\n\\n```\\n\\n\\n================================================\\nFile: api-reference/input-widgets/textinput.mdx\\n================================================\\n---\\ntitle: \"TextInput\"\\n---\\n\\n### Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The identifier used to retrieve the widget value from the settings.\\n</ParamField>\\n\\n<ParamField path=\"label\" type=\"str\">\\n  The label of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"initial\" type=\"str\" optional>\\n  The initial value of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"placeholder\" type=\"str\" optional>\\n  The placeholder value of the input widget.\\n</ParamField>\\n\\n<ParamField path=\"tooltip\" type=\"str\" optional>\\n  The tooltip text shown when hovering over the tooltip icon next to the label.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The text displayed underneath the input widget.\\n</ParamField>\\n\\n### Usage\\n\\n```python Code Example\\nimport chainlit as cl\\nfrom chainlit.input_widget import TextInput\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    settings = await cl.ChatSettings(\\n        [\\n            TextInput(id=\"AgentName\", label=\"Agent Name\", initial=\"AI\"),\\n        ]\\n    ).send()\\n    value = settings[\"AgentName\"]\\n\\n```\\n\\n\\n================================================\\nFile: api-reference/integrations/haystack.mdx\\n================================================\\n---\\ntitle: Haystack\\n---\\n\\n<Note>\\n  The current Haystack integration allows you to run chainlit apps and visualise\\n  intermediary steps. Playground capabilities will be added with the release of\\n  Haystack 2.0.\\n</Note>\\n\\nHaystack is an end-to-end NLP framework that enables you to build NLP applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform question answering, answer generation, semantic document search, or build tools that are capable of complex decision making and query resolution, you can use the state-of-the-art NLP models with Haystack to build end-to-end NLP applications solving your use case.\\nCheck out their repo: https://github.com/deepset-ai/haystack.\\n\\n<Frame caption=\"A Haystack agent run with reasoning steps\">\\n  <img src=\"/images/chainlit-haystack.png\" />\\n</Frame>\\n## Installation\\n\\n```bash\\npip install farm-haystack chainlit\\n```\\n\\n## Integration\\n\\nCreate a new Python file named app.py with the code below.\\n\\n<CodeGroup>\\n\\n```python Code Example\\nfrom haystack.agents.conversational import ConversationalAgent\\nimport chainlit as cl\\n\\n## Agent Code\\n\\nagent = ConversationalAgent(\\n  prompt_node=conversational_agent_prompt_node,\\n  memory=memory,\\n  prompt_template=agent_prompt,\\n  tools=[search_tool],\\n)\\n\\ncl.HaystackAgentCallbackHandler(agent)\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    response = await cl.make_async(agent.run)(message.content)\\n    await cl.Message(author=\"Agent\", content=response[\"answers\"][0].answer).send()\\n\\n```\\n\\n</CodeGroup>\\n\\nThis code adds the Chainlit callback handler to the Haystack callback manager.\\nThe callback handler is responsible for listening to the chain\\'s intermediate steps and sending them to the UI.\\n\\nThen, you can run `chainlit run app.py` in your terminal to run the app and interact with your agent.\\n\\n## Example\\n\\nCheck out this full example from the cookbook: https://github.com/Chainlit/cookbook/tree/main/haystack\\n\\n\\n\\n================================================\\nFile: api-reference/integrations/langchain.mdx\\n================================================\\n---\\ntitle: \"Langchain Callback Handler\"\\n---\\n\\nThe following code example demonstrates how to pass a callback handler:\\n\\n```python\\nllm = OpenAI(temperature=0)\\nllm_math = LLMMathChain.from_llm(llm=llm)\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n\\n    res = await llm_math.acall(message.content, callbacks=[cl.LangchainCallbackHandler()])\\n\\n    await cl.Message(content=\"Hello\").send()\\n```\\n\\n## Final Answer streaming\\n\\nIf streaming is enabled at the LLM level, Langchain will only stream the intermediate steps. You can enable final answer streaming by passing `stream_final_answer=True` to the callback handler.\\n\\n```python\\n# Optionally, you can also pass the prefix tokens that will be used to identify the final answer\\nanswer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\\n\\ncl.LangchainCallbackHandler(\\n        stream_final_answer=True,\\n        answer_prefix_tokens=answer_prefix_tokens,\\n    )\\n```\\n\\n<Warning>\\n  Final answer streaming will only work with prompts that have a consistent\\n  final answer pattern. It will also not work with\\n  `AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`\\n</Warning>\\n\\n\\n\\n================================================\\nFile: api-reference/integrations/llamaindex.mdx\\n================================================\\n---\\ntitle: \"LlamaIndex Callback Handler\"\\n---\\n\\nCallback Handler to enable Chainlit to display intermediate steps in the UI.\\n\\n### Usage\\n\\n```python Code Example\\nfrom llama_index.core.callbacks import CallbackManager\\nfrom llama_index.core.service_context import ServiceContext\\nimport chainlit as cl\\n\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    service_context = ServiceContext.from_defaults(callback_manager=CallbackManager([cl.LlamaIndexCallbackHandler()]))\\n    # use the service context to create the predictor\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-audio-chunk.mdx\\n================================================\\n---\\ntitle: \"on_audio_chunk\"\\n---\\n\\nHook to react to an incoming audio chunk from the user\\'s microphone.\\n\\n## Usage\\n\\n```python\\nfrom io import BytesIO\\nimport chainlit as cl\\n\\n\\n@cl.on_audio_chunk\\nasync def on_audio_chunk(chunk: cl.InputAudioChunk):\\n    pass\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-audio-end.mdx\\n================================================\\n---\\ntitle: \"on_audio_end\"\\n---\\n\\nHook to react to the end of an audio recording coming from the user\\'s microphone.\\n\\n## Usage\\n\\n```python\\nfrom io import BytesIO\\nimport chainlit as cl\\n\\n\\n@cl.on_audio_end\\nasync def on_audio_end():\\n    pass\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-chat-end.mdx\\n================================================\\n---\\ntitle: \"on_chat_end\"\\n---\\n\\nHook to react to the user websocket disconnection event.\\n\\n## Usage\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\ndef start():\\n    print(\"hello\", cl.user_session.get(\"id\"))\\n\\n\\n@cl.on_chat_end\\ndef end():\\n    print(\"goodbye\", cl.user_session.get(\"id\"))\\n\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-chat-resume.mdx\\n================================================\\n---\\ntitle: \"on_chat_resume\"\\n---\\n\\nDecorator to enable users to continue a conversation.\\nRequires both [data persistence](/data-persistence/overview) and [authentication](/authentication) to be enabled.\\n\\nThis decorator will automatically:\\n\\n- Send the persisted messages and elements to the UI.\\n- Restore the user session.\\n\\n<Warning>\\n  Only JSON serializable fields of the user session will be saved and restored.\\n</Warning>\\n\\n## Usage\\n\\nAt minimum, you will need to use the `@cl.on_chat_resume` decorator to resume conversations.\\n\\n```python\\n@cl.on_chat_resume\\nasync def on_chat_resume(thread):\\n    pass\\n```\\n\\nHowever, if you are using a Langchain agent for instance, you will need to reinstantiate and set it in the user session yourself.\\n\\n<Card\\n  title=\"Resume Langchain Chat Example\"\\n  color=\"#F80061\"\\n  icon=\"message\"\\n  href=\"https://github.com/Chainlit/cookbook/tree/main/resume-chat\"\\n>\\n  Practical example of how to resume a chat with context.\\n</Card>\\n\\n## Parameters\\n\\n<ParamField path=\"thread\" type=\"ThreadDict\">\\n  The persisted chat to resume.\\n</ParamField>\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-chat-start.mdx\\n================================================\\n---\\ntitle: \"on_chat_start\"\\n---\\n\\nHook to react to the user websocket connection event.\\n\\n## Usage\\n\\n```python Code Example\\nfrom chainlit import AskUserMessage, Message, on_chat_start\\n\\n\\n@on_chat_start\\nasync def main():\\n    res = await AskUserMessage(content=\"What is your name?\", timeout=30).send()\\n    if res:\\n        await Message(\\n            content=f\"Your name is: {res[\\'content\\']}.\\\\nChainlit installation is working!\\\\nYou can now start building your own chainlit apps!\",\\n        ).send()\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-logout.mdx\\n================================================\\n---\\ntitle: \"on_logout\"\\n---\\n\\nDecorator to react to a user logging out. Useful to clear cookies or other user data through the HTTP response.\\n\\n## Parameters\\n\\n<ParamField path=\"request\" type=\"fastapi.Request\">\\n  The request object.\\n</ParamField>\\n\\n<ParamField path=\"response\" type=\"fastapi.Response\">\\n  The response object.\\n</ParamField>\\n## Usage\\n\\n```python\\nfrom fastapi import Request, Response\\n\\nimport chainlit as cl\\n\\n\\n@cl.on_logout\\ndef main(request: Request, response: Response):\\n    response.delete_cookie(\"my_cookie\")\\n```\\n\\n\\n\\n================================================\\nFile: api-reference/lifecycle-hooks/on-message.mdx\\n================================================\\n---\\ntitle: \"on_message\"\\n---\\n\\nDecorator to react to messages coming from the UI.\\nThe decorated function is called every time a new message is received.\\n\\n## Parameters\\n\\n<ParamField path=\"message\" type=\"cl.Message\">\\n  The message coming from the UI.\\n</ParamField>\\n\\n## Usage\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_message\\ndef main(message: cl.Message):\\n  content = message.content\\n  # do something\\n```\\n\\n\\n\\n================================================\\nFile: authentication/header.mdx\\n================================================\\n---\\ntitle: Header\\n---\\n\\nHeader auth is a simple way to authenticate users using a header. It is typically used to delegate authentication to a reverse proxy.\\n\\nThe `header_auth_callback` function is called with the headers of the request. It should return a `User` object if the user is authenticated, or `None` if the user is not authenticated.\\nThe callback function (defined by the user) is responsible for managing the authentication logic.\\n\\n## Example\\n\\n```python\\nfrom typing import Optional\\n\\nimport chainlit as cl\\n\\n\\n@cl.header_auth_callback\\ndef header_auth_callback(headers: Dict) -> Optional[cl.User]:\\n  # Verify the signature of a token in the header (ex: jwt token)\\n  # or check that the value is matching a row from your database\\n  if headers.get(\"test-header\") == \"test-value\":\\n    return cl.User(identifier=\"admin\", metadata={\"role\": \"admin\", \"provider\": \"header\"})\\n  else:\\n    return None\\n```\\n\\nUsing this code, you will not be able to access the app unless the header `test-header` is set to `test-value` when sending any request to the app.\\n\\n\\n================================================\\nFile: authentication/oauth.mdx\\n================================================\\n---\\ntitle: OAuth\\n---\\n\\nOAuth lets you use third-party services to authenticate your users.\\n\\n<Note>\\n  To active an OAuth provider, you need to define both the OAuth callback in\\n  your code and the provider(s) environment variables.\\n</Note>\\n\\n## Providers\\n\\nFollow these guides to create an OAuth app for your chosen provider(s). Then copy the information into the right environment variable to active the provider.\\n\\n<Warning>\\n  If your app is served behind a reverse proxy (like cloud run) you will have to\\n  set the `CHAINLIT_URL` environment variable. For instance, if you host your\\n  application at `https://mydomain.com`, `CHAINLIT_URL` should be set to\\n  `https://mydomain.com`.\\n</Warning>\\n\\n### GitHub\\n\\nGo to this page to [create a new GitHub OAuth app](https://github.com/settings/apps).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/github/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/github/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_GITHUB_CLIENT_ID`: Client ID\\n- `OAUTH_GITHUB_CLIENT_SECRET`: Client secret\\n\\n### Gitlab\\n\\nGo to this page to [create a new GitLab OAuth app](https://docs.gitlab.com/ee/integration/oauth_provider.html). When creating the app, you need to allow the `openid`, `profile` and `email` scopes.\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/gitlab/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/gitlab/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_GITLAB_CLIENT_ID`: Client ID\\n- `OAUTH_GITLAB_CLIENT_SECRET`: Client secret\\n- `OAUTH_GITLAB_DOMAIN`: domain name (without the protocol)\\n\\n### Google\\n\\nGo to this page to [create a new Google OAuth app](https://console.developers.google.com/apis/credentials).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/google/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/google/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_GOOGLE_CLIENT_ID`: Client ID\\n- `OAUTH_GOOGLE_CLIENT_SECRET`: Client secret\\n\\n### Azure Active Directory\\n\\nFollow this guide to [create a new Azure Active Directory OAuth app](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/azure-ad/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/azure-ad/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_AZURE_AD_CLIENT_ID`: Client ID\\n- `OAUTH_AZURE_AD_CLIENT_SECRET`: Client secret\\n- `OAUTH_AZURE_AD_TENANT_ID`: Azure tenant ID\\n\\n<Note>\\n  If your application supports \"Accounts in this organizational directory only\"\\n  (Single tenant), you will need to explicitly set:\\n  `OAUTH_AZURE_AD_ENABLE_SINGLE_TENANT=true`. If not, do not set this\\n  environment variable at all.\\n</Note>\\n\\n### Okta\\n\\nFollow this guide to [create OIDC app integrations](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/okta/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/okta/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_OKTA_CLIENT_ID`: Client ID\\n- `OAUTH_OKTA_CLIENT_SECRET`: Client secret\\n- `OAUTH_OKTA_DOMAIN`: Domain name for your okta setup - e.g. https://company.okta.com\\n\\nThere are several ways to configure the Okta OAuth routes:\\n\\n- When using the [Single Sign-On to Okta](https://developer.okta.com/docs/reference/api/oidc/#composing-your-base-url) setup, you need to set the `OAUTH_OKTA_AUTHORIZATION_SERVER_ID` environment variable to `false`.\\n- When using Okta [as the identity platform for your app or API](https://developer.okta.com/docs/reference/api/oidc/#_2-okta-as-the-identity-platform-for-your-app-or-api) either:\\n  - set the `OAUTH_OKTA_AUTHORIZATION_SERVER_ID` environment variable to `default` if you have a developer account,\\n  - or set it to the authorization server id from your Custom Authorization Server.\\n\\n### Descope\\n\\nHead to the [Descope sign-up page](https://www.descope.com/sign-up), to get started with your account and set up your authentication.\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/descope/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/descope/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_DESCOPE_CLIENT_ID`: Descope Project ID, which can be found under [Project Settings](https://app.descope.com/settings/project) in the console.\\n- `OAUTH_DESCOPE_CLIENT_SECRET`: Descope Access Key, which can be created under [Access Keys](https://app.descope.com/accesskeys) in the console.\\n\\n### Auth0\\n\\nFollow this guide to [create an Auth0 application](https://auth0.com/docs/get-started/auth0-overview/create-applications).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/auth0/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/auth0/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_AUTH0_CLIENT_ID`: Client ID\\n- `OAUTH_AUTH0_CLIENT_SECRET`: Client secret\\n- `OAUTH_AUTH0_DOMAIN`: Domain name for your auth0 setup\\n\\nOptional environment variables:\\n\\n- `OAUTH_AUTH0_ORIGINAL_DOMAIN`: Original domain name for your auth0 setup, if you are using a custom domain\\n\\n### Amazon Cognito\\n\\nFollow this guide to [create a new Amazon Cognito User Pool](https://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-create-user-pool.html).\\n\\nThe callback URL should be: `CHAINLIT_URL/auth/oauth/aws-cognito/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/aws-cognito/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_COGNITO_CLIENT_ID`: Client ID\\n- `OAUTH_COGNITO_CLIENT_SECRET`: Client secret\\n- `OAUTH_COGNITO_DOMAIN`: Cognito Domain\\n\\n### Keycloak\\n\\nFollow this documentation to [create a new client](https://www.keycloak.org/docs/latest/server_admin/index.html#assembly-managing-clients_server_administration_guide) in your realm.\\n\\nYou have the option of changing the `id` of your Keycloak provider, which by default is `keycloak`. This is useful if you want to display a more appropriate name on your login page. Use the `OAUTH_KEYCLOAK_NAME` environment variable to set the name. Don\\'t choose an `id` that conflicts with any of the other Oauth providers.\\n\\nThe callback URL for your client should be: `CHAINLIT_URL/auth/oauth/${OAUTH_KEYCLOAK_NAME}/callback`. If your Chainlit app is hosted at localhost:8000, you should use `http://localhost:8000/auth/oauth/${OAUTH_KEYCLOAK_NAME}/callback`.\\n\\nYou need to set the following environment variables:\\n\\n- `OAUTH_KEYCLOAK_CLIENT_ID`: Client ID\\n- `OAUTH_KEYCLOAK_CLIENT_SECRET`: Client secret\\n- `OAUTH_KEYCLOAK_REALM`: The realm which contains your client.\\n- `OAUTH_KEYCLOAK_BASE_URL`: Your Keycloak URL. \\n- `OAUTH_KEYCLOAK_NAME`: Optional, see above.\\n\\n### Custom Provider\\nIt\\'s possible to plug-in for any OAuth provider using Chainlit. Required steps are:\\n- modifying `providers` variable in runtime\\n- implementing `CustomOAuthProvider(OAuthProvider)` class with methods and fields: \\n  - `get_token(self, code, url)`\\n  - `get_user_info(self, token)` \\n  - `authorize_params`\\n  - `env`\\n- providing environmental variables as described in `env`, for example:\\n  - `YOUR_PROVIDER_CLIENT_ID`\\n  - `YOUR_PROVIDER_CLIENT_SECRET`\\n\\n[This cookbook example](https://github.com/Chainlit/cookbook/tree/main/auth) describes how to do it, also check [base class](https://github.com/Chainlit/chainlit/blob/2cb38ad8596ac547355f87266bc15ab3dfd632d2/backend/chainlit/oauth_providers.py#L13) for reference.\\n\\n## Prompt Configuration\\n\\nStarting from version 1.3.0, Chainlit allows you to configure how OAuth providers handle re-authentication through the `prompt` parameter. This is particularly useful for controlling the login behavior when users log out.\\n\\nYou can configure this behavior using two environment variables:\\n- `OAUTH_PROMPT`: Sets the default prompt behavior for all OAuth providers\\n- `OAUTH_<PROVIDER>_PROMPT`: Sets the prompt behavior for a specific provider (e.g., `OAUTH_GITHUB_PROMPT`)\\n\\nThe supported values for these variables are:\\n- `none`: No interaction required (default if not set)\\n- `login`: Forces re-authentication\\n- `consent`: Asks for approval of the requested scopes\\n- `select_account`: Allows users to select a different account\\n\\nFor example:\\n```bash\\n# Force consent prompt for all providers\\nOAUTH_PROMPT=consent\\n\\n# Override specific provider to force login\\nOAUTH_GITHUB_PROMPT=login\\n```\\n\\nNote: The behavior and support for different prompt values may vary between OAuth providers. For instance:\\n- GitHub responds well to `prompt=consent`\\n- Some providers like Descope only respect `prompt=login`\\n\\nThis feature is particularly useful when you want to:\\n- Allow users to properly log out and switch accounts\\n- Force re-authentication for security purposes\\n- Give users the option to change which scopes they approve\\n- Prevent automatic re-authentication after logout\\n\\nThe prompt parameter is defined in the OpenID Connect Core 1.0 specification. For more technical details, refer to the [OpenID Connect documentation](https://openid.net/specs/openid-connect-core-1_0.html#AuthRequest).\\n\\n## Examples\\n\\n### Allow all users who passed the oauth authentication.\\n\\n```python\\nfrom typing import Dict, Optional\\nimport chainlit as cl\\n\\n\\n@cl.oauth_callback\\ndef oauth_callback(\\n  provider_id: str,\\n  token: str,\\n  raw_user_data: Dict[str, str],\\n  default_user: cl.User,\\n) -> Optional[cl.User]:\\n  return default_user\\n```\\n\\n### Only allow users from a specific google domain.\\n\\n```python\\nfrom typing import Dict, Optional\\nimport chainlit as cl\\n\\n\\n@cl.oauth_callback\\ndef oauth_callback(\\n  provider_id: str,\\n  token: str,\\n  raw_user_data: Dict[str, str],\\n  default_user: cl.User,\\n) -> Optional[cl.User]:\\n  if provider_id == \"google\":\\n    if raw_user_data[\"hd\"] == \"example.org\":\\n      return default_user\\n  return None\\n```\\n\\n\\n\\n================================================\\nFile: authentication/overview.mdx\\n================================================\\n---\\ntitle: Overview\\n---\\n\\nChainlit applications are public by default.\\nTo enable authentication and make your app private, you need to:\\n\\n1. Define a `CHAINLIT_AUTH_SECRET` environment variable. This is a secret string that is used to sign the authentication tokens. You can change it at any time, but it will log out all users. You can easily generate one using `chainlit create-secret`.\\n2. Add one or more authentication callbacks to your app:\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Password Auth\"\\n    icon=\"shield\"\\n    color=\"#ea5a0c\"\\n    href=\"/authentication/password\"\\n  >\\n    Authenticate users with login/password.\\n  </Card>\\n  <Card\\n    title=\"OAuth\"\\n    icon=\"google\"\\n    color=\"#0285c7\"\\n    href=\"/authentication/oauth\"\\n  >\\n    Authenticate users with your own OAuth app (like Google).\\n  </Card>\\n  <Card\\n    title=\"Header\"\\n    icon=\"code\"\\n    color=\"#16a34a\"\\n    href=\"/authentication/header\"\\n  >\\n    Authenticate users based on a custom header.\\n  </Card>\\n</CardGroup>\\n\\nEach callback take a different input and optionally return a `cl.User` object. If the callback returns `None`, the authentication is considered as failed.\\n\\n<Warning>\\n  Make sure each user has a unique identifier to prevent them from sharing their\\n  data.\\n</Warning>\\n\\n## Get the current authenticated user\\n\\nYou can access the current authenticated user through the [User Session](/concepts/user-session).\\n\\n```py\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    app_user = cl.user_session.get(\"user\")\\n    await cl.Message(f\"Hello {app_user.identifier}\").send()\\n```\\n\\n\\n\\n================================================\\nFile: authentication/password.mdx\\n================================================\\n---\\ntitle: Password\\n---\\n\\nThe `@cl.password_auth_callback` receives the username and password from the login form. Returning an `cl.User` object will authenticate the user while returning `None` will fail the authentication.\\n\\nYou can verify the credentials against any service that you\\'d like (your own DB, a private google sheet etc.).\\n\\n<Warning>\\n  The usual security best practices applies here, hash password before storing\\n  them.\\n</Warning>\\n\\n## Example\\n\\n```python\\nfrom typing import Optional\\nimport chainlit as cl\\n\\n@cl.password_auth_callback\\ndef auth_callback(username: str, password: str):\\n    # Fetch the user matching username from your database\\n    # and compare the hashed password with the value stored in the database\\n    if (username, password) == (\"admin\", \"admin\"):\\n        return cl.User(\\n            identifier=\"admin\", metadata={\"role\": \"admin\", \"provider\": \"credentials\"}\\n        )\\n    else:\\n        return None\\n```\\n\\n\\n\\n================================================\\nFile: backend/command-line.mdx\\n================================================\\n---\\ntitle: Command Line Options\\n---\\n\\nThe Chainlit CLI (Command Line Interface) is a tool that allows you to interact with the Chainlit system via command line. It provides several commands to manage your Chainlit applications.\\n\\n## Commands\\n\\n### `init`\\n\\nThe `init` command initializes a Chainlit project by creating a configuration file located at `.chainlit/config.toml`\\n\\n```bash\\nchainlit init\\n```\\n\\n### `run`\\n\\nThe `run` command starts a Chainlit application.\\n\\n```bash\\nchainlit run [OPTIONS] TARGET\\n```\\n\\nOptions:\\n\\n- `-w, --watch`: Reload the app when the module changes. When this option is specified, the file watcher will be started and any changes to files will cause the server to reload the app, allowing faster iterations.\\n- `-h, --headless`: Prevents the app from opening in the browser.\\n- `-d, --debug`: Sets the log level to debug. Default log level is error.\\n- `-c, --ci`: Runs in CI mode.\\n- `--no-cache`: Disables third parties cache, such as langchain.\\n- `--host`: Specifies a different host to run the server on.\\n- `--port`: Specifies a different port to run the server on.\\n- `--root-path`: Specifies a subpath to run the server on.\\n\\n\\n\\n================================================\\nFile: backend/env-variables.mdx\\n================================================\\n---\\ntitle: Environment Variables\\n---\\n\\nHardcoding API keys in your code is not a good practice. It makes your code less portable and less flexible. It also makes it harder to keep your code secure. Instead, you should use environment variables to store values that are specific to your development environment.\\n\\nChainlit will automatically load environment variables from a `.env` file in the root of your project. This file should be added to your `.gitignore` file so that it is not committed to your repository.\\n\\n```bash .env\\nOPENAI_API_KEY=sk-...\\nPINECONE_API_KEY=...\\n```\\n\\n## Public Apps & Environment Variables\\n\\nIf you want to share your app to a broader audience, you should not put your own OpenAI API keys in the `.env` file.\\nInstead, you should use `user_env` in the Chainlit config to ask each user to provide their own keys.\\n\\nYou can then access the user\\'s keys in your code using:\\n\\n```python\\nimport chainlit as cl\\n\\nuser_env = cl.user_session.get(\"env\")\\n```\\n\\n\\n\\n================================================\\nFile: backend/config/features.mdx\\n================================================\\n---\\ntitle: Features\\n---\\n\\n### Options\\n\\n<ParamField path=\"unsafe_allow_html\" type=\"bool\" default={false}>\\n  Process and display HTML in messages. This can be a security risk (see\\n  https://stackoverflow.com/questions/19603097/why-is-it-dangerous-to-render-user-generated-html-or-javascript).\\n</ParamField>\\n\\n<ParamField path=\"edit_message\" type=\"bool\" default={true}>\\n  Allow the user to edit their messages.\\n</ParamField>\\n\\n<ParamField path=\"latex\" type=\"bool\" default={false}>\\n  Process and display mathematical expressions. This can clash with \"$\"\\n  characters in messages.\\n</ParamField>\\n\\n<ParamField path=\"spontaneous_file_upload.enabled\" type=\"bool\" default={true}>\\n  Authorize users to upload files with messages. The files are then accessible\\n  in [cl.on_message](/api-reference/lifecycle-hooks/on-message).\\n</ParamField>\\n\\n<ParamField\\n  path=\"spontaneous_file_upload.accept\"\\n  type=\"Union[List[str], Dict[str, List[str]]]\"\\n  default={[\"*/*\"]}\\n>\\n  Restrict user to only upload accepted mime file types. Example: [\"text/plain\",\\n  \"application/pdf\", \"image/x-png\"]\\n</ParamField>\\n\\n<ParamField path=\"spontaneous_file_upload.max_files\" type=\"int\" default={20}>\\n  Restrict user to upload maximum number of files at a time.\\n</ParamField>\\n\\n<ParamField path=\"spontaneous_file_upload.max_size_mb\" type=\"int\" default={500}>\\n  Restrict uploading file size (MB).\\n</ParamField>\\n\\n<ParamField path=\"auto_tag_thread\" type=\"bool\">\\n  Automatically tag threads with the current chat profile (if a chat profile is\\n  used)\\n</ParamField>\\n\\n### Default configuration\\n\\n```toml\\n[features]\\nedit_message = true\\nunsafe_allow_html = false\\nlatex = false\\n[features.spontaneous_file_upload]\\n    enabled = true\\n    accept = [\"*/*\"]\\n    max_files = 20\\n    max_size_mb = 500\\nauto_tag_thread = true\\n```\\n\\n\\n\\n================================================\\nFile: backend/config/overview.mdx\\n================================================\\n---\\ntitle: \"Overview\"\\n---\\n\\nThe `.chainlit/config.toml` file is created when you run `chainlit run ...` or `chainlit init`. It allows you to configure your Chainlit app and to enable/disable specific features.\\n\\nIt is composed of three sections:\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"project\"\\n    icon=\"rocket-launch\"\\n    color=\"#ea5a0c\"\\n    href=\"/backend/config/project\"\\n  >\\n    Project configuration.\\n  </Card>\\n  <Card\\n    title=\"features\"\\n    icon=\"flag\"\\n    color=\"#0285c7\"\\n    href=\"/backend/config/features\"\\n  >\\n    Enable/disable features.\\n  </Card>\\n  <Card title=\"UI\" icon=\"palette\" color=\"#16a34a\" href=\"/backend/config/ui\">\\n    UI configuration.\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: backend/config/project.mdx\\n================================================\\n---\\ntitle: Project\\n---\\n\\n### Options\\n\\n<ParamField path=\"enable_telemetry\" type=\"bool\" default={true}>\\n  Whether to enable telemetry (default: true). No personal data is collected.\\n</ParamField>\\n\\n<ParamField path=\"allow_origins\" type=\"List[str]\" default={[\"*\"]}>\\n  Authorized origins to access the app/copilot.\\n</ParamField>\\n\\n<ParamField path=\"user_env\" type=\"List[str]\" default={[]}>\\n  List of environment variables to be provided by each user to use the app.\\n</ParamField>\\n\\n<ParamField path=\"session_timeout\" type=\"int\" default={3600}>\\n  Duration (in seconds) during which the session is saved when the connection is\\n  lost\\n</ParamField>\\n\\n<ParamField path=\"cache\" type=\"bool\" default={false}>\\n  Enable third parties caching (e.g LangChain cache)\\n</ParamField>\\n\\n<ParamField path=\"follow_symlink\" type=\"bool\" default={false}>\\n  Follow symlink for asset mount (see\\n  https://github.com/Chainlit/chainlit/issues/317)\\n</ParamField>\\n\\n### Default configuration\\n\\n```toml\\n[project]\\n# Whether to enable telemetry (default: true). No personal data is collected.\\nenable_telemetry = true\\n\\n# List of environment variables to be provided by each user to use the app.\\nuser_env = []\\n\\n# Duration (in seconds) during which the session is saved when the connection is lost\\nsession_timeout = 3600\\n\\n# Enable third parties caching (e.g LangChain cache)\\ncache = false\\n\\n# Follow symlink for asset mount (see https://github.com/Chainlit/chainlit/issues/317)\\n# follow_symlink = false\\n```\\n\\n\\n\\n================================================\\nFile: backend/config/ui.mdx\\n================================================\\n---\\ntitle: UI\\n---\\n\\n### Options\\n\\n<ParamField path=\"name\" type=\"str\" default=\"My Chatbot\">\\n  The name of both the application and the chatbot.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The content of the `<meta name=\"description\">` of the application.\\n</ParamField>\\n\\n<ParamField\\n  path=\"cot\"\\n  type=\"Literal[\\'hidden\\', \\'tool_call\\', \\'full\\']\"\\n  default=\"full\"\\n>\\n  The chain of thought (COT) is a feature that shows the user the steps the\\n  chatbot took to reach a conclusion. You can hide the COT, only show the tool\\n  calls, or show it in full.\\n</ParamField>\\n\\n<ParamField path=\"default_collapse_content\" type=\"bool\" default={true}>\\n  When handling large text content we collapse it for keeping the threads\\n  concise. You can disable manually disable this behavior.\\n</ParamField>\\n\\n<ParamField path=\"default_expand_message\" type=\"bool\" default={false}>\\n  Sub-messages are hiden by default, you can \"expand\" the parent message to show\\n  those messages. Toggling this setting will display the sub-messages by\\n  default.\\n</ParamField>\\n\\n<ParamField path=\"github\" type=\"str\" optional>\\n  Passing this option will display a Github-shaped link. If not passed we will\\n  display the link to Chainlit repo.\\n</ParamField>\\n\\n### Default configuration\\n\\n```toml\\n[UI]\\n# Name of the app and chatbot.\\nname = \"Chatbot\"\\n\\n# Description of the app and chatbot. This is used for HTML tags.\\n# description = \"\"\\n\\n# Large size content are by default collapsed for a cleaner ui\\ndefault_collapse_content = true\\n\\n# The default value for the expand messages settings.\\ndefault_expand_messages = false\\n\\n# Chain of Thought (CoT) display mode. Can be \"hidden\", \"tool_call\" or \"full\".\\ncot = \"full\"\\n\\n# Link to your github repo. This will add a github button in the UI\\'s header.\\n# github = \"\"\\n\\n# Specify a CSS file that can be used to customize the user interface.\\n# The CSS file can be served from the public directory or via an external link.\\n# custom_css = \"/public/test.css\"\\n\\n[UI.theme]\\n  #layout = \"wide\"\\n  #font_family = \"Inter, sans-serif\"\\n  # Override default MUI light theme. (Check theme.ts)\\n  [UI.theme.light]\\n      #background = \"#FAFAFA\"\\n      #paper = \"#FFFFFF\"\\n\\n      [UI.theme.light.primary]\\n          #main = \"#F80061\"\\n          #dark = \"#980039\"\\n          #light = \"#FFE7EB\"\\n\\n  # Override default MUI dark theme. (Check theme.ts)\\n  [UI.theme.dark]\\n      #background = \"#FAFAFA\"\\n      #paper = \"#FFFFFF\"\\n\\n      [UI.theme.dark.primary]\\n          #main = \"#F80061\"\\n          #dark = \"#980039\"\\n          #light = \"#FFE7EB\"\\n```\\n\\n\\n\\n================================================\\nFile: concepts/action.mdx\\n================================================\\n---\\ntitle: \"Action\"\\n---\\n\\nActions are a way to send clickable buttons to the user interface. Each action is attached to a [Message](/api-reference/message) and can be used to trigger a python function when the user clicks on it.\\n\\n## Create an action\\n\\nActions are sent to the UI through messages:\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_chat_start\\nasync def start():\\n    # Sending an action button within a chatbot message\\n    actions = [\\n        cl.Action(\\n            name=\"action_button\",\\n            icon=\"mouse-pointer-click\",\\n            payload={\"value\": \"example_value\"},\\n            label=\"Click me!\"\\n        )\\n    ]\\n\\n    await cl.Message(content=\"Interact with this action button:\", actions=actions).send()\\n```\\n\\n## Define a Python Callback\\n\\nTo handle the user\\'s click on the action button, you need to define a callback function with the `@cl.action_callback` decorator:\\n\\n```python\\n@cl.action_callback(\"action_button\")\\nasync def on_action(action: cl.Action):\\n    print(action.payload)\\n```\\n\\n<Card\\n  title=\"Action API\"\\n  color=\"#FFCF30\"\\n  icon=\"bolt\"\\n  href=\"/api-reference/action\"\\n>\\n  Learn how more about Actions.\\n</Card>\\n\\n\\n\\n\\n================================================\\nFile: concepts/chat-lifecycle.mdx\\n================================================\\n---\\ntitle: Chat Life Cycle\\n---\\n\\nWhenever a user connects to your Chainlit app, a new chat session is created. A chat session goes through a life cycle of events, which you can respond to by defining hooks.\\n\\n## On Chat Start\\n\\nThe [on_chat_start](/api-reference/lifecycle-hooks/on-chat-start) decorator is used to define a hook that is called when a new chat session is created.\\n\\n```python\\n@cl.on_chat_start\\ndef on_chat_start():\\n    print(\"A new chat session has started!\")\\n```\\n\\n## On Message\\n\\nThe [on_message](/api-reference/lifecycle-hooks/on-message) decorator is used to define a hook that is called when a new message is received from the user.\\n\\n```python\\n@cl.on_message\\ndef on_message(msg: cl.Message):\\n    print(\"The user sent: \", msg.content)\\n```\\n\\n## On Stop\\n\\nThe `on_stop` decorator is used to define a hook that is called when the user clicks the stop button while a task was running.\\n\\n```python\\n@cl.on_stop\\ndef on_stop():\\n    print(\"The user wants to stop the task!\")\\n```\\n\\n## On Chat End\\n\\nThe [on_chat_end](/api-reference/lifecycle-hooks/on-chat-end) decorator is used to define a hook that is called when the chat session ends either because the user disconnected or started a new chat session.\\n\\n```python\\n@cl.on_chat_end\\ndef on_chat_end():\\n    print(\"The user disconnected!\")\\n```\\n\\n## On Chat Resume\\n\\nThe [on_chat_resume](/api-reference/lifecycle-hooks/on-chat-resume) decorator is used to define a hook that is called when a user resumes a chat session that was previously disconnected. This can only happen if [authentication](/authentication) and [data persistence](/data-persistence) are enabled.\\n\\n```python\\nfrom chainlit.types import ThreadDict\\n\\n@cl.on_chat_resume\\nasync def on_chat_resume(thread: ThreadDict):\\n    print(\"The user resumed a previous chat session!\")\\n```\\n\\n\\n\\n================================================\\nFile: concepts/command.mdx\\n================================================\\n---\\ntitle: \"Command\"\\n---\\n\\nCommands are a great way to capture user intent in a deterministic way.\\n\\n## Attributes\\n\\n<ParamField path=\"id\" type=\"str\">\\n  Identifier for the command, this will be used in the UI.\\n</ParamField>\\n\\n<ParamField path=\"icon\" type=\"str\" optional>\\n  The lucide icon name for the command. See https://lucide.dev/icons/.\\n</ParamField>\\n\\n<ParamField path=\"description\" type=\"str\" optional>\\n  The description of the command.\\n</ParamField>\\n\\n<ParamField path=\"button\" type=\"boolean\" optional>\\n  Whether to display the command as a button in the message composer.\\n</ParamField>\\n\\n<ParamField path=\"persistent\" type=\"boolean\" optional>\\n  Whether to keep the command active after the user sent the message.\\n</ParamField>\\n\\n\\n## Set available commands\\n\\nYou can set the available commands at any moment using the `cl.context.emitter.set_commands` method.\\n\\n```python\\nimport chainlit as cl\\n\\ncommands = [\\n    {\"id\": \"Picture\", \"icon\": \"image\", \"description\": \"Use DALL-E\"},\\n    {\"id\": \"Search\", \"icon\": \"globe\", \"description\": \"Find on the web\"},\\n    {\\n        \"id\": \"Canvas\",\\n        \"icon\": \"pen-line\",\\n        \"description\": \"Collaborate on writing and code\",\\n    },\\n]\\n\\n@cl.on_chat_start\\nasync def start():\\n    await cl.context.emitter.set_commands(commands)\\n\\n@cl.on_message\\nasync def message(msg: cl.Message):\\n    if msg.command == \"Picture\":\\n        # User is using the Picture command\\n        pass\\n    pass\\n```\\n\\n<Frame caption=\"The user selecting a command\">\\n  <img src=\"/images/command.gif\" />\\n</Frame>\\n\\n\\n\\n\\n================================================\\nFile: concepts/element.mdx\\n================================================\\n---\\ntitle: \"Element\"\\n---\\n\\nText messages are the building blocks of a chatbot, but we often want to send more than just text to the user such as images, videos, and more.\\n\\nThat is where elements come in. Each element is a piece of content that can be attached to a [Message](/concepts/message) or a [Step](/concepts/step) and displayed on the user interface.\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Image Element\"\\n    icon=\"image\"\\n    color=\"#0285c7\"\\n    href=\"/api-reference/elements/image\"\\n  >\\n    Ideal to display generated images.\\n  </Card>\\n  <Card\\n    title=\"PDF Element\"\\n    icon=\"file-pdf\"\\n    color=\"#16a34a\"\\n    href=\"/api-reference/elements/pdf\"\\n  >\\n    Ideal to display RAG sources.\\n  </Card>\\n  <Card\\n    title=\"Custom Element\"\\n    icon=\"react\"\\n    color=\"#61DBFB\"\\n    href=\"/api-reference/elements/custom\"\\n  >\\n    Write your own element in JSX.\\n  </Card>\\n  <Card\\n    title=\"More Elements\"\\n    icon=\"wind\"\\n    color=\"#dc2626\"\\n    href=\"/api-reference/elements\"\\n  >\\n    The complete list of elements you can display on the user interface.\\n  </Card>\\n</CardGroup>\\n\\n## Example\\n\\nTo attach an element to a message or step, we need to:\\n\\n1. Instantiate the element\\n2. Attach the element to a message or step\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    image = cl.Image(path=\"./cat.jpeg\", name=\"image1\", display=\"inline\")\\n\\n    # Attach the image to the message\\n    await cl.Message(\\n        content=\"This message has an image!\",\\n        elements=[image],\\n    ).send()\\n```\\n\\n## Display Options\\n\\nThere are 3 display options that determine how an element is rendered:\\n\\n### Side\\n\\n```python\\n@cl.on_chat_start\\nasync def start():\\n    # Notice the display option\\n    image = cl.Image(path=\"./cat.jpeg\", name=\"cat image\", display=\"side\")\\n\\n    await cl.Message(\\n        # Notice that the name of the image is referenced in the message content\\n        content=\"Here is the cat image!\",\\n        elements=[image],\\n    ).send()\\n```\\n\\nThe image will not be displayed in the message. Instead, the name of the image will be displayed as clickable link.\\nWhen the user clicks on the link, the image will be displayed on the side of the message.\\n\\n### Page\\n\\n```python\\n@cl.on_chat_start\\nasync def start():\\n    # Notice the display option\\n    image = cl.Image(path=\"./cat.jpeg\", name=\"cat image\", display=\"page\")\\n\\n    await cl.Message(\\n        # Notice that the name of the image is referenced in the message content\\n        content=\"Here is the cat image!\",\\n        elements=[image],\\n    ).send()\\n```\\n\\nThe image will not be displayed in the message. Instead, the name of the image will be displayed as clickable link.\\nClicking on the link will redirect to a dedicated page where the image will be displayed.\\n\\n### Inline\\n\\n```python\\n@cl.on_chat_start\\nasync def start():\\n    # Notice the display option\\n    image = cl.Image(path=\"./cat.jpeg\", name=\"cat image\", display=\"inline\")\\n\\n    await cl.Message(\\n        # Notice that the name of the image is NOT referenced in the message content\\n        content=\"Hello!\",\\n        elements=[image],\\n    ).send()\\n```\\n\\nThe image will be displayed below with the message regardless of whether the image name is referenced in the message content.\\n\\n## Control the Element Sidebar from Python\\n\\nYou can open/close the sidebar directly in Python. Elements attached to the sidebar will not be persisted, as this sidebar state is not the result of an interaction in the UI.\\n\\n```python\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    # Define the elements you want to display\\n    elements = [\\n        cl.Image(path=\"./cat.jpeg\", name=\"image1\"),\\n        cl.Pdf(path=\"./dummy.pdf\", name=\"pdf1\"),\\n        cl.Text(content=\"Here is a side text document\", name=\"text1\"),\\n        cl.Text(content=\"Here is a page text document\", name=\"text2\"),\\n    ]\\n\\n    # Setting elements will open the sidebar\\n    await cl.ElementSidebar.set_elements(elements)\\n    await cl.ElementSidebar.set_title(\"Test title\")\\n\\n@cl.on_message\\nasync def message(msg: cl.Message):\\n    # You can update the elements\\n    await cl.ElementSidebar.set_elements([cl.Text(content=\"Text changed!\")])\\n    # You can update the title\\n    await cl.ElementSidebar.set_title(\"Title changed!\")\\n\\n    await cl.sleep(2)\\n\\n    # Setting the elements to an empty array will close the sidebar\\n    await cl.ElementSidebar.set_elements([])\\n```\\n\\n\\n================================================\\nFile: concepts/message.mdx\\n================================================\\n---\\ntitle: Message\\n---\\n\\nA Message is a piece of information that is sent from the user to an assistant and vice versa.\\nCoupled with life cycle hooks, they are the building blocks of a chat.\\n\\nA message has a content, a timestamp and cannot be nested.\\n\\n## Example: Reply to a user message\\n\\nLets create a simple assistant that replies to a user message with a greeting.\\n\\n```py\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    response = f\"Hello, you just sent: {message.content}!\"\\n    await cl.Message(response).send()\\n```\\n\\n<Card\\n  title=\"Message API\"\\n  icon=\"message\"\\n  color=\"#F80061\"\\n  href=\"/api-reference/message\"\\n>\\n  Learn more about the Message API.\\n</Card>\\n\\n## Chat Context\\n\\nSince LLMs are stateless, you will often have to accumulate the messages of the current conversation in a list to provide the full context to LLM with each query.\\n\\nYou could do that manually with the [user_session](/concepts/user-session). However, Chainlit provides a built-in way to do this:\\n\\n```py chat_context\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    # Get all the messages in the conversation in the OpenAI format\\n    print(cl.chat_context.to_openai())\\n\\n    # Send the response\\n    response = f\"Hello, you just sent: {message.content}!\"\\n    await cl.Message(response).send()\\n```\\n\\nEvery message sent or received will be automatically accumulated in `cl.chat_context`.\\nYou can then use `cl.chat_context.to_openai()` to get the conversation in the OpenAI format and feed it to the LLM.\\n\\n\\n\\n================================================\\nFile: concepts/starters.mdx\\n================================================\\n---\\ntitle: Starters\\n---\\n\\nStarters are suggestions to help your users get started with your assistant.\\n\\n```python starters.py\\nimport chainlit as cl\\n\\n@cl.set_starters\\nasync def set_starters():\\n    return [\\n        cl.Starter(\\n            label=\"Morning routine ideation\",\\n            message=\"Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.\",\\n            icon=\"/public/idea.svg\",\\n            ),\\n\\n        cl.Starter(\\n            label=\"Explain superconductors\",\\n            message=\"Explain superconductors like I\\'m five years old.\",\\n            icon=\"/public/learn.svg\",\\n            ),\\n        cl.Starter(\\n            label=\"Python script for daily email reports\",\\n            message=\"Write a script to automate sending daily email reports in Python, and walk me through how I would set it up.\",\\n            icon=\"/public/terminal.svg\",\\n            ),\\n        cl.Starter(\\n            label=\"Text inviting friend to wedding\",\\n            message=\"Write a text asking a friend to be my plus-one at a wedding next month. I want to keep it super short and casual, and offer an out.\",\\n            icon=\"/public/write.svg\",\\n            )\\n        ]\\n# ...\\n```\\n\\n<Frame caption=\"Starters example\">\\n  <img src=\"/images/starters.gif\" />\\n</Frame>\\n\\n## With Chat Profiles\\n\\nStarters also work with [Chat Profiles](/advanced-features/chat-profiles). You can define different starters for different chat profiles.\\n\\n```python starters_with_chat_profiles.py\\n@cl.set_chat_profiles\\nasync def chat_profile(current_user: cl.User):\\n    if current_user.metadata[\"role\"] != \"ADMIN\":\\n        return None\\n\\n    return [\\n        cl.ChatProfile(\\n            name=\"My Chat Profile\",\\n            icon=\"https://picsum.photos/250\",\\n            markdown_description=\"The underlying LLM model is **GPT-3.5**, a *175B parameter model* trained on 410GB of text data.\",\\n            starters=[\\n                cl.Starter(\\n                    label=\"Morning routine ideation\",\\n                    message=\"Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.\",\\n                    icon=\"/public/idea.svg\",\\n                ),\\n                cl.Starter(\\n                    label=\"Explain superconductors\",\\n                    message=\"Explain superconductors like I\\'m five years old.\",\\n                    icon=\"/public/learn.svg\",\\n                ),\\n            ],\\n        )\\n    ]\\n```\\n\\n\\n\\n================================================\\nFile: concepts/step.mdx\\n================================================\\n---\\ntitle: Step\\n---\\n\\nLLM powered Assistants take multiple steps to process a user\\'s request, forming a chain of thought.\\nUnlike a [Message](concepts/message), a Step has a type, an input/output and a start/end.\\n\\nDepending on the `config.ui.cot` setting, the full chain of thought can be displayed in full, hidden or only the tool calls.\\n\\nIn [Literal AI](https://literalai.com), the full chain of thought is logged for debugging and replayability purposes.\\n\\n## A Simple Tool Calling Example\\n\\nLets take a simple example of a Chain of Thought that takes a user\\'s message, process it and sends a response.\\n\\n```py\\nimport chainlit as cl\\n\\n\\n@cl.step(type=\"tool\")\\nasync def tool():\\n    # Simulate a running task\\n    await cl.sleep(2)\\n\\n    return \"Response from the tool!\"\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    # Call the tool\\n    tool_res = await tool()\\n\\n    # Send the final answer.\\n    await cl.Message(content=\"This is the final answer\").send()\\n```\\n\\n<Frame caption=\"Output of the code above\">\\n  <img src=\"/images/step.png\" />\\n</Frame>\\n\\n## Step API\\n\\nThere are two ways to create steps, either by using the the `@cl.step` decorator or by using the `cl.Step` class.\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"@cl.step\"\\n    icon=\"at\"\\n    color=\"#F80061\"\\n    href=\"/api-reference/step-decorator\"\\n  >\\n    Easier to use but requires to split your step logic in a function.\\n  </Card>\\n  <Card\\n    title=\"with cl.Step():\"\\n    icon=\"code\"\\n    color=\"#F80061\"\\n    href=\"/api-reference/step-class\"\\n  >\\n    More verbose but usable in any context as a Python Context Manager.\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: concepts/user-session.mdx\\n================================================\\n---\\ntitle: User Session\\n---\\n\\nThe user session is designed to persist data in memory through the [life cycle](/concepts/chat-lifecycle) of a chat session. Each user session is unique to a user and a given chat session.\\n\\n## Why use the user session?\\n\\nLet\\'s say you want to keep track of each chat session message count.\\n\\nA naive implementation might look like this:\\n\\n<Warning>\\n  This example is for illustrative purposes only. It is not recommended to use\\n  this code in production.\\n</Warning>\\n\\n```python Naive Example\\nimport chainlit as cl\\n\\ncounter = 0\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    global counter\\n    counter += 1\\n\\n    await cl.Message(content=f\"You sent {counter} message(s)!\").send()\\n```\\n\\nAt first glance, this code seems to work. However, it has a major flaw. If two users are chatting with the bot at the same time, both users will increment the same `counter`.\\n\\nThis is where the user session comes in. Let\\'s rewrite the above example using the user session:\\n\\n```python Correct example\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\ndef on_chat_start():\\n    cl.user_session.set(\"counter\", 0)\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    counter = cl.user_session.get(\"counter\")\\n    counter += 1\\n    cl.user_session.set(\"counter\", counter)\\n\\n    await cl.Message(content=f\"You sent {counter} message(s)!\").send()\\n```\\n\\n## User Session Default Values\\n\\nBy default, Chainlit stores chat session related data in the user session.\\n\\nThe following keys are reserved for chat session related data:\\n\\n<ParamField path=\"id\" type=\"str\">\\n  The session id.\\n</ParamField>\\n<ParamField path=\"user\" type=\"cl.User\">\\n  Only set if you are enabled [Authentication](/authentication). Contains the\\n  user object of the user that started this chat session.\\n</ParamField>\\n<ParamField path=\"chat_profile\" type=\"str\">\\n  Only relevant if you are using the [Chat\\n  Profiles](/advanced-features/chat-profiles) feature. Contains the chat profile\\n  selected by this user.\\n</ParamField>\\n<ParamField path=\"chat_settings\" type=\"Dict\">\\n  Only relevant if you are using the [Chat\\n  Settings](/advanced-features/chat-settings) feature. Contains the chat\\n  settings given by this user.\\n</ParamField>\\n<ParamField path=\"env\" type=\"Dict\" default=\"{}\">\\n  Only relevant if you are using the [user_env](/backend/config/project) config.\\n  Contains the environment variables given by this user.\\n</ParamField>\\n\\n\\n\\n================================================\\nFile: customisation/avatars.mdx\\n================================================\\n---\\ntitle: Avatars\\n---\\n\\nThe default assistant avatar is the favicon of the application. See how to customize the favicon [here](/customisation/custom-logo-and-favicon).\\n\\nHowever, you can customize the avatar by placing an image file in the `/public/avatars` folder.\\nThe image file should be named after the author of the message. For example, if the author is `My Assistant`, the avatar should be named `my_assistant.png`.\\n\\n```\\npublic/\\n└── avatars/\\n    └── my_assistant.png\\n```\\n\\n\\n\\n================================================\\nFile: customisation/custom-css.mdx\\n================================================\\n---\\ntitle: CSS\\n---\\n\\nChainlit Application allows for design customization through the use of a custom CSS stylesheet. To enable this, modify your configuration settings in .chainlit/config.toml.\\n\\n```toml config.toml\\n[UI]\\n# ...\\n# This can either be a css file in your `public` dir or a URL\\ncustom_css = \\'/public/stylesheet.css\\'\\n```\\n\\n<Note>\\n  At the moment, we do not provide a detailed guide of all the available css\\n  classes. It is up to you to dig in the Web Inspector and find the css class\\n  you wish to override.\\n</Note>\\n\\nOnce the configuration is updated, restart the application. Your custom styling will now be applied.\\n\\n\\n\\n================================================\\nFile: customisation/custom-js.mdx\\n================================================\\n---\\ntitle: JS\\n---\\n\\nYou can inject a custom JavaScript script into the application by adding the following to your `config.toml`:\\n\\n```toml config.toml\\n[UI]\\n# ...\\n# This can either be a css file in your `public` dir or a URL\\ncustom_js = \\'/public/my_js_script.js\\'\\n```\\n\\nOnce the configuration is updated, restart the application. Your custom script will now be loaded.\\n\\n\\n\\n================================================\\nFile: customisation/custom-logo-and-favicon.mdx\\n================================================\\n---\\ntitle: Logo and Favicon\\n---\\n\\nYou can customize the Chainlit application with your own logo and favicon.\\n\\n<Warning>\\n  Assets such as favicons and logos are cached by default by your browser. You\\n  might have to clear your browser cache to see the changes.\\n</Warning>\\n\\n## Use your Logo\\n\\nChainlit Application offers support for both dark and light modes. To accommodate this, prepare two versions of your logo, named `logo_dark.png` and `logo_light.png`. Place these logos in a `/public` folder next to your application. Once you restart the application, your custom logos should be displayed accordingly.\\n\\n<Card\\n  title=\"Custom Logo Example\"\\n  color=\"#08947c\"\\n  icon=\"pied-piper-alt\"\\n  href=\"https://github.com/Chainlit/cookbook/tree/main/custom-logo\"\\n>\\n  Practical example of how to use custom logos in your Chainlit application.\\n</Card>\\n\\n## Use your Favicon\\n\\nTo further enhance branding, you can also update the application\\'s favicon. Place an image file named `favicon` in the `public` folder next to your application. After restarting the application, the new favicon will take effect.\\n\\n## Customize Login Page Background image\\n\\nIf authentication is enabled, a background image (defaulting to the Chainlit logo) will be displayed on the login page.\\nYou can customize it by editing the following fields of your `.chainlit/config.toml` file.\\n\\n```toml config.toml\\n[UI]\\n# Custom login page image, relative to public directory or external URL\\nlogin_page_image = \"/public/custom-background.jpg\"\\n\\n# Custom login page image filter (Tailwind internal filters, no dark/light variants)\\n# login_page_image_filter = \"brightness-50 grayscale\"\\n# login_page_image_dark_filter = \"contrast-200 blur-sm\"\\n```\\n\\n\\n================================================\\nFile: customisation/overview.mdx\\n================================================\\n---\\nTitle: Overview\\n---\\n\\nYou can tailor your Chainlit Application to reflect your organization\\'s branding or personal style. Our intention is to provide a good level of customization to ensure a consistent user experience that aligns with your visual guidelines.\\n\\nIn this section we will go through the different options available.\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Custom Logo and Favicon\"\\n    icon=\"image\"\\n    color=\"#ea5a0c\"\\n    href=\"/customisation/custom-logo-and-favicon\"\\n  >\\n    Learn how to display your own logo and favicon\\n  </Card>\\n  <Card\\n    title=\"Custom CSS\"\\n    icon=\"paint-roller\"\\n    color=\"#0285c7\"\\n    href=\"/customisation/custom-css\"\\n  >\\n    Learn how to provide your own CSS stylesheet.\\n  </Card>\\n  <Card\\n    title=\"Translation Files\"\\n    icon=\"file-code\"\\n    color=\"#007bff\"\\n    href=\"/customisation/translation\"\\n  >\\n    Learn how to navigate and modify translation files for UI text customization.\\n  </Card>\\n  <Card\\n    title=\"Theme\"\\n    icon=\"palette\"\\n    color=\"#16a34a\"\\n    href=\"/customisation/theme\"\\n  >\\n    Learn about creating your own theme.\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: customisation/theme.mdx\\n================================================\\n---\\ntitle: Theme\\n---\\n\\nChainlit\\'s theme is based on CSS variables.\\n\\nTo modify the CSS variables, create a `theme.json` file under `/public` with the following content.\\n\\nYou can check [Shadcn\\'s documentation](https://ui.shadcn.com/docs/theming#list-of-variables) to learn about the role of each variable.\\n\\n<Note>If the UI is not updated, try to empty your browser cache.</Note>\\n\\n```json theme.json\\n{\\n    \"custom_fonts\": [],\\n    \"variables\": {\\n        \"light\": {\\n            \"--font-sans\": \"\\'Inter\\', sans-serif\",\\n            \"--font-mono\": \"source-code-pro, Menlo, Monaco, Consolas, \\'Courier New\\', monospace\",\\n            \"--background\": \"0 0% 100%\",\\n            \"--foreground\": \"0 0% 5%\",\\n            \"--card\": \"0 0% 100%\",\\n            \"--card-foreground\": \"0 0% 5%\",\\n            \"--popover\": \"0 0% 100%\",\\n            \"--popover-foreground\": \"0 0% 5%\",\\n            \"--primary\": \"340 92% 52%\",\\n            \"--primary-foreground\": \"0 0% 100%\",\\n            \"--secondary\": \"210 40% 96.1%\",\\n            \"--secondary-foreground\": \"222.2 47.4% 11.2%\",\\n            \"--muted\": \"0 0% 90%\",\\n            \"--muted-foreground\": \"0 0% 36%\",\\n            \"--accent\": \"0 0% 95%\",\\n            \"--accent-foreground\": \"222.2 47.4% 11.2%\",\\n            \"--destructive\": \"0 84.2% 60.2%\",\\n            \"--destructive-foreground\": \"210 40% 98%\",\\n            \"--border\": \"0 0% 90%\",\\n            \"--input\": \"0 0% 90%\",\\n            \"--ring\": \"340 92% 52%\",\\n            \"--radius\": \"0.75rem\",\\n            \"--sidebar-background\": \"0 0% 98%\",\\n            \"--sidebar-foreground\": \"240 5.3% 26.1%\",\\n            \"--sidebar-primary\": \"240 5.9% 10%\",\\n            \"--sidebar-primary-foreground\": \"0 0% 98%\",\\n            \"--sidebar-accent\": \"240 4.8% 95.9%\",\\n            \"--sidebar-accent-foreground\": \"240 5.9% 10%\",\\n            \"--sidebar-border\": \"220 13% 91%\",\\n            \"--sidebar-ring\": \"217.2 91.2% 59.8%\"\\n        },\\n        \"dark\": {\\n            \"--font-sans\": \"\\'Inter\\', sans-serif\",\\n            \"--font-mono\": \"source-code-pro, Menlo, Monaco, Consolas, \\'Courier New\\', monospace\",\\n            \"--background\": \"0 0% 13%\",\\n            \"--foreground\": \"0 0% 93%\",\\n            \"--card\": \"0 0% 18%\",\\n            \"--card-foreground\": \"210 40% 98%\",\\n            \"--popover\": \"0 0% 18%\",\\n            \"--popover-foreground\": \"210 40% 98%\",\\n            \"--primary\": \"340 92% 52%\",\\n            \"--primary-foreground\": \"0 0% 100%\",\\n            \"--secondary\": \"0 0% 19%\",\\n            \"--secondary-foreground\": \"210 40% 98%\",\\n            \"--muted\": \"0 1% 26%\",\\n            \"--muted-foreground\": \"0 0% 71%\",\\n            \"--accent\": \"0 0% 26%\",\\n            \"--accent-foreground\": \"210 40% 98%\",\\n            \"--destructive\": \"0 62.8% 30.6%\",\\n            \"--destructive-foreground\": \"210 40% 98%\",\\n            \"--border\": \"0 1% 26%\",\\n            \"--input\": \"0 1% 26%\",\\n            \"--ring\": \"340 92% 52%\",\\n            \"--sidebar-background\": \"0 0% 9%\",\\n            \"--sidebar-foreground\": \"240 4.8% 95.9%\",\\n            \"--sidebar-primary\": \"224.3 76.3% 48%\",\\n            \"--sidebar-primary-foreground\": \"0 0% 100%\",\\n            \"--sidebar-accent\": \"0 0% 13%\",\\n            \"--sidebar-accent-foreground\": \"240 4.8% 95.9%\",\\n            \"--sidebar-border\": \"240 3.7% 15.9%\",\\n            \"--sidebar-ring\": \"217.2 91.2% 59.8%\"\\n        }\\n    }\\n}\\n```\\n\\nAs you may have noticed, the colors are not expressed in Hexadecimal but rather in HSL. This is mandatory.\\nYou can easily [convert any color to HSL](https://www.google.com/search?q=hex+to+hsl).\\n\\nThe `custom_fonts` array can receive URLs (typically from google fonts) like:\\n\\n```json\\ncustom_fonts: [\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap\"]\\n```\\n\\n\\n\\n\\n================================================\\nFile: customisation/translation.mdx\\n================================================\\n---\\ntitle: Translation\\n---\\n\\nTranslation files are located in the `.chainlit/translations` directory. The files are named after the language code, e.g. `en-US.json` for English (United States).\\n\\n<Note>\\n  The language is dynamically set for each user based on the language of the\\n  browser. The default language is `en-US`.\\n</Note>\\n\\n## Customizing UI text\\n\\nIn addition to standard translations, you can customize the text of front-end components used within the UI. Each UI element is associated with a unique translation key in the translation files. By modifying these keys, you can personalize or localize the UI text according to your needs.\\n\\nFor example, to change the label of a navigation tab from \"Readme\" to \"Documentation\", locate the corresponding key in your translation file (e.g., `components.organisms.header.readme`) and update the value:\\n\\n```json\\n\"components.organisms.header.readme\": \"Documentation\"\\n```\\n\\n## Adding a new language\\n\\nTo add a new language, create a new file in the `.chainlit/translations` directory with the language code as the filename. The language code should be in the format of `languageCode-COUNTRYCODE`, e.g. `en-US` for English (United States) or `en-GB` for English (United Kingdom).\\n\\n## Lint translations\\n\\nTo lint the translations, run the following command:\\n\\n```bash\\nchainlit lint-translations\\n```\\n\\n## Translate chainlit.md file\\n\\nYou can define multiple translations for the `chainlit.md` file. For instance `chainlit_pt-BR.md` for Portuguese (Brazil) and `chainlit_es-ES.md` for Spanish (Spain).\\nThe file will be loaded based on the browser\\'s language, defaulting to `chainlit.md` if no translation is available.\\n\\n## Resetting\\n\\nTo reset the the translations, remove the `.chainlit/translations` directory and restart your Chainlit application:\\n\\n```bash\\nchainlit run my-app.py\\n```\\n\\n\\n================================================\\nFile: data-layers/official.mdx\\n================================================\\n---\\ntitle: Official Data Layer\\n---\\n\\nFollow the steps in this repository to persist your conversations in 2 minutes:\\n<Card\\n  title=\"Official Data Layer\"\\n  color=\"#F80061\"\\n  icon=\"message\"\\n  href=\"https://github.com/Chainlit/chainlit-datalayer\"\\n>\\n  Out-of-the-box data layer schema to store your threads, steps, feedback, etc.\\n</Card>\\n\\n<Warning>\\nDo not forget to have your Chainlit application point to the database you set up by\\nadding the `DATABASE_URL` environment variable in your `.env`.  \\n\\nIf you wish to store elements, the same goes for your files system configuration.\\n</Warning>\\n\\n<Tip>\\nCustom element `props` are stored directly in PostgreSQL, not on cloud storage.\\n</Tip>\\n\\n\\n================================================\\nFile: data-layers/overview.mdx\\n================================================\\n---\\ntitle: Overview\\n---\\n\\nChoose one of the following options for your open source data layer:\\n- use the official Chainlit data layer (PostgreSQL + asyncpg)\\n- leverage a community-based data layer\\n- or build your own!\\n\\n<CardGroup cols={2}>\\n    <Card\\n        title=\"Official data layer\"\\n        icon=\"check\"\\n        color=\"#16a34a\"\\n        href=\"/data-layers/official\"\\n    >\\n        The official Chainlit data layer\\n    </Card>\\n    <Card\\n        title=\"Custom data layer API\"\\n        icon=\"text\"\\n        color=\"#ea5a0c\"\\n        href=\"/api-reference/data-persistence/custom-data-layer\"\\n    >\\n        The custom data layer implementation reference\\n    </Card>\\n</CardGroup>\\n\\n\\n## Official data layer\\n\\nWhen using the [official data layer](/data-layers/official), just add the `DATABASE_URL` variable to your `.env` and\\na cloud storage configuration if relevant. \\n\\n## Custom data layers\\n\\nFollow the [reference](/api-reference/data-persistence/custom-data-layer) for an exhaustive list of the methods your custom data layer needs to implement.\\n\\n\\n================================================\\nFile: data-persistence/feedback.mdx\\n================================================\\n---\\ntitle: Human Feedback\\n---\\n\\nHuman feedback is a crucial part of developing your LLM app or agent.\\n\\nIt allows your users to provide direct feedback on the interaction, which can be used to improve the performance and accuracy of your system.\\n\\nBy enabling data persistence, each run triggered by a user input will be accompanied by thumbs up and thumbs down icons. Users can also add a text comment to their feedback for more detailed input.\\n\\n<Frame caption=\"Feedback with comment\">\\n  <img src=\"/images/feedback.jpeg\" />\\n</Frame>\\n\\n## Benefits\\n\\n- **Dataset Creation:** Feedback interactions implicitly generate valuable training data to improve the agent\\'s responses over time.\\n\\n- **Accuracy Measurement:** Feedback scores enable objective measurement and comparison of different agent versions, facilitating continuous model improvement.\\n\\n- **User-Centric Development:** Direct feedback promotes a user-centric approach, ensuring the model evolves to meet user needs and expectations.\\n\\n- **Training and Fine-Tuning:** Human feedback allows for direct model training and fine-tuning based on specific interactions.\\n\\n## How-to\\n\\nTo use human feedback, you first need to enable [data persistence](/data-persistence/overview).\\n\\n<Frame caption=\"Human feedback\">\\n  <img src=\"/images/human-feedback.gif\" />\\n</Frame>\\n\\n## Conclusion\\n\\nHuman feedback is a powerful tool for improving the performance of your LLM app. By enabling data persistence and collecting feedback, you can create a dataset that can be used to improve the system\\'s accuracy.\\n\\n\\n\\n================================================\\nFile: data-persistence/history.mdx\\n================================================\\n---\\ntitle: Chat History\\n---\\n\\nChat history allow users to search, browse and resume their past conversations.\\n\\nIf data persistence is enabled but the user is not authenticated, the conversations will be stored but users won\\'t be able to see the chat history.\\n\\nYou need both data persistence and [authentication](/authentication) configured to enable the chat history.\\n\\n<Frame caption=\"Chat History\">\\n  <img src=\"/images/history.gif\" />\\n</Frame>\\n\\n## Resume a conversation\\n\\nTo let users continue persisted conversations, use [cl.on_chat_resume](/api-reference/lifecycle-hooks/on-chat-resume).\\n\\n<Frame caption=\"Resuming a conversation\">\\n  <img src=\"/images/chat-resume.gif\" />\\n</Frame>\\n\\n\\n\\n================================================\\nFile: data-persistence/overview.mdx\\n================================================\\n---\\ntitle: Overview\\n---\\n\\nBy default, your Chainlit app does not persist the chats and elements it generates. However, the ability to store and utilize this data can be a crucial part of your project or organization.\\n\\n\\n## Enable Data Persistence\\nTo enable data persistence in your Chainlit app, you have several options:\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Open Source Data Layer\"\\n    icon=\"toolbox\"\\n    href=\"/data-layers/overview\"\\n  >\\n    Use the official Chainlit data layer, leverage a community data layer or build your own.\\n  </Card>\\n  <Card\\n    title=\"Literal AI\"\\n    icon=\"thumbs-up\"\\n    href=\"/llmops/literalai\"\\n  >\\n    Literal AI is a hosted LLM observability and evaluation platform.\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: data-persistence/tags-metadata.mdx\\n================================================\\n---\\ntitle: Tags & Metadata\\n---\\n\\nTags and metadata provide valuable context for your threads, steps and generations.\\n\\n```py\\n@cl.step(type=\"run\")\\nasync def func(input):\\n    # some code\\n    cl.context.current_step.metadata = {\"experiment\":\"1\"}\\n    cl.context.current_step.tags = [\"to review\"]\\n    # some code\\n    return output\\n```\\n\\nMore information [here](https://docs.literalai.com/guides/logs#add-metadata-and-tags).\\n\\n\\n================================================\\nFile: deploy/copilot.mdx\\n================================================\\n---\\ntitle: Copilot\\n---\\n\\nSoftware Copilot are a new kind of assistant embedded in your app/product. They are designed to help users get the most out of your app by providing contextual guidance and take actions on their behalf.\\n\\n<Frame caption=\"Preview\">\\n  <img src=\"/images/copilot.gif\" />\\n</Frame>\\n\\n<Note>\\n  [Contact us](mailto:contact@literalai.com) to get advice on how to ship\\n  reliable and secure AI Copilots or Assistants in your product.\\n</Note>\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ✅        | ✅       | ✅    | ✅       | ❌           | ✅            | ✅       |\\n\\n## Embedding the Copilot\\n\\nFirst, make sure your Chainlit server is running. Then, add the following script at the end of your website\\'s `<body>` tag:\\n\\n<Note>\\n  This example assumes your Chainlit server is running on\\n  `http://localhost:8000`\\n</Note>\\n\\n```html\\n<head>\\n  <meta charset=\"utf-8\" />\\n</head>\\n<body>\\n  <!-- ... -->\\n  <script src=\"http://localhost:8000/copilot/index.js\"></script>\\n  <script>\\n    window.addEventListener(\"chainlit-call-fn\", (e) => {\\n      const { name, args, callback } = e.detail;\\n      callback(\"You sent: \" + args.msg);\\n    });\\n  </script>\\n  <script>\\n    window.mountChainlitWidget({\\n      chainlitServer: \"http://localhost:8000\",\\n    });\\n  </script>\\n</body>\\n```\\n\\n<Warning>\\n  Remember the HTML file has to be served by a server, opening it directly in\\n  your browser won\\'t work. You can use simple HTTP server for tests purpose.\\n</Warning>\\n\\nThat\\'s it! You should now see a floating button on the bottom right corner of your website. Clicking on it will open the Copilot.\\n\\nYou can programmatically toggle the copilot with `window.toggleChainlitCopilot()`.\\n\\n## Widget Configuration\\n\\nThe `mountChainlitWidget` function accepts the following options:\\n\\n```ts\\nexport interface IWidgetConfig {\\n  // URL of the Chainlit server\\n  chainlitServer: string;\\n  // Required if authentication is enabled on the server\\n  accessToken?: string;\\n  // Theme of the copilot\\n  theme?: \"light\" | \"dark\";\\n  // Custom styling to apply to the widget button\\n  button?: {\\n    // ID of the container element to mount the button to\\n    containerId?: string;\\n    // URL of the image to use as the button icon\\n    imageUrl?: string;\\n    // The tailwind classname to apply to the button\\n    className?: string;\\n  };\\n}\\n```\\n\\n## Function Calling\\n\\nThe Copilot can call functions on your website. This is useful for taking actions on behalf of the user. For example, you can call a function to create a new document, or to open a modal.\\n\\nFirst, create a `CopilotFunction` in your Chainlit server:\\n\\n```py\\nimport chainlit as cl\\n\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    if cl.context.session.client_type == \"copilot\":\\n        fn = cl.CopilotFunction(name=\"test\", args={\"msg\": msg.content})\\n        res = await fn.acall()\\n        await cl.Message(content=res).send()\\n```\\n\\nThen, in your app/website, add the following event listener:\\n\\n```js\\nwindow.addEventListener(\"chainlit-call-fn\", (e) => {\\n  const { name, args, callback } = e.detail;\\n  if (name === \"test\") {\\n    console.log(name, args);\\n    callback(\"You sent: \" + args.msg);\\n  }\\n});\\n```\\n\\nAs you can see, the event listener receives the function name, arguments, and a callback function. The callback function should be called with the result of the function call.\\n\\n## Send a Message\\n\\nThe Copilot can also send messages directly to the Chainlit server. This is useful for sending context information or user actions to the Chainlit server (like the user selected from cell A1 to B1 on a table).\\n\\nFirst, update the `@cl.on_message` decorated function to your Chainlit server:\\n\\n```py\\nimport chainlit as cl\\n\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    if cl.context.session.client_type == \"copilot\":\\n\\n        if msg.type == \"system_message\":\\n          # do something with the message\\n          return\\n\\n        fn = cl.CopilotFunction(name=\"test\", args={\"msg\": msg.content})\\n        res = await fn.acall()\\n        await cl.Message(content=res).send()\\n```\\n\\nThen, in your app/website, you can emit an event like this:\\n\\n```js\\nwindow.sendChainlitMessage({\\n  type: \"system_message\",\\n  output: \"Hello World!\",\\n});\\n```\\n\\n## Security\\n\\n### Cross Origin Resource Sharing (CORS)\\n\\nDon\\'t forget to add the origin of the host website to the [allow_origins](/backend/config/project) config field to a list of allowed origins.\\n\\n### Authentication\\n\\nIf you want to authenticate users on the Copilot, you can enable [authentication](/authentication) on the Chainlit server.\\n\\n<Warning>\\n  If the Chainlit app and the host website are deployed on different domains,\\n  you will have to add `CHAINLIT_COOKIE_SAMESITE=none` to the Chainlit app env\\n  variables.\\n</Warning>\\n\\nWhile the standalone Chainlit application handles the authentication process, the Copilot needs to be configured with an access token. This token is used to authenticate the user with the Chainlit server.\\n\\nThe host app/website is responsible for generating the token and passing it to the as `accessToken`. Here are examples of how to generate the token in different languages:\\n\\n<Note>\\n  You will need the `CHAINLIT_AUTH_SECRET` you generated when [configuring\\n  authentication](/authentication).\\n</Note>\\n\\n<CodeGroup>\\n\\n```py jwt.py\\nimport jwt\\nfrom datetime import datetime, timedelta\\n\\nCHAINLIT_AUTH_SECRET = \"your-secret\"\\n\\ndef create_jwt(identifier: str, metadata: dict) -> str:\\n    to_encode = {\\n      \"identifier\": identifier,\\n      \"metadata\": metadata,\\n      \"exp\": datetime.utcnow() + timedelta(minutes=60 * 24 * 15),  # 15 days\\n      }\\n\\n    encoded_jwt = jwt.encode(to_encode, CHAINLIT_AUTH_SECRET, algorithm=\"HS256\")\\n    return encoded_jwt\\n\\naccess_token = create_jwt(\"user-1\", {\"name\": \"John Doe\"})\\n```\\n\\n```ts jwt.ts\\nimport jwt from \"jsonwebtoken\";\\n\\nconst CHAINLIT_AUTH_SECRET = \"your-secret\";\\n\\ninterface Metadata {\\n  [key: string]: any;\\n}\\n\\nfunction createJwt(identifier: string, metadata: Metadata): string {\\n  const toEncode = {\\n    identifier: identifier,\\n    metadata: metadata,\\n    exp: Math.floor(Date.now() / 1000) + 60 * 60 * 24 * 15, // 15 days\\n  };\\n  const encodedJwt = jwt.sign(toEncode, CHAINLIT_AUTH_SECRET, {\\n    algorithm: \"HS256\",\\n  });\\n  return encodedJwt;\\n}\\n\\nconst accessToken = createJwt(\"user-1\", { name: \"John Doe\" });\\n```\\n\\n</CodeGroup>\\n\\n\\n\\n================================================\\nFile: deploy/discord.mdx\\n================================================\\n---\\ntitle: Discord\\n---\\n\\nTo make your Chainlit app available on Discord, you will need to create a Discord app and set up the necessary environment variables.\\n\\n## How it Works\\n\\nThe Discord bot will listen to messages mentioning it in channels and direct messages.\\nIt will send replies to a dedicated thread or DM depending on the context.\\n\\n<Frame caption=\"Preview\">\\n  <img src=\"/images/discord/demo.gif\" />\\n</Frame>\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ❌        | ✅       | ❌    | ❌       | ✅           | ❌            | ✅       |\\n\\n## Install the Discord Library\\n\\nThe Discord library is not included in the Chainlit dependencies. You will have to install it manually.\\n\\n```bash\\npip install discord\\n```\\n\\n## Create a Discord App\\n\\nTo start, navigate to the [Discord apps dashboard](https://discord.com/developers/applications). Here, you should find a button that says New Application. When you click this button, select the option to create your app from scratch.\\n\\n<Frame caption=\"Create a Discord App\">\\n  <img src=\"/images/discord/create-app.png\" />\\n</Frame>\\n\\n## Set the Environment Variables\\n\\nNavigate to the Bot tab and click on `Reset Token`. This will make the token visible. Copy it and set it as an environment variable in your Chainlit app.\\n\\n<Frame caption=\"Copy the Bot Token\">\\n  <img src=\"/images/discord/bot-token.png\" />\\n</Frame>\\n\\n```bash\\nDISCORD_BOT_TOKEN=your_bot_token\\n```\\n\\n## Set Intents\\n\\nNavigate to the Bot tab and enable the `MESSAGE CONTENT INTENT`, then click on Save Changes.\\n\\n<Frame caption=\"Set Intents\">\\n  <img src=\"/images/discord/intent.png\" />\\n</Frame>\\n\\n## Working Locally\\n\\nIf you are working locally, you will have to expose your local Chainlit app to the internet to receive incoming messages to Discord. You can use [ngrok](https://ngrok.com/) for this.\\n\\n```bash\\nngrok http 8000\\n```\\n\\n## Start the Chainlit App\\n\\nSince the Chainlit app is not running, the Discord bot will not be able to communicate with it.\\n\\nFor the example, we will use this simple app:\\n\\n```python my_app.py\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    # Access the original discord message\\n    print(cl.user_session.get(\"discord_message\"))\\n    # Access the discord user\\n    print(cl.user_session.get(\"user\"))\\n\\n    # Access potential attached files\\n    attached_files = msg.elements\\n\\n    await cl.Message(content=\"Hello World\").send()\\n```\\n\\nStart the Chainlit app.\\n\\n<Note>\\n  Using -h to not open the default Chainlit UI since we are using Discord.\\n</Note>\\n\\n```bash\\nchainlit run my_app.py -h\\n```\\n\\n## Install the Discord Bot to Your Workspace\\n\\nNavigate to the OAuth2 tab. In the OAuth2 URL Generator, select the `bot` scope.\\n\\n<Frame caption=\"Configure Installation\">\\n  <img src=\"/images/discord/install-bot.png\" />\\n</Frame>\\n\\nThen, in the Bot Permissions section, select the following permissions.\\n\\n<Note>\\n  You can check that you have selected the right permissions by looking at the\\n  number of permissions parameter of the URL. It should be `377957238848`.\\n</Note>\\n\\n<Frame caption=\"Bot Permissions\">\\n  <img src=\"/images/discord/bot-permissions.png\" />\\n</Frame>\\n\\nCopy the generated URL and paste it in your browser. You will be prompted to add the bot to a server. Select the server you want to add the bot to.\\n\\nThat\\'s it! You should now be able to interact with your Chainlit app through Discord.\\n\\n## Chat History\\n\\nChat history is directly available through discord.\\n\\n```python\\nfrom chainlit.discord.app import client as discord_client\\n\\nimport chainlit as cl\\nimport discord\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    # The user session resets on every Discord message.\\n    # So we add previous chat messages manually.\\n    messages = cl.user_session.get(\"messages\", [])\\n    channel: discord.abc.MessageableChannel = cl.user_session.get(\"discord_channel\")\\n\\n    if channel:\\n        cl.user_session.get(\"messages\")\\n        discord_messages = [message async for message in channel.history(limit=10)]\\n\\n        # Go through last 10 messages and remove the current message.\\n        for x in discord_messages[::-1][:-1]:\\n            messages.append({\\n                \"role\": \"assistant\" if x.author.name == discord_client.user.name else \"user\",\\n                \"content\": x.clean_content if x.clean_content else x.channel.name # first message is empty\\n            })\\n\\n    # Your code here\\n```\\n\\n\\n\\n================================================\\nFile: deploy/overview.mdx\\n================================================\\n---\\ntitle: Overview\\n---\\n\\nA Chainlit application can be consumed through multiple platforms. Write your assistant logic once, use everywhere!\\n\\n## Available Platforms\\n\\n<CardGroup cols={2}>\\n<Card\\n  title=\"Web App\"\\n  href=\"/deploy/webapp\"\\n  icon=\"browser\"\\n  color=\"#F80061\"\\n  >\\n    The native Chainlit UI. Available on port 8000.\\n</Card>\\n\\n<Card title=\"Copilot\" icon=\"sparkles\" color=\"#facc15\" href=\"/deploy/copilot\">\\n  Embed your Chainlit app on any website as a Copilot.\\n</Card>\\n\\n<Card\\n  title=\"Custom React App\"\\n  color=\"#61DBFB\"\\n  icon=\"react\"\\n  href=\"/deploy/react\"\\n>\\n  Learn how to integrate your custom React frontend with the Chainlit backend.\\n</Card>\\n<Card\\n  title=\"Teams\"\\n      href=\"/deploy/teams\"\\n  icon={\\n    <svg viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path\\n        fill=\"#5059C9\"\\n        d=\"M10.765 6.875h3.616c.342 0 .619.276.619.617v3.288a2.272 2.272 0 01-2.274 2.27h-.01a2.272 2.272 0 01-2.274-2.27V7.199c0-.179.145-.323.323-.323zM13.21 6.225c.808 0 1.464-.655 1.464-1.462 0-.808-.656-1.463-1.465-1.463s-1.465.655-1.465 1.463c0 .807.656 1.462 1.465 1.462z\"\\n      />\\n      <path\\n        fill=\"#7B83EB\"\\n        d=\"M8.651 6.225a2.114 2.114 0 002.117-2.112A2.114 2.114 0 008.65 2a2.114 2.114 0 00-2.116 2.112c0 1.167.947 2.113 2.116 2.113zM11.473 6.875h-5.97a.611.611 0 00-.596.625v3.75A3.669 3.669 0 008.488 15a3.669 3.669 0 003.582-3.75V7.5a.611.611 0 00-.597-.625z\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.814 6.875v5.255a.598.598 0 01-.596.595H5.193a3.951 3.951 0 01-.287-1.476V7.5a.61.61 0 01.597-.624h3.31z\"\\n        opacity=\".1\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.488 6.875v5.58a.6.6 0 01-.596.595H5.347a3.22 3.22 0 01-.267-.65 3.951 3.951 0 01-.172-1.15V7.498a.61.61 0 01.596-.624h2.985z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.488 6.875v4.93a.6.6 0 01-.596.595H5.08a3.951 3.951 0 01-.172-1.15V7.498a.61.61 0 01.596-.624h2.985z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.163 6.875v4.93a.6.6 0 01-.596.595H5.079a3.951 3.951 0 01-.172-1.15V7.498a.61.61 0 01.596-.624h2.66z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.814 5.195v1.024c-.055.003-.107.006-.163.006-.055 0-.107-.003-.163-.006A2.115 2.115 0 016.593 4.6h1.625a.598.598 0 01.596.594z\"\\n        opacity=\".1\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.488 5.52v.699a2.115 2.115 0 01-1.79-1.293h1.195a.598.598 0 01.595.594z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.488 5.52v.699a2.115 2.115 0 01-1.79-1.293h1.195a.598.598 0 01.595.594z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"#000000\"\\n        d=\"M8.163 5.52v.647a2.115 2.115 0 01-1.465-1.242h.87a.598.598 0 01.595.595z\"\\n        opacity=\".2\"\\n      />\\n      <path\\n        fill=\"url(#microsoft-teams-color-16__paint0_linear_2372_494)\"\\n        d=\"M1.597 4.925h5.969c.33 0 .597.267.597.596v5.958a.596.596 0 01-.597.596h-5.97A.596.596 0 011 11.479V5.521c0-.33.267-.596.597-.596z\"\\n      />\\n      <path\\n        fill=\"#ffffff\"\\n        d=\"M6.152 7.193H4.959v3.243h-.76V7.193H3.01v-.63h3.141v.63z\"\\n      />\\n      <defs>\\n        <linearGradient\\n          id=\"microsoft-teams-color-16__paint0_linear_2372_494\"\\n          x1=\"2.244\"\\n          x2=\"6.906\"\\n          y1=\"4.46\"\\n          y2=\"12.548\"\\n          gradientUnits=\"userSpaceOnUse\"\\n        >\\n          <stop stopColor=\"#5A62C3\" />\\n          <stop offset=\".5\" stopColor=\"#4D55BD\" />\\n          <stop offset=\"1\" stopColor=\"#3940AB\" />\\n        </linearGradient>\\n      </defs>\\n    </svg>\\n  }\\n>\\n  Make your Chainlit app available on Teams.\\n</Card>\\n<Card\\n  title=\"Slack\"\\n  icon={\\n    <svg\\n      enableBackground=\"new 0 0 2447.6 2452.5\"\\n      viewBox=\"0 0 2447.6 2452.5\"\\n      xmlns=\"http://www.w3.org/2000/svg\"\\n    >\\n      <g clipRule=\"evenodd\" fillRule=\"evenodd\">\\n        <path\\n          d=\"m897.4 0c-135.3.1-244.8 109.9-244.7 245.2-.1 135.3 109.5 245.1 244.8 245.2h244.8v-245.1c.1-135.3-109.5-245.1-244.9-245.3.1 0 .1 0 0 0m0 654h-652.6c-135.3.1-244.9 109.9-244.8 245.2-.2 135.3 109.4 245.1 244.7 245.3h652.7c135.3-.1 244.9-109.9 244.8-245.2.1-135.4-109.5-245.2-244.8-245.3z\"\\n          fill=\"#36c5f0\"\\n        />\\n        <path\\n          d=\"m2447.6 899.2c.1-135.3-109.5-245.1-244.8-245.2-135.3.1-244.9 109.9-244.8 245.2v245.3h244.8c135.3-.1 244.9-109.9 244.8-245.3zm-652.7 0v-654c.1-135.2-109.4-245-244.7-245.2-135.3.1-244.9 109.9-244.8 245.2v654c-.2 135.3 109.4 245.1 244.7 245.3 135.3-.1 244.9-109.9 244.8-245.3z\"\\n          fill=\"#2eb67d\"\\n        />\\n        <path\\n          d=\"m1550.1 2452.5c135.3-.1 244.9-109.9 244.8-245.2.1-135.3-109.5-245.1-244.8-245.2h-244.8v245.2c-.1 135.2 109.5 245 244.8 245.2zm0-654.1h652.7c135.3-.1 244.9-109.9 244.8-245.2.2-135.3-109.4-245.1-244.7-245.3h-652.7c-135.3.1-244.9 109.9-244.8 245.2-.1 135.4 109.4 245.2 244.7 245.3z\"\\n          fill=\"#ecb22e\"\\n        />\\n        <path\\n          d=\"m0 1553.2c-.1 135.3 109.5 245.1 244.8 245.2 135.3-.1 244.9-109.9 244.8-245.2v-245.2h-244.8c-135.3.1-244.9 109.9-244.8 245.2zm652.7 0v654c-.2 135.3 109.4 245.1 244.7 245.3 135.3-.1 244.9-109.9 244.8-245.2v-653.9c.2-135.3-109.4-245.1-244.7-245.3-135.4 0-244.9 109.8-244.8 245.1 0 0 0 .1 0 0\"\\n          fill=\"#e01e5a\"\\n        />\\n      </g>\\n    </svg>\\n  }\\n  href=\"/deploy/slack\"\\n>\\n  Make your Chainlit app available on Slack.\\n</Card>\\n<Card\\n  title=\"Discord\"\\n    href=\"/deploy/discord\"\\n  icon={\\n<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 127.14 96.36\"><path fill=\"#5865f2\" d=\"M107.7,8.07A105.15,105.15,0,0,0,81.47,0a72.06,72.06,0,0,0-3.36,6.83A97.68,97.68,0,0,0,49,6.83,72.37,72.37,0,0,0,45.64,0,105.89,105.89,0,0,0,19.39,8.09C2.79,32.65-1.71,56.6.54,80.21h0A105.73,105.73,0,0,0,32.71,96.36,77.7,77.7,0,0,0,39.6,85.25a68.42,68.42,0,0,1-10.85-5.18c.91-.66,1.8-1.34,2.66-2a75.57,75.57,0,0,0,64.32,0c.87.71,1.76,1.39,2.66,2a68.68,68.68,0,0,1-10.87,5.19,77,77,0,0,0,6.89,11.1A105.25,105.25,0,0,0,126.6,80.22h0C129.24,52.84,122.09,29.11,107.7,8.07ZM42.45,65.69C36.18,65.69,31,60,31,53s5-12.74,11.43-12.74S54,46,53.89,53,48.84,65.69,42.45,65.69Zm42.24,0C78.41,65.69,73.25,60,73.25,53s5-12.74,11.44-12.74S96.23,46,96.12,53,91.08,65.69,84.69,65.69Z\"/></svg>\\n  }\\n>\\n  Make your Chainlit app available on Discord.\\n</Card>\\n</CardGroup>\\n\\n## Tips & Tricks\\n\\n### Start Chainlit with -h\\n\\nWhen running a Chainlit app in production, you should always add `-h` to the\\n`chainlit run` command. Otherwise a browser window will be opened server side\\nand might break your deployment.\\n\\n### Double check the host\\n\\nBy default, the Chainlit server host is `127.0.0.1`.\\nTypically, if you are running Chainlit on docker, you want to add `--host 0.0.0.0` to your chainlit command.\\n\\n### Account for websockets\\nChainlit is built upon websockets, which means the service you deploy your app\\nto has to support them. When auto scaling, make sure to enable sticky sessions (or session affinity).\\n\\nEven with sticky sessions, load balancers sometime struggle to consistently route a client to the same container.\\nIn that case you can set `transports = [\"websocket\"]` in your `.chainlit/config.toml` file.\\n\\n### Deploying Chainlit on a subpath\\nIf you need to deploy your Chainlit app to a subpath like\\n`https://my-app.com/chainlit`, you will need to set the `--root-path\\n/chainlit` flag when running the `chainlit run` command. This will ensure that\\nthe app is served from the correct path.\\n\\n### Cross origins\\nIf your end users consumes the Chainlit UI from the same origin as the server, everything will work out of the box.\\nHowever, if you embed Chainlit on a website, the connection will fail because of CORS.\\n\\nIn that case, you will have to update the `allow_origins` field of your `.chainlit/config.toml`.\\n\\n## Community resource\\n\\nAfter you\\'ve successfully set up and tested your Chainlit application locally, the next step is to make it accessible to a wider audience by deploying it to a hosting service. This guide provides various options for self-hosting your Chainlit app.\\n\\n- on [Ploomber Cloud](https://docs.cloud.ploomber.io/en/latest/apps/chainlit.html)\\n- on [AWS](https://ankushgarg.super.site/how-to-deploy-your-chatgpt-like-app-with-chainlit-and-aws-ecs)\\n- on [Azure Container](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/create-an-azure-openai-langchain-chromadb-and-chainlit-chat-app/ba-p/3885602)\\n- on [Google Cloud Run](https://pseudohvr.medium.com/deploying-chainlit-on-gcp-72231ba6b77f)\\n- on [Google App Engine](https://github.com/amjadraza/langchain-chainlit-docker-deployment-template)\\n- on [Replit](https://replit.com/@DanConstantini/Build-a-Chatbot-with-OpenAI-LangChain-and-Chainlit?v=1)\\n- on [Render](https://discord.com/channels/1088038867602526210/1126834266504966294/1126845898287230977)\\n- on [Fly.io](https://dev.to/willydouhard/how-to-deploy-your-chainlit-app-to-flyio-38ja)\\n- on [HuggingFace Spaces](https://github.com/Chainlit/cookbook/tree/main/chroma-qa-chat)\\n\\n\\n================================================\\nFile: deploy/slack.mdx\\n================================================\\n---\\ntitle: Slack\\n---\\n\\nTo make your Chainlit app available on Slack, you will need to create a Slack app and set up the necessary environment variables.\\n\\n## How it Works\\n\\nThe Slack bot will listen to messages mentioning it in channels and direct messages.\\nIt will send replies to a dedicated thread or DM depending on the context.\\n\\n<Frame caption=\"Preview\">\\n  <img src=\"/images/slack/demo.gif\" />\\n</Frame>\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ❌        | ✅       | ❌    | ❌       | ✅           | ❌            | ✅       |\\n\\n## Install the Slack Bolt Library\\n\\nThe Slack Bolt library is not included in the Chainlit dependencies. You will have to install it manually.\\n\\n```bash\\npip install slack_bolt\\n```\\n\\n## Create a Slack App\\n\\nTo start, navigate to the [Slack apps dashboard for the Slack API](https://api.slack.com/apps). Here, you should find a green button that says Create New App. When you click this button, select the option to create your app from scratch.\\n\\nCreate a name for your bot, such as \"ChainlitDemo\". Select the workspace you would like your bot to exist in.\\n\\n<Frame caption=\"Create a Slack App\">\\n  <img src=\"/images/slack/create-app.png\" />\\n</Frame>\\n\\n## Working Locally\\n\\nIf you are working locally, you will have to expose your local Chainlit app to the internet to receive incoming messages to Slack. You can use [ngrok](https://ngrok.com/) for this.\\n\\n```bash\\nngrok http 8000\\n```\\n\\nThis will give you a public URL that you can use to set up the app manifest. Do not forget to replace it once you deploy Chainlit to a public host.\\n\\n## Set the App Manifest\\n\\nGo to App Manifest and paste the following Yaml.\\n\\n<Note>Replace the `{placeholders}` with your own values.</Note>\\n\\n```yaml\\ndisplay_information:\\n  name: { APP_NAME }\\nfeatures:\\n  bot_user:\\n    display_name: { APP_NAME }\\n    always_online: false\\noauth_config:\\n  scopes:\\n    user:\\n      - im:history\\n      - channels:history\\n    bot:\\n      - app_mentions:read\\n      - channels:read\\n      - chat:write\\n      - files:read\\n      - files:write\\n      - im:history\\n      - im:read\\n      - im:write\\n      - users:read\\n      - users:read.email\\n      - channels:history\\n      - groups:history\\nsettings:\\n  event_subscriptions:\\n    request_url: https://{ CHAINLIT_APP_HOST }/slack/events\\n    bot_events:\\n      - app_home_opened\\n      - app_mention\\n      - message.im\\n  interactivity:\\n    is_enabled: true\\n    request_url: https://{ CHAINLIT_APP_HOST }/slack/events\\n  org_deploy_enabled: false\\n  socket_mode_enabled: false\\n  token_rotation_enabled: false\\n```\\n\\nClick on Save Changes.\\n\\n<Frame caption=\"Set the App Manifest\">\\n  <img src=\"/images/slack/app-manifest.png\" />\\n</Frame>\\n\\nYou will see a warning stating that the URL is not verified. You can ignore this for now.\\n\\n## [Optional] Allow users to send DMs to Chainlit\\n\\nBy default the app will only listen to mentions in channels.\\n\\nIf you want to allow users to send direct messages to the app, go to App Home and enable \"Allow users to send Slash commands and messages from the messages tab\".\\n\\n<Frame caption=\"Allow DMs\">\\n  <img src=\"/images/slack/allow-dm.png\" />\\n</Frame>\\n\\n## Install the Slack App to Your Workspace\\n\\nNavigate to the Install App tab and click on Install to Workspace.\\n\\n## Set the Environment Variables\\n\\n<Note>Set the environment variables outside of your application code.</Note>\\n\\n### Bot Token\\n\\nOnce the slack application is installed, you will see the Bot User OAuth Token. Set this as an environment variable in your Chainlit app.\\n\\n<Frame caption=\"Copy the Bot Token\">\\n  <img src=\"/images/slack/bot-token.png\" />\\n</Frame>\\n\\n```bash\\nSLACK_BOT_TOKEN=your_bot_token\\n```\\n\\n### Signing Secret\\n\\nNavigate to the Basic Information tab and copy the Signing Secret. Then set it as an environment variable in your Chainlit app.\\n\\n<Frame caption=\"Copy the Signing Secret\">\\n  <img src=\"/images/slack/signing-secret.png\" />\\n</Frame>\\n\\n```bash\\nSLACK_SIGNING_SECRET=your_signing_secret\\n```\\n\\n## Start the Chainlit App\\n\\nSince the Chainlit app is not running, the Slack app will not be able to communicate with it.\\n\\nFor the example, we will use this simple app:\\n\\n```python my_app.py\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    # Access the original slack event\\n    print(cl.user_session.get(\"slack_event\"))\\n    # Access the slack user\\n    print(cl.user_session.get(\"user\"))\\n\\n    # Access potential attached files\\n    attached_files = msg.elements\\n\\n    await cl.Message(content=\"Hello World\").send()\\n```\\n\\n<Note>\\n  Reminder: Make sure the environment variables are set and that your local\\n  chainlit app is exposed to the internet via ngrok.\\n</Note>\\n\\nStart the Chainlit app:\\n\\n```bash\\nchainlit run my_app.py -h\\n```\\n\\n<Note>\\n  Using -h to not open the default Chainlit UI since we are using Slack.\\n</Note>\\n\\nYou should now be able to interact with your Chainlit app through Slack.\\n\\n## Chat History\\n\\nChat history is directly available through the `fetch_slack_message_history` method.\\nIt will fetch the last messages from the current thread or DM channel.\\n\\n```python\\nimport chainlit as cl\\nimport discord\\n\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    fetch_slack_message_history = cl.user_session.get(\"fetch_slack_message_history\")\\n\\n    if fetch_slack_message_history:\\n        print(await fetch_slack_message_history(limit=10))\\n\\n    # Your code here\\n```\\n\\n\\n\\n================================================\\nFile: deploy/teams.mdx\\n================================================\\n---\\ntitle: Teams\\n---\\n\\nTo make your Chainlit app available on Teams, you will need to create a Teams bot and set up the necessary environment variables.\\n\\n## How it Works\\n\\nThe Teams bot will be available in direct messages.\\n\\n<Frame caption=\"Preview\">\\n  <img src=\"/images/teams/demo.gif\" />\\n</Frame>\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ❌        | ✅       | ❌    | ❌       | ❌           | ❌            | ✅       |\\n\\n## Install the Botbuilder Library\\n\\nThe Botbuilder library is not included in the Chainlit dependencies. You will have to install it manually.\\n\\n```bash\\npip install botbuilder-core\\n```\\n\\n## Create a Teams App\\n\\nTo start, navigate to the [App Management](https://dev.teams.microsoft.com/apps) page. Here, create a new app.\\n\\n<Frame caption=\"Create a Teams App\">\\n  <img src=\"/images/teams/create-app.png\" />\\n</Frame>\\n\\n## Fill the App Basic Information\\n\\nNavigate to Configure > Basic Information and fill in the basic information about your app.\\nYou won\\'t be able to publish your app until you fill in all the required fields.\\n\\n<Frame caption=\"Basic infos\">\\n  <img src=\"/images/teams/basic-infos.png\" />\\n</Frame>\\n\\n## Create the Bot\\n\\nNavigate to Configure > App features and add the Bot feature.\\nCreate a new bot and give it the following permissions and save.\\n\\n<Frame caption=\"Bot permissions\">\\n  <img src=\"/images/teams/bot-permissions.png\" />\\n</Frame>\\n\\n## Go to the Bot Framework Portal\\n\\nNavigate to the [Bot Framework Portal](https://dev.botframework.com/bots/), click on the Bot you just created and go to the Settings page.\\n\\n## Get the App ID\\n\\nIn the Bot Framework Portal, you will find the app ID. Copy it and set it as an environment variable in your Chainlit app.\\n\\n```\\nTEAMS_APP_ID=your_app_id\\n```\\n\\n<Frame caption=\"Get the App ID\">\\n  <img src=\"/images/teams/app-id.png\" />\\n</Frame>\\n\\n## Working Locally\\n\\nIf you are working locally, you will have to expose your local Chainlit app to the internet to receive incoming messages to Teams. You can use [ngrok](https://ngrok.com/) for this.\\n\\n```bash\\nngrok http 8000\\n```\\n\\nThis will give you a public URL that you can use to set up the app manifest. Do not forget to replace it once you deploy Chainlit to a public host.\\n\\n## Set the Message Endpoint\\n\\nUnder Configuration, set the messaging endpoint to your Chainlit app HTTPS URL and add the `/teams/events` suffix.\\n\\n<Frame caption=\"Messaging endpoint\">\\n  <img src=\"/images/teams/endpoint.png\" />\\n</Frame>\\n\\n## Get the App Secret\\n\\nOn the same page, you will find a blue \"Manage Microsoft App ID and password\" button. Click on it.\\n\\n<Frame caption=\"Manage password\">\\n  <img src=\"/images/teams/manage-password.png\" />\\n</Frame>\\n\\nNavigate to Manage > Certificates & secrets and create a new client secret. Copy it and set it as an environment variable in your Chainlit app.\\n\\n```\\nTEAMS_APP_PASSWORD=your_app_secret\\n```\\n\\n## Support Multi Tenant Account Types\\n\\nNavigate to Manage > Authentication and toggle \"Accounts in any organizational directory (Any Microsoft Entra ID tenant - Multitenant)\" then save.\\n\\n<Frame caption=\"Multi tenant\">\\n  <img src=\"/images/teams/multitenant.png\" />\\n</Frame>\\n\\n## Start the Chainlit App\\n\\nSince the Chainlit app is not running, the Teams bot will not be able to communicate with it.\\n\\nFor the example, we will use this simple app:\\n\\n```python my_app.py\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    # Access the teams user\\n    print(cl.user_session.get(\"user\"))\\n\\n    # Access potential attached files\\n    attached_files = msg.elements\\n\\n    await cl.Message(content=\"Hello World\").send()\\n```\\n\\n<Note>\\n  Reminder: Make sure the environment variables are set and that your local\\n  chainlit app is exposed to the internet via ngrok.\\n</Note>\\n\\nStart the Chainlit app:\\n\\n```bash\\nchainlit run my_app.py -h\\n```\\n\\n<Note>\\n  Using -h to not open the default Chainlit UI since we are using Teams.\\n</Note>\\n\\n## Publish the Bot\\n\\nBack to the [App Management](https://dev.teams.microsoft.com/apps) page, navigate to \"Publish to org\" and click on \"Publish\".\\n\\n<Frame caption=\"Publish\">\\n  <img src=\"/images/teams/publish.png\" />\\n</Frame>\\n\\n## Authorize the Bot\\n\\nThe Bot will have to be authorized by the Teams admin before it can be used. To do so navigate to the [Teams admin center](https://admin.teams.microsoft.com/policies/manage-apps) and find the app.\\n\\n<Frame caption=\"Publish\">\\n  <img src=\"/images/teams/app-blocked.png\" />\\n</Frame>\\n\\nThen authorize it.\\n\\n<Frame caption=\"Publish\">\\n  <img src=\"/images/teams/authorize-app.png\" />\\n</Frame>\\n\\nYou should now be able to interact with your Chainlit app through Teams.\\n\\n\\n\\n================================================\\nFile: deploy/webapp.mdx\\n================================================\\n---\\ntitle: Web App\\n---\\n\\nThe native Chainlit UI that is available on port 8000. Should open in your default browser when you run `chainlit run`.\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ✅        | ✅       | ✅    | ✅       | ✅           | ✅            | ✅       |\\n\\n<Frame caption=\"Preview\">\\n  <img src=\"/images/starters.gif\" />\\n</Frame>\\n\\n## Window Messaging\\n\\nWhen running the Web App inside an iframe, the server and parent window can communicate using window messages. This is useful for sending context information to the Chainlit server and updating your parent window based on the server\\'s response.\\n\\nAdd a `@cl.on_window_message` decorated function to your Chainlit server to receive messages sent from the parent window.\\n\\n```py\\nimport chainlit as cl\\n\\n@cl.on_window_message\\nasync def window_message(message: str):\\n  if message.startswith(\"Client: \"):\\n    await cl.Message(content=f\"Window message received: {message}\").send()\\n```\\n\\nThen, in your app/website, you can emit a window message like this:\\n\\n```js\\nconst iframe = document.getElementById(\\'the-iframe\\');\\niframe.contentWindow.postMessage(\\'Client: Hello from parent window\\', \\'*\\');\\n```\\n\\nTo send a message from the server to the parent window, use `cl.send_window_message`:\\n\\n```py\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def message():\\n  await cl.send_window_message(\"Server: Hello from Chainlit\")\\n```\\n\\nThe parent window can listen for messages like this:\\n\\n```js\\nwindow.addEventListener(\\'message\\', (event) => {\\n  if (event.data.startsWith(\"Server: \")) {\\n    console.log(\\'Parent window received:\\', event.data);\\n  }\\n});\\n```\\n\\n### Example\\n\\nCheck out this example from the cookbook that uses the window messaging feature: https://github.com/Chainlit/cookbook/tree/main/window-message\\n\\n\\n\\n================================================\\nFile: deploy/react/additional-resources.mdx\\n================================================\\n## Additional Resources\\n\\n- [@chainlit/react-client npm package](https://www.npmjs.com/package/@chainlit/react-client)  \\n  Explore the @chainlit/react-client npm package.\\n\\n- [Recoil Documentation](https://recoiljs.org/docs/introduction/getting-started)  \\n  Learn more about setting up and using Recoil for state management in React applications.\\n\\n- [SWR Documentation](https://swr.vercel.app/)  \\n  Discover how to leverage SWR for data fetching, caching, and revalidation in React applications.\\n\\n- [Socket.IO Documentation](https://socket.io/docs/v4/)  \\n  Understand how real-time communication is handled via Socket.IO, integral to the `useChatInteract` hook\\'s operations.\\n\\n- [JWT Documentation](https://jwt.io/introduction/)  \\n  Learn about JSON Web Tokens (JWT) and how they are used for secure authentication.\\n\\n\\n\\n================================================\\nFile: deploy/react/installation-and-setup.mdx\\n================================================\\n## Overview\\n\\nThe `@chainlit/react-client` package provides a set of React hooks as well as an API client to connect to your **Chainlit** application from any React application. The package includes hooks for managing chat sessions, messages, data, and interactions.\\n\\n## Installation\\n\\nTo install the package, run the following command in your project directory:\\n\\n```bash\\nnpm install @chainlit/react-client\\n```\\n\\nThis package uses **Recoil** to manage its state. This means you will have to wrap your application in a recoil provider:\\n\\n```typescript\\nimport React from \\'react\\';\\nimport ReactDOM from \\'react-dom/client\\';\\nimport { RecoilRoot } from \\'recoil\\';\\n\\nimport { ChainlitAPI, ChainlitContext } from \\'@chainlit/react-client\\';\\n\\nconst CHAINLIT_SERVER_URL = \\'http://localhost:8000\\';\\n\\nconst apiClient = new ChainlitAPI(CHAINLIT_SERVER_URL, \\'webapp\\');\\n\\nReactDOM.createRoot(document.getElementById(\\'root\\') as HTMLElement).render(\\n  <React.StrictMode>\\n    <ChainlitContext.Provider value={apiClient}>\\n      <RecoilRoot>\\n        <MyApp />\\n      </RecoilRoot>\\n    </ChainlitContext.Provider>\\n  </React.StrictMode>\\n);\\n```\\n\\n\\n================================================\\nFile: deploy/react/overview.mdx\\n================================================\\n---\\ntitle: \"Overview\"\\n---\\n\\nChainlit allows you to create a custom frontend for your application, offering you the flexibility to design a unique user experience. By integrating your frontend with Chainlit\\'s backend, you can harness the full power of Chainlit\\'s features, including:\\n\\n- Abstractions for easier development\\n- Monitoring and observability\\n- Seamless integrations with various tools\\n- Robust authentication mechanisms\\n- Support for multi-user environments\\n- Efficient data streaming capabilities\\n\\n<CardGroup cols={2}>\\n  <Card\\n    title=\"Installation and Setup\"\\n    icon=\"gear\"\\n    color=\"#61DBFB\"\\n    href=\"/deploy/react/installation-and-setup\"\\n  >\\n    Learn how to install and set up the Chainlit React client.\\n  </Card>\\n  <Card\\n    title=\"Usage\"\\n    icon=\"bolt\"\\n    color=\"#f59e0b\"\\n    href=\"/deploy/react/usage\"\\n  >\\n    Explore the key features provided by the React client.\\n  </Card>\\n  <Card\\n    title=\"Additional Resources\"\\n    icon=\"flag\"\\n    color=\"#3b82f6\"\\n    href=\"/deploy/react/additional-resources\"\\n  >\\n    Explore additional resources for the React client.\\n  </Card>\\n  <Card\\n    title=\"Custom React frontend\"\\n    color=\"#61DBFB\"\\n    icon=\"react\"\\n    href=\"https://github.com/Chainlit/cookbook/tree/main/custom-frontend\"\\n  >\\n    Learn how to integrate your custom React frontend with the Chainlit backend.\\n  </Card>\\n</CardGroup>\\n\\nThe [@chainlit/react-client](https://www.npmjs.com/package/@chainlit/react-client) package is designed for integrating Chainlit applications with React. It offers several hooks and an API client for seamless connection and interaction.\\n\\n## Supported Features\\n\\n| Message | Streaming | Elements | Audio | Ask User | Chat History | Chat Profiles | Feedback |\\n| ------- | --------- | -------- | ----- | -------- | ------------ | ------------- | -------- |\\n| ✅      | ✅        | ✅       | ✅    | ✅       | ✅           | ✅            | ✅       |\\n\\n\\n\\n================================================\\nFile: deploy/react/usage.mdx\\n================================================\\n\\n## React Hooks\\n\\nThe `@chainlit/react-client` package provides several React hooks to manage various aspects of your chat application seamlessly:\\n\\n- **[`useChatSession`](#usechatsession-hook)**: Manages the chat session\\'s connection to the WebSocket server.\\n- **[`useChatMessages`](#usechatmessages-hook)**: Manages retrieval and rendering of chat messages.\\n- **[`useChatData`](#usechatdata-hook)**: Accesses chat-related data and states.\\n- **[`useChatInteract`](#usechatinteract-hook)**: Provides methods to interact with the chat system.\\n- **[`useAuth`](#useauth-hook)**: Handles authentication processes.\\n- **[`useApi`](#useapi-hook)**: Simplifies API interactions with built-in support for data fetching and error handling.\\n\\n---\\n\\n### `useChatSession` Hook\\n\\nThis hook is responsible for managing the chat session\\'s connection to the WebSocket server.\\n\\n#### Methods\\n\\n- **`connect`**: Establishes a connection to the WebSocket server.\\n- **`disconnect`**: Disconnects from the WebSocket server.\\n- **`setChatProfile`**: Sets the chat profile state.\\n\\n#### Example\\n\\n```tsx\\nimport { useChatSession } from \\'@chainlit/react-client\\';\\n\\nconst ChatComponent = () => {\\n  const { connect, disconnect, chatProfile, setChatProfile } = useChatSession();\\n\\n  // Connect to the WebSocket server\\n  useEffect(() => {\\n    connect({\\n      userEnv: {\\n        /* user environment variables */\\n      },\\n      accessToken: \\'Bearer YOUR ACCESS TOKEN\\', // Optional Chainlit auth token\\n    });\\n\\n    return () => {\\n      disconnect();\\n    };\\n  }, []);\\n\\n  // Rest of your component logic\\n};\\n```\\n\\n---\\n\\n### `useChatMessages` Hook\\n\\nThe `useChatMessages` hook provides access to the current chat messages, the first user interaction, and the active thread ID within your React application. It leverages Recoil for state management, ensuring that your components reactively update in response to state changes.\\n\\n#### Returned Values\\n\\n- **`threadId`** (`string | undefined`):  \\n  The identifier of the current chat thread.\\n- **`messages`** (`IStep[]`):  \\n  An array of chat messages.\\n- **`firstInteraction`** (`string | undefined`):  \\n  The content of the first user-initiated interaction.\\n\\n#### Example\\n\\n```tsx\\nimport { useChatMessages } from \\'@chainlit/react-client\\';\\n\\nconst MessagesComponent = () => {\\n  const { messages, firstInteraction, threadId } = useChatMessages();\\n\\n  return (\\n    <div>\\n      <h2>Thread ID: {threadId}</h2>\\n      {firstInteraction && <p>First Interaction: {firstInteraction}</p>}\\n      {messages.map((message) => (\\n        <p key={message.id}>{message.content}</p>\\n      ))}\\n    </div>\\n  );\\n};\\n```\\n\\n---\\n\\n### `useChatData` Hook\\n\\nThe `useChatData` hook offers comprehensive access to various chat-related states and data within your React application.\\n\\n#### Returned Properties\\n\\n- **`actions`** (`IAction[]`)\\n- **`askUser`** (`IAsk | undefined`)\\n- **`chatSettingsValue`** (`any`)\\n- **`connected`** (`boolean`)\\n- **`disabled`** (`boolean`)\\n- **`error`** (`boolean | undefined`)\\n- **`loading`** (`boolean`)\\n- **`tasklists`** (`ITasklistElement[]`)\\n\\n#### Example\\n\\n```tsx\\nimport { useChatData } from \\'@chainlit/react-client\\';\\n\\nconst ChatStatusComponent = () => {\\n  const { connected, loading, error, actions, askUser, chatSettingsValue } = useChatData();\\n\\n  return (\\n    <div>\\n      <h2>Chat Status</h2>\\n      {loading && <p>Loading chat...</p>}\\n      {error && <p>There was an error with the chat session.</p>}\\n      <p>{connected ? \\'Connected to chat.\\' : \\'Disconnected from chat.\\'}</p>\\n\\n      <h3>Available Actions</h3>\\n      <ul>\\n        {actions.map((action) => (\\n          <li key={action.id}>{action.name}</li>\\n        ))}\\n      </ul>\\n\\n      {askUser && (\\n        <div>\\n          <h3>User Prompt</h3>\\n          <p>{askUser.message}</p>\\n        </div>\\n      )}\\n\\n      <h3>Chat Settings</h3>\\n      <pre>{JSON.stringify(chatSettingsValue, null, 2)}</pre>\\n    </div>\\n  );\\n};\\n```\\n\\n---\\n\\n### `useChatInteract` Hook\\n\\nThe `useChatInteract` hook provides a comprehensive set of methods to interact with the chat system within your React application.\\n\\n#### Methods\\n\\n- **`sendMessage`**\\n- **`replyMessage`**\\n- **`clear`**\\n- **`uploadFile`**\\n- **`callAction`**\\n- **`startAudioStream`**\\n- **`sendAudioChunk`**\\n- **`stopTask`**\\n\\n#### Example\\n\\n```tsx\\nimport { useChatInteract } from \\'@chainlit/react-client\\';\\n\\nconst ChatInteraction = () => {\\n  const { sendMessage, replyMessage, clear } = useChatInteract();\\n\\n  return (\\n    <div>\\n      <button onClick={() => sendMessage({ content: \\'Hello!\\' })}>Send</button>\\n      <button onClick={() => replyMessage({ content: \\'Reply!\\' })}>Reply</button>\\n      <button onClick={clear}>Clear</button>\\n    </div>\\n  );\\n};\\n```\\n\\n---\\n\\n### `useAuth` Hook\\n\\nThe `useAuth` hook manages authentication within your React application, providing functionalities like user sessions and token management.\\n\\n#### Properties & Methods\\n\\n- **`authConfig`**\\n- **`user`**\\n- **`accessToken`**\\n- **`isLoading`**\\n- **`logout`**\\n\\n#### Example\\n\\n```tsx\\nimport { useAuth } from \\'@chainlit/react-client\\';\\n\\nconst UserProfile = () => {\\n  const { user, logout } = useAuth();\\n\\n  if (!user) return <p>No user logged in.</p>;\\n\\n  return (\\n    <div>\\n      <p>Username: {user.username}</p>\\n      <button onClick={logout}>Logout</button>\\n    </div>\\n  );\\n};\\n```\\n\\n---\\n\\n### `useApi` Hook\\n\\nThe `useApi` hook simplifies data fetching and error handling using [SWR](https://swr.vercel.app/).\\n\\n#### Example\\n\\n```tsx\\nimport { useApi } from \\'@chainlit/react-client\\';\\n\\nconst Settings = () => {\\n  const { data, error, isLoading } = useApi(\\'/project/settings\\');\\n\\n  if (isLoading) return <p>Loading...</p>;\\n  if (error) return <p>Error: {error.message}</p>;\\n\\n  return <pre>{JSON.stringify(data, null, 2)}</pre>;\\n};\\n```\\n\\n\\n================================================\\nFile: examples/community.mdx\\n================================================\\n---\\ntitle: Community\\n---\\n\\n## Videos\\n\\n- [Build Python LLM apps in minutes Using Chainlit ⚡️](https://www.youtube.com/watch?v=tv7rn5AsxFY) from [Krish Naik](https://twitter.com/Krishnaik06)\\n- [Build an Arxiv QA Chat Application in Minutes!](https://www.youtube.com/watch?v=9SBUStfCtmk) from [Chris Alexiuk](https://twitter.com/c_s_ale)\\n- [Chainlit: Build LLM Apps in MINUTES!](https://www.youtube.com/watch?v=rcXPq3UcxIY) from [WorldOfAI](https://www.youtube.com/@intheworldofai)\\n- [Now Build & Share LLM Apps Super Fast with Chainlit](https://www.youtube.com/watch?v=_S3usFpVJOM) from [Sunny Bhaveen Chandra](https://www.youtube.com/c/c17hawke)\\n- [Chainlit CrashCourse - Build LLM ChatBot with Chainlit and Python & GPT](https://www.youtube.com/watch?v=pqriC9OT2aY) from [JCharisTech](https://www.youtube.com/@JCharisTech)\\n- [Chat with ... anything](https://twitter.com/waseemhnyc/status/1665923724426502148) by [Waseem H](https://twitter.com/waseemhnyc)\\n- [Unleash the Power of Falcon with LangChain: Step-by-Step Guide to Run Chat App using Chainlit](https://www.youtube.com/watch?v=HG0_0lqrWs4&ab_channel=MenloParkLab) by [Menlo Park Lab](https://www.youtube.com/@menloparklab)\\n- [Chainlit tutorial series](https://www.youtube.com/playlist?list=PL2fGiugrNoogRNUHUWCDAnooWKmfVDnFS) (in chinese) by [01coder](https://www.youtube.com/@01coder30)\\n\\n## Articles\\n\\n- [AI Agents tutorial: How to create information retrieval Chatbot](https://lablab.ai/t/agents-retrieval-chatbot) from [Jakub Misiło](https://www.linkedin.com/in/jmisilo/)\\n- [Create an Azure OpenAI, LangChain, ChromaDB, and Chainlit Chat App in Container Apps using Terraform](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/create-an-azure-openai-langchain-chromadb-and-chainlit-chat-app/ba-p/3885602) from [Paolo Salvatori](https://techcommunity.microsoft.com/t5/user/viewprofilepage/user-id/988334#profile)\\n- [Create A Chatbot with Internet Connectivity Powered by Langchain and Chainlit](https://levelup.gitconnected.com/create-a-chatbot-with-internet-connectivity-powered-by-langchain-and-chainlit-cba86f57ab2e) from [Yeyu Hang](https://medium.com/@wenbohuang0307)\\n- [For Chatbot Development, Streamlit Is Good, But Chainlit Is Better](https://levelup.gitconnected.com/for-chatbot-development-streamlit-is-good-but-chainlit-is-better-4112f9473a69) from [Yeyu Hang](https://medium.com/@wenbohuang0307)\\n- [Build and Deploy a Chat App Powered by LangChain and Chainlit using Docker](https://levelup.gitconnected.com/build-deploy-a-chat-app-powered-by-langchain-chainlit-using-docker-4f687da08625) from [MA Raza, Ph.D.](https://medium.com/gitconnected/build-deploy-a-chat-app-powered-by-langchain-chainlit-using-docker-4f687da08625)\\n\\nNote that some of those tutorials might use the old sync version of the package. See the [Migration Guide](/examples/openai-sql) to update those!\\n\\n\\n\\n================================================\\nFile: examples/cookbook.mdx\\n================================================\\n---\\ntitle: Cookbook\\n---\\n\\nThe Cookbook repository serves as a valuable resource and starting point for developers looking to explore the capabilities of Chainlit in creating LLM apps.\\n\\nIt provides a diverse collection of **example projects**, each residing in its own folder, showcasing the integration of various tools such as **OpenAI, Anthropiс, LangChain, LlamaIndex, ChromaDB, Pinecone and more**.\\n\\nWhether you are seeking basic tutorials or in-depth use cases, the Cookbook repository offers inspiration and practical insights!\\n\\n<Card\\n  title=\"https://github.com/Chainlit/cookbook\"\\n  icon=\"github\"\\n  href=\"https://github.com/Chainlit/cookbook\"\\n></Card>\\n\\n\\n\\n================================================\\nFile: examples/openai-sql.mdx\\n================================================\\n---\\ntitle: Text to SQL\\n---\\n\\nLet\\'s build a simple app that helps users to create SQL queries with natural language.\\n\\n<Frame caption=\"Preview of the final result\">\\n  <video controls autoPlay loop muted src=\"/images/text-to-sql.mp4\" />\\n</Frame>\\n\\n## Prerequisites\\n\\nThis example has extra dependencies. You can install them with:\\n\\n```bash\\npip install chainlit openai\\n```\\n\\n## Imports\\n\\n```python app.py\\nfrom openai import AsyncOpenAI\\n\\n\\nimport chainlit as cl\\n\\ncl.instrument_openai()\\n\\nclient = AsyncOpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\\n```\\n\\n## Define a prompt template and LLM settings\\n\\n````python app.py\\ntemplate = \"\"\"SQL tables (and columns):\\n* Customers(customer_id, signup_date)\\n* Streaming(customer_id, video_id, watch_date, watch_minutes)\\n\\nA well-written SQL query that {input}:\\n```\"\"\"\\n\\n\\nsettings = {\\n    \"model\": \"gpt-3.5-turbo\",\\n    \"temperature\": 0,\\n    \"max_tokens\": 500,\\n    \"top_p\": 1,\\n    \"frequency_penalty\": 0,\\n    \"presence_penalty\": 0,\\n    \"stop\": [\"```\"],\\n}\\n````\\n\\n## Add the Assistant Logic\\n\\nHere, we decorate the `main` function with the [@on_message](/api-reference/lifecycle-hooks/on-message) decorator to tell Chainlit to run the `main` function each time a user sends a message.\\n\\nThen, we wrap our text to sql logic in a [Step](/concepts/step).\\n\\n```python app.py\\n@cl.set_starters\\nasync def starters():\\n    return [\\n       cl.Starter(\\n           label=\">50 minutes watched\",\\n           message=\"Compute the number of customers who watched more than 50 minutes of video this month.\"\\n       )\\n    ]\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    stream = await client.chat.completions.create(\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": template.format(input=message.content),\\n            }\\n        ], stream=True, **settings\\n    )\\n\\n    msg = await cl.Message(content=\"\", language=\"sql\").send()\\n\\n    async for part in stream:\\n        if token := part.choices[0].delta.content or \"\":\\n            await msg.stream_token(token)\\n\\n    await msg.update()\\n```\\n\\n## Try it out\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nYou can ask questions like `Compute the number of customers who watched more than 50 minutes of video this month`.\\n\\n\\n\\n================================================\\nFile: examples/qa.mdx\\n================================================\\n---\\ntitle: Document QA\\n---\\n\\nIn this example, we\\'re going to build an chatbot QA app. We\\'ll learn how to:\\n\\n- Upload a document\\n- Create vector embeddings from a file\\n- Create a chatbot app with the ability to display sources used to generate an answer\\n\\nThis example is inspired from the [LangChain doc](https://python.langchain.com/en/latest/use_cases/question_answering.html)\\n\\n## Prerequisites\\n\\nThis example has extra dependencies. You can install them with:\\n\\n```bash\\npip install langchain langchain-community chromadb tiktoken openai langchain-openai\\n```\\n\\nThen, you need to go to create an OpenAI key [here](https://platform.openai.com/account/api-keys).\\n\\n<Note>\\n  The state of the union file is available\\n  [here](https://github.com/Chainlit/cookbook/blob/main/llama-index/data/state_of_the_union.txt)\\n</Note>\\n\\n## Conversational Document QA with LangChain\\n\\n```python qa.py\\nimport os\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain.chains import (\\n    ConversationalRetrievalChain,\\n)\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\nfrom langchain_community.chat_message_histories import ChatMessageHistory\\nfrom langchain.memory import ConversationBufferMemory\\n\\nimport chainlit as cl\\n\\nos.environ[\"OPENAI_API_KEY\"] = (\\n    \"OPENAI_API_KEY\"\\n)\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    files = None\\n\\n    # Wait for the user to upload a file\\n    while files is None:\\n        files = await cl.AskFileMessage(\\n            content=\"Please upload a text file to begin!\",\\n            accept=[\"text/plain\"],\\n            max_size_mb=20,\\n            timeout=180,\\n        ).send()\\n\\n    file = files[0]\\n\\n    msg = cl.Message(content=f\"Processing `{file.name}`...\")\\n    await msg.send()\\n\\n    with open(file.path, \"r\", encoding=\"utf-8\") as f:\\n        text = f.read()\\n\\n    # Split the text into chunks\\n    texts = text_splitter.split_text(text)\\n\\n    # Create a metadata for each chunk\\n    metadatas = [{\"source\": f\"{i}-pl\"} for i in range(len(texts))]\\n\\n    # Create a Chroma vector store\\n    embeddings = OpenAIEmbeddings()\\n    docsearch = await cl.make_async(Chroma.from_texts)(\\n        texts, embeddings, metadatas=metadatas\\n    )\\n\\n    message_history = ChatMessageHistory()\\n\\n    memory = ConversationBufferMemory(\\n        memory_key=\"chat_history\",\\n        output_key=\"answer\",\\n        chat_memory=message_history,\\n        return_messages=True,\\n    )\\n\\n    # Create a chain that uses the Chroma vector store\\n    chain = ConversationalRetrievalChain.from_llm(\\n        ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True),\\n        chain_type=\"stuff\",\\n        retriever=docsearch.as_retriever(),\\n        memory=memory,\\n        return_source_documents=True,\\n    )\\n\\n    # Let the user know that the system is ready\\n    msg.content = f\"Processing `{file.name}` done. You can now ask questions!\"\\n    await msg.update()\\n\\n    cl.user_session.set(\"chain\", chain)\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    chain = cl.user_session.get(\"chain\")  # type: ConversationalRetrievalChain\\n    cb = cl.AsyncLangchainCallbackHandler()\\n\\n    res = await chain.acall(message.content, callbacks=[cb])\\n    answer = res[\"answer\"]\\n    source_documents = res[\"source_documents\"]  # type: List[Document]\\n\\n    text_elements = []  # type: List[cl.Text]\\n\\n    if source_documents:\\n        for source_idx, source_doc in enumerate(source_documents):\\n            source_name = f\"source_{source_idx}\"\\n            # Create the text element referenced in the message\\n            text_elements.append(\\n                cl.Text(\\n                    content=source_doc.page_content, name=source_name, display=\"side\"\\n                )\\n            )\\n        source_names = [text_el.name for text_el in text_elements]\\n\\n        if source_names:\\n            answer += f\"\\\\nSources: {\\', \\'.join(source_names)}\"\\n        else:\\n            answer += \"\\\\nNo sources found\"\\n\\n    await cl.Message(content=answer, elements=text_elements).send()\\n```\\n\\n## Try it out\\n\\n```bash\\nchainlit run qa.py\\n```\\n\\nYou can then upload any `.txt` file to the UI and ask questions about it.\\nIf you are using `state_of_the_union.txt` you can ask questions like `What did the president say about Ketanji Brown Jackson?`.\\n\\n![QA](/images/qa.png)\\n\\n\\n\\n================================================\\nFile: examples/security.mdx\\n================================================\\n---\\ntitle: \"Security - PII\"\\n---\\n\\nWhen building chat applications, it\\'s crucial to ensure the secure handling of sensitive data, especially Personal Identifiable Information (PII). PII can be directly or indirectly linked to an individual, making it essential to protect user privacy by preventing the transmission of such data to language models.\\n\\n### Example of PII\\n\\nConsider the text below, where PII has been highlighted:\\n\\n> Hello, my name is **John** and I live in **New York**.\\n> My credit card number is **3782-8224-6310-005** and my phone number is **(212) 688-5500**.\\n\\nAnd here is the anonymized version:\\n\\n> Hello, my name is \\\\<PERSON\\\\> and I live in \\\\<LOCATION\\\\>. My credit card number is \\\\<CREDIT_CARD\\\\> and my phone number is \\\\<PHONE_NUMBER\\\\>.\\n\\n## Analyze and anonymize data\\n\\nIntegrate [Microsoft Presidio](https://microsoft.github.io/presidio/) for robust data sanitization in your Chainlit application.\\n\\n```python Code Example\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    # Notice that the message is passed as is\\n    response = await cl.Message(\\n        content=f\"Received: {message.content}\",\\n    ).send()\\n```\\n\\nBefore proceeding, ensure that the Python packages required for PII analysis and anonymization are installed. Run the following commands in your terminal to install them:\\n\\n```shell\\npip install presidio-analyzer presidio-anonymizer spacy\\npython -m spacy download en_core_web_lg\\n```\\n\\nCreate an async context manager that utilizes the Presidio Analyzer to inspect the incoming text for any PII. This context manager can be included in your main function to scrutinize messages before they are processed.\\nWhen PII is detected, you should present the user with the option to either continue or cancel the operation. Use Chainlit\\'s messaging system to accomplish this.\\n\\n```python Code Example\\nfrom presidio_analyzer import AnalyzerEngine\\nfrom contextlib import asynccontextmanager\\n\\nanalyzer = AnalyzerEngine()\\n\\n@asynccontextmanager\\nasync def check_text(text: str):\\n  pii_results = analyzer.analyze(text=text, language=\"en\")\\n\\n  if pii_results:\\n    response = await cl.AskActionMessage(\\n      content=\"PII detected\",\\n      actions=[\\n        cl.Action(name=\"continue\", payload={\"value\": \"continue\"}, label=\"✅ Continue\"),\\n        cl.Action(name=\"cancel\", payload={\"value\": \"continue\"}, label=\"❌ Cancel\"),\\n      ],\\n    ).send()\\n\\n    if response is None or response.get(\"payload\").get(\"value\") == \"cancel\":\\n      raise InterruptedError\\n\\n  yield\\n\\n# ...\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n  async with check_text(message.content):\\n    # This block is only executed when the user press \"Continue\"\\n    response = await cl.Message(\\n        content=f\"Received: {message.content}\",\\n    ).send()\\n```\\n\\nIf your application has a requirement to anonymize PII, Presidio can also do that. Modify the check_text context manager to return anonymized text when PII is detected.\\n\\n```python Code Example\\nfrom presidio_anonymizer import AnonymizerEngine\\n\\nanonymizer = AnonymizerEngine()\\n\\n@asynccontextmanager\\nasync def check_text(text: str):\\n  pii_results = analyzer.analyze(text=text, language=\"en\")\\n\\n  if pii_results:\\n    response = await cl.AskActionMessage(\\n      content=\"PII detected\",\\n      actions=[\\n        cl.Action(name=\"continue\", payload={\"value\": \"continue\"}, label=\"✅ Continue\"),\\n        cl.Action(name=\"cancel\", payload={\"value\": \"continue\"}, label=\"❌ Cancel\"),\\n      ],\\n    ).send()\\n\\n    if response is None or response.get(\"payload\").get(\"value\") == \"cancel\":\\n      raise InterruptedError\\n\\n    yield anonymizer.anonymize(\\n      text=text,\\n      analyzer_results=pii_results,\\n    ).text\\n  else:\\n    yield text\\n\\n# ...\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n  async with check_text(message.content) as anonymized_message:\\n    response = await llm_chain.arun(\\n      anonymized_message\\n      callbacks=[cl.AsyncLangchainCallbackHandler()]\\n    )\\n```\\n\\n\\n\\n================================================\\nFile: get-started/installation.mdx\\n================================================\\n---\\ntitle: Installation\\n---\\n\\nChainlit requires `python>=3.9`.\\n\\nYou can install Chainlit it via pip as follows:\\n\\n```bash\\npip install chainlit\\n```\\n\\nThis will make the `chainlit` command available on your system.\\n\\nMake sure everything runs smoothly:\\n\\n```bash\\nchainlit hello\\n```\\n\\nThis should spawn the chainlit UI and ask for your name like so:\\n![Hello](/images/hello.png)\\n\\n## Next steps\\n\\n<CardGroup cols={2}>\\n<Card\\n  title=\"In Pure Python\"\\n  color=\"#ea5a0c\"\\n  icon=\"python\"\\n  href=\"/get-started/pure-python\">\\n  Learn on how to use Chainlit with any python code.\\n</Card>\\n\\n<Card\\n  title=\"Integrations\"\\n  color=\"#0285c7\"\\n  icon=\"link\"\\n  href=\"/integrations\">\\n  Integrate Chainlit with other frameworks.\\n</Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: get-started/overview.mdx\\n================================================\\n---\\ntitle: \"Overview\"\\n---\\n\\nChainlit is an open-source Python package to build production ready Conversational AI.\\n\\n<Frame caption=\"Build Conversational AI with Chainlit\">\\n  <video\\n    controls\\n    autoPlay\\n    loop\\n    muted\\n    src=\"/images/overview.mp4\"\\n  />\\n</Frame>\\n\\n## Key features\\n\\n1. [Build fast:](/examples/openai-sql) Get started in a couple lines of Python\\n\\n2. [Authentication:](/authentication/overview) Integrate with corporate identity providers and existing authentication infrastructure\\n\\n3. [Data persistence:](/data-persistence/overview) Collect, monitor and analyze data from your users\\n\\n4. [Visualize multi-steps reasoning:](/concepts/step) Understand the intermediary steps that produced an output at a glance\\n\\n5. [Multi Platform:](/deploy/overview) Write your assistant logic once, use everywhere\\n\\n## Integrations\\n\\nChainlit is compatible with all Python programs and libraries. That being said, it comes with a set of integrations with popular libraries and frameworks.\\n\\n<CardGroup cols={2}>\\n  <Card\\n  title=\"OpenAI\"\\n  icon=\"circle\"\\n  color=\"#dddddd\"\\n  href=\"/integrations/openai\">\\n    Learn how to explore your OpenAI calls in Chainlit.\\n  </Card>\\n\\n<Card\\n  title=\"OpenAI Assistant\"\\n  icon=\"circle\"\\n  color=\"#F80061\"\\n  href=\"https://github.com/Chainlit/cookbook/tree/main/openai-data-analyst\"\\n>\\n  Learn how to integrate your OpenAI Assistants with Chainlit.\\n</Card>\\n\\n<Card\\n  title=\"Mistral AI\"\\n  icon=\"circle\"\\n  color=\"#ea5a0c\"\\n  href=\"/integrations/mistralai\"\\n>\\n  Learn how to use any Mistral AI calls in Chainlit.\\n</Card>\\n\\n<Card\\n  title=\"Llama Index\"\\n  icon=\"circle\"\\n  color=\"#0285c7\"\\n  href=\"/integrations/llama-index\"\\n>\\n  Learn how to integrate your Llama Index code with Chainlit.\\n</Card>\\n\\n<Card\\n  title=\"LangChain\"\\n  icon=\"circle\"\\n  color=\"#3afadc\"\\n  href=\"/integrations/langchain\"\\n>\\n  Learn how to use any LangChain agent with Chainlit.\\n</Card>\\n\\n<Card\\n  title=\"Autogen\"\\n  icon=\"circle\"\\n  color=\"#F80061\"\\n  href=\"https://github.com/Chainlit/cookbook/tree/main/pyautogen\"\\n>\\n  Learn how to integrate your Autogen agents with Chainlit.\\n</Card>\\n\\n<Card\\n  title=\"Haystack\"\\n  icon=\"circle\"\\n  color=\"#16a34a\"\\n  href=\"/integrations/haystack\"\\n>\\n  Learn how to integrate your Haystack code with Chainlit.\\n</Card>\\n\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: get-started/pure-python.mdx\\n================================================\\n---\\ntitle: In Pure Python\\n---\\n\\nIn this tutorial, we\\'ll walk through the steps to create a minimal LLM app.\\n\\n## Prerequisites\\n\\nBefore getting started, make sure you have the following:\\n\\n- A working installation of Chainlit\\n- Basic understanding of Python programming\\n\\n## Step 1: Create a Python file\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 2: Write the Application Logic\\n\\nIn `app.py`, import the Chainlit package and define a function that will handle incoming messages from the chatbot UI. Decorate the function with the `@cl.on_message` decorator to ensure it gets called whenever a user inputs a message.\\n\\nHere\\'s the basic structure of the script:\\n\\n```python app.py\\nimport chainlit as cl\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    # Your custom logic goes here...\\n\\n    # Send a response back to the user\\n    await cl.Message(\\n        content=f\"Received: {message.content}\",\\n    ).send()\\n```\\n\\nThe `main` function will be called every time a user inputs a message in the chatbot UI. You can put your custom logic within the function to process the user\\'s input, such as analyzing the text, calling an API, or computing a result.\\n\\nThe [Message](/api-reference/message) class is responsible for sending a reply back to the user. In this example, we simply send a message containing the user\\'s input.\\n\\n## Step 3: Run the Application\\n\\nTo start your Chainlit app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n![PythonExample](/images/python-example.png)\\n\\n## Next Steps\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Concepts\" icon=\"lightbulb\" color=\"#ea5a0c\" href=\"/concepts\">\\n    Learn about the core concepts of Chainlit\\n  </Card>\\n  <Card\\n    title=\"Cookbook\"\\n    icon=\"book\"\\n    color=\"#0285c7\"\\n    href=\"https://github.com/Chainlit/cookbook\"\\n  >\\n    Explore the Chainlit cookbook for more examples\\n  </Card>\\n</CardGroup>\\n\\n\\n\\n================================================\\nFile: guides/iframe.mdx\\n================================================\\n---\\ntitle: Embed in Iframe\\n---\\n\\nThis section outlines the steps and specifications for embedding the external Chatbot UI, provided by Chainlit, into an existing frontend service. This integration is achieved using an HTML `<iframe>`. Below we detail the properties and considerations that need attention.\\n\\n## Overview\\n\\nThe `<iframe>` tag specifies an inline frame. It is used to embed another document within the current HTML document. In this scenario, we\\'re embedding the Chainlit Chatbot interface that resides at a different URL.\\n\\n## Code Snippet\\n\\nHere is the basic setup of the `<iframe>` used to embed the Chatbot UI:\\n\\n```html\\n<iframe\\n    src=\"https://your-chainlit-instance.com\"  /* URL of the chatbot interface */\\n    width=\"100%\"  /* The iframe takes the full width of the parent element */\\n    height=\"600px\"  /* Fixed height of the iframe */\\n    frameBorder=\"0\"  /* No border around the iframe */\\n    title=\"Embedded Webpage\"  /* Descriptive title for accessibility purposes */\\n    style={{ border: \\'none\\' }}  /* Additional styling options */\\n></iframe>\\n```\\n\\n## Attributes and Style\\n\\n- `src`: The URL of the page to embed. This should be the URL of the Chainlit chatbot interface.\\n- `width` & `height`: These attributes define the size of the iframe. The width is set to 100% of the parent container, and the height is a fixed 600px. Adjust according to your layout requirements.\\n- `frameBorder`: This attribute specifies whether or not to display a border around the iframe. It\\'s set to \"0\" to avoid any border, making the embedded page blend seamlessly with your content.\\n- `title`: A text description of the frame\\'s contents, providing essential information for assistive technologies, such as screen readers.\\n- `style`: Inline CSS for the iframe. Here, it\\'s used to ensure there\\'s no border around the iframe, but it can include other styles as necessary.\\n\\n## Security Considerations\\n\\nWhen embedding iframes, consider the following security implications:\\n\\n- **Mixed Content**: Ensure that the embedded content is served over HTTPS to prevent mixed content issues, where secure pages (served over HTTPS) also contain elements using the insecure HTTP protocol.\\n- **Sandboxing**: If additional security is required, especially when embedding content from untrusted sources, consider using the `sandbox` attribute to impose restrictions on the content.\\n- **Content Security Policy (CSP)**: If your website uses CSP, ensure that the policy allows for the embedding of external content from the specified URL.\\n\\n## Accessibility\\n\\nEnsure the embedded interface adheres to accessibility guidelines:\\n\\n- The `title` attribute should accurately describe the embedded content for screen readers.\\n- Ensure that the chatbot interface within the iframe itself is built according to accessibility standards, with features such as keyboard navigation and ARIA roles and properties.\\n\\n## Responsive Design\\n\\nTo maintain a responsive design:\\n\\n- Test the iframe in multiple viewports to see how it responds to different screen sizes. The width is set to 100%, so it should adjust to the parent container\\'s width, but other aspects might not adjust as cleanly.\\n- Consider using CSS and JavaScript to adjust the iframe\\'s dimensions dynamically based on the viewport size.\\n\\n## Troubleshooting\\n\\n- **Blocked Content**: If the iframe content doesn\\'t load, ensure it’s not being blocked by browser policies or extensions.\\n- **Cross-Origin Issues**: Check for console errors related to CORS. The server where the chatbot UI is hosted must set appropriate headers allowing the content to be embedded on your domain.\\n- **Layout Shifts**: If the layout shifts unexpectedly, verify the fixed height of the iframe isn\\'t causing visual issues, especially on smaller screens.\\n\\n## Conclusion\\n\\nEmbedding the Chainlit chatbot interface within an iframe allows users to interact with the chatbot directly on our platform. However, it requires careful attention to security, accessibility, and responsive design. Regular testing and updates are necessary to maintain the integrity and user-friendliness of the integration.\\n\\n\\n\\n================================================\\nFile: guides/sync-async.mdx\\n================================================\\n---\\ntitle: Async / Sync\\n---\\n\\nAsynchronous programming is a powerful way to handle multiple tasks concurrently without blocking the execution of your program. Chainlit is async by default to allow agents to execute tasks in parallel and allow multiple users on a single app.\\nPython introduced the `asyncio` library to make it easier to write asynchronous code using the `async/await` syntax. This onboarding guide will help you understand the basics of asynchronous programming in Python and how to use it in your Chainlit project.\\n\\n### Understanding async/await\\n\\nThe `async` and `await` keywords are used to define and work with asynchronous code in Python. An `async` function is a coroutine, which is a special type of function that can pause its execution and resume later, allowing other tasks to run in the meantime.\\n\\nTo define an async function, use the `async def` syntax:\\n\\n```python\\nasync def my_async_function():\\n    # Your async code goes here\\n```\\n\\nTo call an async function, you need to use the `await` keyword:\\n\\n```python\\nasync def another_async_function():\\n    result = await my_async_function()\\n```\\n\\n### Working with Chainlit\\n\\nChainlit uses asynchronous programming to handle events and tasks efficiently. When creating a Chainlit agent, you\\'ll often need to define async functions to handle events and perform actions.\\n\\nFor example, to create an async function that responds to messages in Chainlit:\\n\\n```python\\nimport chainlit as cl\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    # Your custom logic goes here\\n\\n    # Send a response back to the user\\n    await cl.Message(\\n        content=f\"Received: {message.content}\",\\n    ).send()\\n```\\n\\n### Long running synchronous tasks\\n\\nIn some cases, you need to run long running synchronous functions in your Chainlit project. To prevent blocking the event loop, you can utilize the `make_async` function provided by the Chainlit library to transform a synchronous function into an asynchronous one:\\n\\n```python\\nfrom chainlit import make_async\\n\\ndef my_sync_function():\\n    # Your synchronous code goes here\\n    import time\\n    time.sleep(10)\\n    return 0\\n\\nasync_function = make_async(my_sync_function)\\n\\nasync def main():\\n    result = await async_function()\\n```\\n\\nBy using this approach, you can maintain the non-blocking nature of your project while still incorporating synchronous functions when necessary.\\n\\n### Call an async function from a sync function\\n\\nIf you need to run an asynchronous function inside a sync function, you can use the `run_sync` function provided by the Chainlit library:\\n\\n```python\\nfrom chainlit import run_sync\\n\\nasync def my_async_function():\\n    # Your asynchronous code goes here\\n\\ndef main():\\n    result = run_sync(my_async_function())\\n\\nmain()\\n```\\n\\nBy following this guide, you should now have a basic understanding of asynchronous programming in Python and how to use it in your Chainlit project.\\nAs you continue to work with Chainlit, you\\'ll find that async/await and the asyncio library provide a powerful and efficient way to handle multiple agents/tasks concurrently.\\n\\n\\n\\n================================================\\nFile: guides/migration/1.0.500.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v1.0.500\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## What changes?\\n\\nFull changelog available [here](https://github.com/Chainlit/chainlit/blob/main/CHANGELOG.md#10500---2023-04-02).\\n\\n## How to migrate?\\n\\n### 1. Regenerate translations\\n\\nSince translation files have been updated, you need to regenerate them. To do so, remove the `.chainlit/translations` folder and restart your application.\\n\\n### 2. Update the multi_modal config setting\\n\\nThe `multi_modal` config setting has been updated. You can either remove the entire `./chainlit/config.toml` file and restart your app or update the `multi_modal` setting manually (example [here](http://localhost:3002/backend/config/features#default-configuration)).\\n\\n\\n\\n================================================\\nFile: guides/migration/1.1.0.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v1.1.0\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## What changes?\\n\\nFull changelog available [here](https://github.com/Chainlit/chainlit/blob/main/CHANGELOG.md#110---2024-05-13).\\n\\n## How to migrate?\\n\\n### Rename the multi_modal config setting\\n\\nThe `multi_modal` config setting has been renamed `spontaneous_file_upload`. You can either remove the entire `./chainlit/config.toml` file and restart your app or rename the `multi_modal` setting manually (example [here](/backend/config/features#default-configuration)).\\n\\n<Warning>\\n  The cl.Message.send() method no longer returns the id of the message but the\\n  message itself. If you were using the id, you will need to update your code.\\n</Warning>\\n\\n\\n\\n================================================\\nFile: guides/migration/1.1.300.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v1.1.300\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## New Feature: Starters\\n\\n<Frame caption=\"Starters example\">\\n  <img src=\"/images/starters.gif\" />\\n</Frame>\\n\\nThis release introduces a new feature called Starters. Starters are suggestions to help your users get started with your assistant.\\nYou can declare up to 4 starters and optionally define an icon for each one.\\n\\n```python starters.py\\nimport chainlit as cl\\n\\n@cl.set_starters\\nasync def set_starters():\\n    return [\\n        cl.Starter(\\n            label=\"Morning routine ideation\",\\n            message=\"Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.\",\\n            icon=\"/public/idea.svg\",\\n            ),\\n\\n        cl.Starter(\\n            label=\"Explain superconductors\",\\n            message=\"Explain superconductors like I\\'m five years old.\",\\n            icon=\"/public/learn.svg\",\\n            ),\\n        cl.Starter(\\n            label=\"Python script for daily email reports\",\\n            message=\"Write a script to automate sending daily email reports in Python, and walk me through how I would set it up.\",\\n            icon=\"/public/terminal.svg\",\\n            ),\\n        cl.Starter(\\n            label=\"Text inviting friend to wedding\",\\n            message=\"Write a text asking a friend to be my plus-one at a wedding next month. I want to keep it super short and casual, and offer an out.\",\\n            icon=\"/public/write.svg\",\\n            )\\n        ]\\n# ...\\n```\\n\\nStarters also work with Chat Profiles. You can define different starters for different chat profiles.\\n\\n```python starters_with_chat_profiles.py\\n@cl.set_chat_profiles\\nasync def chat_profile(current_user: cl.User):\\n    if current_user.metadata[\"role\"] != \"ADMIN\":\\n        return None\\n\\n    return [\\n        cl.ChatProfile(\\n            name=\"My Chat Profile\",\\n            icon=\"https://picsum.photos/250\",\\n            markdown_description=\"The underlying LLM model is **GPT-3.5**, a *175B parameter model* trained on 410GB of text data.\",\\n            starters=[\\n                cl.Starter(\\n                    label=\"Morning routine ideation\",\\n                    message=\"Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.\",\\n                    icon=\"/public/idea.svg\",\\n                ),\\n                cl.Starter(\\n                    label=\"Explain superconductors\",\\n                    message=\"Explain superconductors like I\\'m five years old.\",\\n                    icon=\"/public/learn.svg\",\\n                ),\\n            ],\\n        )\\n    ]\\n```\\n\\n## Rework: Debugging\\n\\nWe created Chainlit with a vision to make debugging as easy as possible. This is why Chainlit was supporting complex Chain of Thoughts and even had its own prompt playground.\\nThis was great but was mixing two different concepts in one place:\\n\\n1. Building conversational AI with best in class user experience.\\n2. Debugging and iterating efficiently.\\n\\nSeparating these two concepts was the right thing to do to:\\n\\n1. Provide an even better UX (see the new [Chain of Thought](#chain-of-thought-rework)).\\n2. Provide an even better debugging experience.\\n\\nYou can enable the new debug mode by adding `-d` to your `chainlit run` command. If your data layer supports it (like [Literal AI](https://literalai.com)), you will see a debug button below each message taking you to the trace/prompt playground.\\n\\n<Frame caption=\"Debug example\">\\n  <img src=\"/images/debug.gif\" />\\n</Frame>\\n\\nThis also means we let go of the prompt playground in Chainlit and welcome a simplified Chain of Thought for your users!\\n\\n## Rework: Chain of Thought\\n\\nThe Chain of Thought has been reworked to only be one level deep and only include tools; ultimately users are only interested in the tools used by the LLM to generate the response.\\n\\n```python new_cot.py\\nimport chainlit as cl\\n\\n@cl.step(type=\"tool\")\\nasync def tool():\\n    # Faking a tool\\n    await cl.sleep(2)\\n    return \"Tool Response\"\\n\\n@cl.on_message\\nasync def on_message():\\n    msg = await cl.Message(\"\").send()\\n    msg.content = await tool()\\n    await msg.update()\\n```\\n\\n<Warning>\\n  Notice that the `root` attribute of [cl.Step](/concepts/step) has been\\n  removed. Use [cl.Message](/concepts/message) to send root level messages.\\n</Warning>\\n\\n<Frame caption=\"New CoT\">\\n  <video controls autoPlay loop src=\"/images/cot.mp4\" />\\n</Frame>\\n\\nThe data layer is still be able to provide the full Chain of Thought for debugging purposes.\\n\\n## Rework: Avatars\\n\\nThe previous `cl.Avatar` element was adding overhead to developers forcing them to resend the avatars to each session.\\nIt was also not working with resumed conversations.\\n\\n`cl.Avatar` has been removed entirely. Now, you should place your avatar files in `/public/avatars`.\\nLet\\'s say your message author is `My Assistant`, then you should place the avatar in `/public/avatars/my-assistant.png`.\\n\\nIf no avatar is found, it will default to the favicon.\\n\\n## Rework: Custom Endpoints\\n\\nChainlit is now mountable as a FastAPI sub application. This allows you to use Chainlit on your existing FastAPI application.\\n\\nCheck the [FastAPI integration](/integrations/fastapi) and [API documentation](/integrations/fastapi) for more information.\\n\\n## Minor Changes\\n\\n1. You can now configure the default theme in the `config.toml` file.\\n\\n```toml config.toml\\n[UI]\\n  [UI.theme]\\n      default = \"dark\"\\n```\\n\\n2. The `running`, `took_one` and `took_other` translations have been replaced by `used`.\\n   Either manually replace them in your translations or delete the translations file to regenerate it.\\n\\n3. The `show_readme_as_default` config has been removed in favor of starters.\\n\\n4. Root level messages will no longer collapse.\\n\\n## Conclusion\\n\\n<Note>\\n  Full changelog available\\n  [here](https://github.com/Chainlit/chainlit/blob/main/CHANGELOG.md#11300rc0---2024-05-27).\\n</Note>\\n\\nThis pre-release brings a lot of changes to Chainlit. It is not yet stable, but we are excited to hear your feedback on it and improve it further!\\n\\n\\n\\n================================================\\nFile: guides/migration/1.1.400.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v1.1.400\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## More control over Chain of Thought\\n\\nThe `hide_cot` config parameter has been replaced with `cot`. The `cot` parameter can be set to `hidden`, `tool_call`, or `full`. This parameter controls the display of the Chain of Thought (COT) in the UI.\\n\\n## `disable_feedback` is gone\\n\\nChainlit 1.1.400 takes a different approach to feedback. Now, a user input will trigger a run. Once the run is complete, the user can provide feedback for the whole run instead of being able to score each message. This change simplifies the feedback process and makes it more intuitive.\\n\\n\\n\\n================================================\\nFile: guides/migration/1.1.404.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v1.1.404\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## Breaking Changes\\n\\n### Python Version Requirement\\n\\nChainlit 1.1.404 requires Python 3.9 or higher. Ensure you\\'re using a compatible Python version before upgrading.\\n\\n### Security Changes\\n\\nChainlit now listens on 127.0.0.1 (localhost) instead of 0.0.0.0 (public) for improved security.\\n\\n#### For Containerized Deployments\\n\\nIf you\\'re using containerized deployments, you may need to specify `--host 0.0.0.0` for your container to work correctly with the new security changes.\\n\\n## New Features and Changes\\n\\n### Environment Variable for Custom Config Locations\\n\\nYou can now use the `CHAINLIT_APP_ROOT` environment variable to specify custom config locations.\\n\\n### Improved Error Handling\\n\\n- HTTP errors in data layers are now handled more gracefully.\\n- Fixed an AttributeError in the llama_index integration.\\n\\n### Configuration Update\\n\\nThe `edit_message` placement in the default config has been corrected. Check your `config.toml` file and update if necessary.\\n\\n## Best Practices\\n\\n1. **Review Your Python Environment**: Ensure you\\'re using Python 3.9 or higher.\\n2. **Update Containerized Deployments**: If using containers, adjust your configurations to include `--host 0.0.0.0` if needed.\\n3. **Check Custom Configurations**: If you\\'ve customized your Chainlit configuration, review it against the new defaults.\\n4. **Test Your Integration**: If you\\'re using the llama_index integration, test thoroughly after upgrading.\\n\\n## Additional Notes\\n\\n- The frontend connection resuming after connection loss has been fixed.\\n- A new pytest-based testing infrastructure has been implemented for improved stability.\\n\\nRemember to thoroughly test your application after upgrading to ensure compatibility with these changes.\\n\\n\\n\\n================================================\\nFile: guides/migration/2.0.0.mdx\\n================================================\\n---\\ntitle: Migrate to Chainlit v2.0.0\\n---\\n\\n<Note>Join the discord for live updates: https://discord.gg/AzyvDHWARx</Note>\\n\\n## Updating Chainlit\\n\\nBegin the migration by updating Chainlit to the latest version:\\n\\n```bash\\npip install --upgrade chainlit\\n```\\n\\n## What changes?\\n\\nThe Chainlit UI (including the copilot) has been completely re-written with Shadcn/Tailwind. This brings several advantages:\\n1. The codebase is simpler and more contribution friendly.\\n2. It enabled the new custom element feature.\\n3. The theme customisation is more powerful.\\n\\nFull changelog available [here](https://github.com/Chainlit/chainlit/blob/main/CHANGELOG.md#200---2025-01-06).\\n\\n## How to migrate?\\n\\n### 1. Regenerate the config file\\n\\nThe following fields have been removed from the `config.toml` file:\\n\\n1. **follow_symlink**: Chainlit no longer uses `StaticFiles` to serve files.\\n2. **font_family**, **custom_font**, **[UI.theme]**: Theme customisation now uses a [separate file](/customisation/theme).\\n2. **audio**: Chainlit audio streaming has been rework to match the [realtime APIs](/advanced-features/multi-modal).\\n\\nYou can either manually remove those field or remove the `.chainlit/config.toml` file and restart your application.\\n\\n### 2. Cookie Auth & Cross Origins\\n\\nAll of the authentication mechanisms now use cookie auth instead of directly using a JWT. This change makes Chainlit more secure.\\n\\nThis does not require any change in your app code. However, this implies that Chainlit is now more picky about cross origins (for instance when using a copilot on a website).\\n\\nIf you need to consume a Chainlit app on a different origin, make sure you allow it in the `config.toml` under `allow_origins`.\\n\\n### 3. Actions\\n\\n1. The **value** field has replaced with `payload` which accepts a Python dict. This makes actions more useful.\\n2. The **description** field has been renamed `tooltip`.\\n3. The field `icon` has been added. You can use any lucide icon name.\\n3. The **collapsed** field has been removed.\\n\\n### 4. Copilot Widget Config\\n\\n1. The **fontFamily** field has been removed. Check the [new custom theme documentation](/customisation/theme).\\n2. the `button.style` field has been replaced with `button.className`. You can use any tailwind class to style the widget button.\\n\\n\\n\\n\\n\\n================================================\\nFile: integrations/embedchain.mdx\\n================================================\\n---\\ntitle: Embedchain\\n---\\n\\nIn this tutorial, we\\'ll walk through the steps to create a Chainlit application integrated with [Embedchain](https://github.com/embedchain/embedchain).\\n\\n<img src=\"/images/embedchain-example.gif\" alt=\"Preview of what you\\'ll be building\" />\\n\\n## Step 1: Create a Chainlit Application\\n\\nIn `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.\\n\\n### With Embedchain\\n\\n```python app.py\\nimport chainlit as cl\\nfrom embedchain import Pipeline as App\\n\\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    app = App.from_config(config={\\n        \\'app\\': {\\n            \\'config\\': {\\n                \\'name\\': \\'chainlit-app\\'\\n            }\\n        },\\n        \\'llm\\': {\\n            \\'config\\': {\\n                \\'stream\\': True,\\n            }\\n        }\\n    })\\n    # import your data here\\n    app.add(\"https://www.forbes.com/profile/elon-musk/\")\\n    app.collect_metrics = False\\n    cl.user_session.set(\"app\", app)\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    app = cl.user_session.get(\"app\")\\n    msg = cl.Message(content=\"\")\\n    for chunk in await cl.make_async(app.chat)(message.content):\\n        await msg.stream_token(chunk)\\n    \\n    await msg.send()\\n```\\n## Step 2: Run the Application\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\n## Next Steps\\n\\nCongratulations! You\\'ve just created your first LLM app with Chainlit and Embedchain.\\n\\nHappy coding! 🎉\\n\\n\\n\\n================================================\\nFile: integrations/fastapi.mdx\\n================================================\\n---\\ntitle: FastAPI\\n---\\n\\nChainlit can be mounted as a FastAPI sub application.\\n\\n```py my_cl_app\\nimport chainlit as cl\\n\\n@cl.on_chat_start\\nasync def main():\\n    await cl.Message(content=\"Hello World\").send()\\n```\\n\\n```py main\\nfrom fastapi import FastAPI\\nfrom chainlit.utils import mount_chainlit\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/app\")\\ndef read_main():\\n    return {\"message\": \"Hello World from main app\"}\\n\\nmount_chainlit(app=app, target=\"my_cl_app.py\", path=\"/chainlit\")\\n```\\n\\nIn the example above, we have a FastAPI application with a single endpoint `/app`. We mount the Chainlit application `my_cl_app.py` to the `/chainlit` path.\\n\\nStart the FastAPI server:\\n\\n```bash\\nuvicorn main:app --host 0.0.0.0 --port 80\\n```\\n\\n<Note>\\nWhen using FastAPI integration, header authentication is the preferred method\\nfor authenticating users. This approach allows Chainlit to delegate the\\nauthentication process to the parent FastAPI application, providing a more\\nseamless and secure integration.\\n</Note>\\n\\n\\n\\n================================================\\nFile: integrations/langchain.mdx\\n================================================\\n---\\ntitle: LangChain/LangGraph\\n---\\n\\nIn this tutorial, we\\'ll walk through the steps to create a Chainlit application integrated with [LangChain](https://github.com/hwchase17/langchain).\\n\\n<Frame caption=\"Preview of what you will build\">\\n  <img src=\"/images/langchain-example.gif\" />\\n</Frame>\\n\\n## Prerequisites\\n\\nBefore getting started, make sure you have the following:\\n\\n- A working installation of Chainlit\\n- The LangChain package installed\\n- An OpenAI API key\\n- Basic understanding of Python programming\\n\\n## Step 1: Create a Python file\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 2: Write the Application Logic\\n\\nIn `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.\\n\\n### With LangChain\\n\\nLet\\'s go through a small example.\\n\\n<Note>\\n  If your agent/chain does not have an async implementation, fallback to the\\n  sync implementation.\\n</Note>\\n\\n<CodeGroup>\\n\\n```python Async LCEL\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain.schema.runnable import Runnable\\nfrom langchain.schema.runnable.config import RunnableConfig\\nfrom typing import cast\\n\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    model = ChatOpenAI(streaming=True)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"You\\'re a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\\n            ),\\n            (\"human\", \"{question}\"),\\n        ]\\n    )\\n    runnable = prompt | model | StrOutputParser()\\n    cl.user_session.set(\"runnable\", runnable)\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    runnable = cast(Runnable, cl.user_session.get(\"runnable\"))  # type: Runnable\\n\\n    msg = cl.Message(content=\"\")\\n\\n    async for chunk in runnable.astream(\\n        {\"question\": message.content},\\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\\n    ):\\n        await msg.stream_token(chunk)\\n\\n    await msg.send()\\n```\\n\\n```python Sync LCEL\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain.schema.runnable import Runnable\\nfrom langchain.schema.runnable.config import RunnableConfig\\n\\nimport chainlit as cl\\n\\n\\n@cl.on_chat_start\\nasync def on_chat_start():\\n    model = ChatOpenAI(streaming=True)\\n    prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"You\\'re a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\",\\n            ),\\n            (\"human\", \"{question}\"),\\n        ]\\n    )\\n    runnable = prompt | model | StrOutputParser()\\n    cl.user_session.set(\"runnable\", runnable)\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\\n\\n    msg = cl.Message(content=\"\")\\n\\n    for chunk in await cl.make_async(runnable.stream)(\\n        {\"question\": message.content},\\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\\n    ):\\n        await msg.stream_token(chunk)\\n\\n    await msg.send()\\n```\\n\\n</CodeGroup>\\n\\nThis code sets up an instance of `Runnable` with a custom `ChatPromptTemplate` for each chat session. The `Runnable` is invoked everytime a user sends a message to generate the response.\\n\\nThe callback handler is responsible for listening to the chain\\'s intermediate steps and sending them to the UI.\\n\\n### With LangGraph\\n\\n```python\\nfrom typing import Literal\\nfrom langchain_core.tools import tool\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.schema.runnable.config import RunnableConfig\\nfrom langchain_core.messages import HumanMessage\\n\\nimport chainlit as cl\\n\\n@tool\\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\\n    \"\"\"Use this to get weather information.\"\"\"\\n    if city == \"nyc\":\\n        return \"It might be cloudy in nyc\"\\n    elif city == \"sf\":\\n        return \"It\\'s always sunny in sf\"\\n    else:\\n        raise AssertionError(\"Unknown city\")\\n\\n\\ntools = [get_weather]\\nmodel = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\nfinal_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\nmodel = model.bind_tools(tools)\\n# NOTE: this is where we\\'re adding a tag that we\\'ll can use later to filter the model stream events to only the model called in the final node.\\n# This is not necessary if you call a single LLM but might be important in case you call multiple models within the node and want to filter events\\n# from only one of them.\\nfinal_model = final_model.with_config(tags=[\"final_node\"])\\ntool_node = ToolNode(tools=tools)\\n\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.graph.message import MessagesState\\nfrom langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\\n\\n\\ndef should_continue(state: MessagesState) -> Literal[\"tools\", \"final\"]:\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then we route to the \"tools\" node\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    # Otherwise, we stop (reply to the user)\\n    return \"final\"\\n\\n\\ndef call_model(state: MessagesState):\\n    messages = state[\"messages\"]\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n\\ndef call_final_model(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_ai_message = messages[-1]\\n    response = final_model.invoke(\\n        [\\n            SystemMessage(\"Rewrite this in the voice of Al Roker\"),\\n            HumanMessage(last_ai_message.content),\\n        ]\\n    )\\n    # overwrite the last AI message from the agent\\n    response.id = last_ai_message.id\\n    return {\"messages\": [response]}\\n\\n\\nbuilder = StateGraph(MessagesState)\\n\\nbuilder.add_node(\"agent\", call_model)\\nbuilder.add_node(\"tools\", tool_node)\\n# add a separate final node\\nbuilder.add_node(\"final\", call_final_model)\\n\\nbuilder.add_edge(START, \"agent\")\\nbuilder.add_conditional_edges(\\n    \"agent\",\\n    should_continue,\\n)\\n\\nbuilder.add_edge(\"tools\", \"agent\")\\nbuilder.add_edge(\"final\", END)\\n\\ngraph = builder.compile()\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    config = {\"configurable\": {\"thread_id\": cl.context.session.id}}\\n    cb = cl.LangchainCallbackHandler()\\n    final_answer = cl.Message(content=\"\")\\n    \\n    for msg, metadata in graph.stream({\"messages\": [HumanMessage(content=msg.content)]}, stream_mode=\"messages\", config=RunnableConfig(callbacks=[cb], **config)):\\n        if (\\n            msg.content\\n            and not isinstance(msg, HumanMessage)\\n            and metadata[\"langgraph_node\"] == \"final\"\\n        ):\\n            await final_answer.stream_token(msg.content)\\n\\n    await final_answer.send()\\n```\\n\\n## Step 3: Run the Application\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n<Warning>\\n  When using LangChain, prompts and completions are not cached by default. To\\n  enable the cache, set the `cache=true` in your chainlit config file.\\n</Warning>\\n\\n\\n\\n================================================\\nFile: integrations/litellm.mdx\\n================================================\\n---\\ntitle: LiteLLM\\n---\\n\\nIn this tutorial, we will guide you through the steps to create a Chainlit application integrated with [LiteLLM Proxy](https://docs.litellm.ai/docs/simple_proxy)\\n\\nThe benefits of using LiteLLM Proxy with Chainlit is:\\n\\n- You can [call 100+ LLMs in the OpenAI API format](https://docs.litellm.ai/docs/providers)\\n- Use Virtual Keys to set budget limits and track usage\\n- see LLM API calls in a step in the UI, and you can explore them in the prompt playground.\\n\\n<Warning>\\n  You shouldn\\'t configure this integration if you\\'re already using another\\n  integration like Haystack, Langchain or LlamaIndex. Both integrations would\\n  record the same generation and create duplicate steps in the UI.\\n</Warning>\\n\\n## Prerequisites\\n\\nBefore getting started, make sure you have the following:\\n\\n- A working installation of Chainlit\\n- The OpenAI package installed\\n- [LiteLLM Proxy Running](https://docs.litellm.ai/docs/proxy/deploy)\\n- [A LiteLLM Proxy API Key](https://docs.litellm.ai/docs/proxy/virtual_keys)\\n- Basic understanding of Python programming\\n\\n## Step 1: Create a Python file\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 2: Write the Application Logic\\n\\nIn `app.py`, import the necessary packages and define one function to handle messages incoming from the UI.\\n\\n```python\\nfrom openai import AsyncOpenAI\\nimport chainlit as cl\\nclient = AsyncOpenAI(\\n    api_key=\"anything\",            # litellm proxy virtual key\\n    base_url=\"http://0.0.0.0:4000\" # litellm proxy base_url\\n)\\n\\n# Instrument the OpenAI client\\ncl.instrument_openai()\\n\\nsettings = {\\n    \"model\": \"gpt-3.5-turbo\", # model you want to send litellm proxy\\n    \"temperature\": 0,\\n    # ... more settings\\n}\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    response = await client.chat.completions.create(\\n        messages=[\\n            {\\n                \"content\": \"You are a helpful bot, you always reply in Spanish\",\\n                \"role\": \"system\"\\n            },\\n            {\\n                \"content\": message.content,\\n                \"role\": \"user\"\\n            }\\n        ],\\n        **settings\\n    )\\n    await cl.Message(content=response.choices[0].message.content).send()\\n```\\n\\n## Step 3: Run the Application\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n\\n\\n================================================\\nFile: integrations/llama-index.mdx\\n================================================\\n---\\ntitle: Llama Index\\n---\\n\\nIn this tutorial, we will guide you through the steps to create a Chainlit application integrated with [Llama Index](https://github.com/jerryjliu/llama_index).\\n\\n<Frame caption=\"Preview of the app you\\'ll build\">\\n  <img src=\"/images/llama-index-example.gif\" />\\n</Frame>\\n\\n## Prerequisites\\n\\nBefore diving in, ensure that the following prerequisites are met:\\n\\n- A working installation of Chainlit\\n- The Llama Index package installed\\n- An OpenAI API key\\n- A basic understanding of Python programming\\n\\n## Step 1: Set Up Your Data Directory\\n\\nCreate a folder named `data` in the root of your app folder. Download the [state of the union](https://github.com/Chainlit/cookbook/blob/main/llama-index/data/state_of_the_union.txt) file (or any files of your own choice) and place it in the `data` folder.\\n\\n## Step 2: Create the Python Script\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 3: Write the Application Logic\\n\\nIn `app.py`, import the necessary packages and define one function to handle a new chat session and another function to handle messages incoming from the UI.\\n\\nIn this tutorial, we are going to use `RetrieverQueryEngine`. Here\\'s the basic structure of the script:\\n\\n```python app.py\\nimport os\\nimport openai\\nimport chainlit as cl\\n\\nfrom llama_index.core import (\\n    Settings,\\n    StorageContext,\\n    VectorStoreIndex,\\n    SimpleDirectoryReader,\\n    load_index_from_storage,\\n)\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.embeddings.openai import OpenAIEmbedding\\nfrom llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\\nfrom llama_index.core.callbacks import CallbackManager\\nfrom llama_index.core.service_context import ServiceContext\\n\\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\\n\\ntry:\\n    # rebuild storage context\\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\\n    # load index\\n    index = load_index_from_storage(storage_context)\\nexcept:\\n    documents = SimpleDirectoryReader(\"./data\").load_data(show_progress=True)\\n    index = VectorStoreIndex.from_documents(documents)\\n    index.storage_context.persist()\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    Settings.llm = OpenAI(\\n        model=\"gpt-3.5-turbo\", temperature=0.1, max_tokens=1024, streaming=True\\n    )\\n    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\\n    Settings.context_window = 4096\\n\\n    service_context = ServiceContext.from_defaults(callback_manager=CallbackManager([cl.LlamaIndexCallbackHandler()]))\\n    query_engine = index.as_query_engine(streaming=True, similarity_top_k=2, service_context=service_context)\\n    cl.user_session.set(\"query_engine\", query_engine)\\n\\n    await cl.Message(\\n        author=\"Assistant\", content=\"Hello! Im an AI assistant. How may I help you?\"\\n    ).send()\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    query_engine = cl.user_session.get(\"query_engine\") # type: RetrieverQueryEngine\\n\\n    msg = cl.Message(content=\"\", author=\"Assistant\")\\n\\n    res = await cl.make_async(query_engine.query)(message.content)\\n\\n    for token in res.response_gen:\\n        await msg.stream_token(token)\\n    await msg.send()\\n```\\n\\nThis code sets up an instance of `RetrieverQueryEngine` for each chat session. The `RetrieverQueryEngine` is invoked everytime a user sends a message to generate the response.\\n\\nThe callback handlers are responsible for listening to the intermediate steps and sending them to the UI.\\n\\n## Step 4: Launch the Application\\n\\nTo kick off your LLM app, open a terminal, navigate to the directory containing `app.py`, and run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag enables auto-reloading so that you don\\'t have to restart the server each time you modify your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n\\n\\n================================================\\nFile: integrations/message-based.mdx\\n================================================\\n---\\ntitle: vLLM, LMStudio, HuggingFace\\n---\\n\\nWe can leverage the OpenAI instrumentation to log calls from inference servers that use messages-based API, such as vLLM, LMStudio or HuggingFace\\'s TGI. \\n\\n<Warning>\\n  You shouldn\\'t configure this integration if you\\'re already using another integration like Haystack, LangChain or LlamaIndex. Both integrations would record the same generation and create duplicate steps in the UI.\\n</Warning>\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\nIn `app.py`, import the necessary packages and define one function to handle messages incoming from the UI.\\n\\n```python\\nfrom openai import AsyncOpenAI\\nimport chainlit as cl\\nclient = AsyncOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\\n# Instrument the OpenAI client\\ncl.instrument_openai()\\n\\nsettings = {\\n    \"model\": \"gpt-3.5-turbo\",\\n    \"temperature\": 0,\\n    # ... more settings\\n}\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    response = await client.chat.completions.create(\\n        messages=[\\n            {\\n                \"content\": \"You are a helpful bot, you always reply in Spanish\",\\n                \"role\": \"system\"\\n            },\\n            {\\n                \"content\": message.content,\\n                \"role\": \"user\"\\n            }\\n        ],\\n        **settings\\n    )\\n    await cl.Message(content=response.choices[0].message.content).send()\\n```\\n\\nCreate a file named `.env` in the same folder as your `app.py` file. Add your OpenAI API key in the `OPENAI_API_KEY` variable. You can optionally add your [Literal AI](https://cloud.getliteral.ai) API key in the `LITERAL_API_KEY`.\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n\\n\\n================================================\\nFile: integrations/mistralai.mdx\\n================================================\\n---\\ntitle: Mistral AI\\n---\\n\\nYou will get the full generation details (prompt, completion, tokens per second...) in your Literal AI dashboard, if your project is using [Literal AI](https://docs.literalai.com).\\n\\n<Warning>\\n  You shouldn\\'t configure this integration if you\\'re already using another\\n  integration like Haystack, Langchain or LlamaIndex. Both integrations would\\n  record the same generation and create duplicate steps in the UI.\\n</Warning>\\n\\n## Prerequisites\\n\\nBefore getting started, make sure you have the following:\\n\\n- A working installation of Chainlit\\n- The Mistral AI python client package installed, `mistralai`\\n- A [Mistral AI API key](https://console.mistral.ai/api-keys/)\\n- Basic understanding of Python programming\\n\\n## Step 1: Create a Python file\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 2: Write the Application Logic\\n\\nIn `app.py`, import the necessary packages and define one function to handle messages incoming from the UI.\\n\\n```python\\nimport os\\nimport chainlit as cl\\nfrom mistralai import Mistral\\n\\n# Initialize the Mistral client\\nclient = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    response = await client.chat.complete_async(\\n        model=\"mistral-small-latest\",\\n        max_tokens=100,\\n        temperature=0.5,\\n        stream=False,\\n        # ... more setting\\n        messages=[\\n            {\\n                \"role\": \"system\",\\n                \"content\": \"You are a helpful bot, you always reply in French.\"\\n            },\\n            {\\n                \"role\": \"user\",\\n                \"content\": message.content # Content of the user message\\n            }\\n        ]\\n    )\\n    await cl.Message(content=response.choices[0].message.content).send()\\n```\\n\\n## Step 3: Fill the environment variables\\n\\nCreate a file named `.env` in the same folder as your `app.py` file. Add your Mistral AI API key in the `MISTRAL_API_KEY` variable.\\nYou can optionally add your Literal AI API key in the `LITERAL_API_KEY`.\\n\\n## Step 4: Run the Application\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n\\n\\n================================================\\nFile: integrations/openai.mdx\\n================================================\\n---\\ntitle: OpenAI\\n---\\n\\n<Note>\\n  If you are using OpenAI assistants, check out the [OpenAI\\n  Assistant](https://github.com/Chainlit/cookbook/tree/main/openai-data-analyst)\\n  example app.\\n</Note>\\n\\nThe benefits of this integration is that you can see the OpenAI API calls in a step in the UI, and you can explore them in the prompt playground.\\n\\nYou will also get the full generation details (prompt, completion, tokens per second...) in your Literal AI dashboard, if your project is using [Literal AI](https://docs.literalai.com).\\n\\nYou need to add `cl.instrument_openai()` after creating your OpenAI client.\\n\\n<Warning>\\n  You shouldn\\'t configure this integration if you\\'re already using another\\n  integration like Haystack, Langchain or LlamaIndex. Both integrations would\\n  record the same generation and create duplicate steps in the UI.\\n</Warning>\\n\\n## Prerequisites\\n\\nBefore getting started, make sure you have the following:\\n\\n- A working installation of Chainlit\\n- The OpenAI package installed\\n- An OpenAI API key\\n- Basic understanding of Python programming\\n\\n## Step 1: Create a Python file\\n\\nCreate a new Python file named `app.py` in your project directory. This file will contain the main logic for your LLM application.\\n\\n## Step 2: Write the Application Logic\\n\\nIn `app.py`, import the necessary packages and define one function to handle messages incoming from the UI.\\n\\n```python\\nfrom openai import AsyncOpenAI\\nimport chainlit as cl\\nclient = AsyncOpenAI()\\n\\n# Instrument the OpenAI client\\ncl.instrument_openai()\\n\\nsettings = {\\n    \"model\": \"gpt-3.5-turbo\",\\n    \"temperature\": 0,\\n    # ... more settings\\n}\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    response = await client.chat.completions.create(\\n        messages=[\\n            {\\n                \"content\": \"You are a helpful bot, you always reply in Spanish\",\\n                \"role\": \"system\"\\n            },\\n            {\\n                \"content\": message.content,\\n                \"role\": \"user\"\\n            }\\n        ],\\n        **settings\\n    )\\n    await cl.Message(content=response.choices[0].message.content).send()\\n```\\n\\n## Step 3: Fill the environment variables\\n\\nCreate a file named `.env` in the same folder as your `app.py` file. Add your OpenAI API key in the `OPENAI_API_KEY` variable. You can optionally add your Literal AI API key in the `LITERAL_API_KEY`.\\n\\n## Step 4: Run the Application\\n\\nTo start your app, open a terminal and navigate to the directory containing `app.py`. Then run the following command:\\n\\n```bash\\nchainlit run app.py -w\\n```\\n\\nThe `-w` flag tells Chainlit to enable auto-reloading, so you don\\'t need to restart the server every time you make changes to your application. Your chatbot UI should now be accessible at http://localhost:8000.\\n\\n\\n\\n================================================\\nFile: llmops/enterprise.mdx\\n================================================\\n---\\ntitle: Enterprise\\n---\\n\\n---\\ntitle: Enterprise\\n---\\n\\nIf your organization can\\'t use third party cloud services for data hosting, we can provide your company with a self-hostable Literal AI docker image (under commercial license).\\n\\n<Note>\\nPlease fill this [form](https://docs.google.com/forms/d/e/1FAIpQLSdPVGqfuaWSC2DfunR6cY4C7kUHl0c2W7DnhzsF9bmMxrVpkg/viewform?usp=header\\n) for enterprise support.\\n</Note>\\n\\n## Define your Literal AI Server\\n\\nOnce you are hosting your own Literal AI instance, you can point to the server for data persistence.\\nYou will need to use the `LITERAL_API_URL` environment variable.\\n\\nModify the `.env` file next to your Chainlit application.\\n\\n```shell .env\\nLITERAL_API_URL=\"https://cloud.your_literal.com\"\\n```\\n\\nAlternatively, inlined:\\n\\n```shell\\nLITERAL_API_URL=\"https://cloud.your_literal.com\" chainlit run main.py\\n```\\n\\n## Activating Data Persistence\\n\\nUsing your own Literal AI instance, you will still need to provide a valid API key to persist the data as described [here](/data-persistence/overview).\\n\\nOnce activated, your chats and elements will be stored on your own server.\\n\\n\\n\\n\\n\\n================================================\\nFile: llmops/literalai.mdx\\n================================================\\n---\\ntitle: Literal AI\\ndescription: Literal AI is a **collaborative platform** for building production-grade LLM apps.\\n---\\n<Frame caption=\"Literal AI Platform Overview\">\\n  <img src=\"/images/literal-ai-overview.png\" alt=\"Literal AI Platform\"/>\\n</Frame>\\n\\n[Literal AI](https://docs.literalai.com) is a collaborative **observability, evaluation and analytics** platform for building production-grade LLM apps. Literal AI offers multimodal logging, including vision, audio, and video.\\n\\nLiteral AI can be leveraged as a data persistence solution, allowing you to quickly enable data storage and analysis for your Chainlit app without setting up your own infrastructure.\\n\\n<Note>\\n  Literal AI has a data retention policy available [here](https://literalai.com).\\n</Note>\\n\\n## Log your conversations threads and agent runs to Literal AI\\n\\n<Steps>\\n  <Step title=\"Sign in to Literal AI\">\\n    Navigate to [Literal AI](https://cloud.getliteral.ai) and sign in.\\n  </Step>\\n  <Step title=\"Create a New Project\">\\n    You will be prompted to create a new project:\\n    <Frame caption=\"Project Creation Screen\">\\n      <img src=\"/images/create-project.png\" />\\n    </Frame>\\n  </Step>\\n  <Step title=\"Get Your API Key\">\\n    Navigate to the `Settings` page. A default API key will be generated for you\\n    <Frame caption=\"Project API Key\">\\n      <img src=\"/images/api-key.png\" />\\n    </Frame>\\n  </Step>\\n  <Step title=\"Set the API Key in your Chainlit App\">\\n\\nOnce you have an API key, you will need to pass it via a `LITERAL_API_KEY` environment variable.\\n\\nNext to your Chainlit application, create a `.env` file and modify it like so:\\n\\n```shell .env\\nLITERAL_API_KEY=\"your key\"\\n```\\n\\nOr inlined:\\n\\n```shell\\nLITERAL_API_KEY=\"your key\" chainlit run main.py\\n```\\n\\nOr inlined for Windows powershell:\\n\\n```shell\\n$ENV:LITERAL_API_KEY=\"your key\"; chainlit run main.py\\n```\\n\\nOnce activated, your chats and elements will be persisted on Literal AI.\\n\\n</Step>\\n</Steps>\\n\\n## Debug Mode\\n\\nYou can enable the new debug mode by adding `-d` to your `chainlit run` command. You will see a debug button below each message taking you to the trace/prompt playground.\\n\\n<Frame caption=\"Debug example\">\\n  <img src=\"/images/debug.gif\" />\\n</Frame>\\n\\n## Deactivation\\n\\nIf you wish to deactivate data persistence, simply comment out or remove the `LITERAL_API_KEY` environment variable.\\n\\n## Data privacy & security\\n\\nWe prioritize your data\\'s privacy and security. We understand how crucial the data fed into Chainlit Cloud is for your business and handle it with utmost care.\\n\\nContact us for detailed information: contact@chainlit.io\\n\\n\\n\\n\\n</documentation>\\n<cookbook>\\nDirectory structure:\\n└── chainlit-cookbook/\\n    ├── map-canvas/\\n    │   ├── README.md\\n    │   ├── app.py\\n    │   ├── chainlit.md\\n    │   ├── requirements.txt\\n    │   └── public/\\n    │       └── elements/\\n    │           └── Map.jsx\\n    └── realtime-assistant/\\n        ├── README.md\\n        ├── app.py\\n        ├── chainlit.md\\n        ├── requirements.txt\\n        ├── .env.example\\n        └── realtime/\\n            ├── __init__.py\\n            └── tools.py\\n\\n================================================\\nFile: map-canvas/README.md\\n================================================\\n---\\ntitle: \\'Chainlit Canvas\\'\\ntags: [\\'canvas\\']\\n---\\n\\n# Chainlit Canvas\\n\\nThis folder shows how to use the element side bar as a canvas\\n\\nhttps://github.com/user-attachments/assets/2acf8b46-8cf3-4908-a9e2-5e0780e0c38c\\n\\n\\n\\n================================================\\nFile: map-canvas/app.py\\n================================================\\nimport json\\nimport chainlit as cl\\nfrom anthropic import AsyncAnthropic\\n\\nSYSTEM = \"you are a helpful assistant.\"\\nMODEL_NAME = \"claude-3-5-sonnet-latest\"\\nc = AsyncAnthropic()\\n\\n\\n@cl.step(type=\"tool\")\\nasync def move_map_to(latitude: float, longitude: float):\\n    await open_map()\\n\\n    fn = cl.CopilotFunction(\\n        name=\"move-map\", args={\"latitude\": latitude, \"longitude\": longitude}\\n    )\\n    await fn.acall()\\n\\n    return \"Map moved!\"\\n\\n\\ntools = [\\n    {\\n        \"name\": \"move_map_to\",\\n        \"description\": \"Move the map to the given latitude and longitude.\",\\n        \"input_schema\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"latitude\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The latitude of the location to move the map to\",\\n                },\\n                \"longitude\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The longitude of the location to move the map to\",\\n                },\\n            },\\n            \"required\": [\"latitude\", \"longitude\"],\\n        },\\n    }\\n]\\n\\nTOOL_FUNCTIONS = {\\n    \"move_map_to\": move_map_to,\\n}\\n\\n\\nasync def call_claude(chat_messages):\\n    msg = cl.Message(content=\"\", author=\"Claude\")\\n\\n    async with c.messages.stream(\\n        max_tokens=1024,\\n        system=SYSTEM,\\n        messages=chat_messages,\\n        tools=tools,\\n        model=MODEL_NAME,\\n    ) as stream:\\n        async for text in stream.text_stream:\\n            await msg.stream_token(text)\\n\\n    await msg.send()\\n    response = await stream.get_final_message()\\n\\n    return response\\n\\n\\nasync def call_tool(tool_use):\\n    tool_name = tool_use.name\\n    tool_input = tool_use.input\\n\\n    tool_function = TOOL_FUNCTIONS.get(tool_name)\\n\\n    if tool_function:\\n        try:\\n            return await tool_function(**tool_input)\\n        except TypeError:\\n            return json.dumps({\"error\": f\"Invalid input for {tool_name}\"})\\n    else:\\n        return json.dumps({\"error\": f\"Invalid tool: {tool_name}\"})\\n\\n\\nasync def open_map():\\n    map_props = {\"latitude\": 37.7749, \"longitude\": -122.4194, \"zoom\": 12}\\n    custom_element = cl.CustomElement(name=\"Map\", props=map_props, display=\"inline\")\\n    await cl.ElementSidebar.set_title(\"canvas\")\\n    await cl.ElementSidebar.set_elements([custom_element], key=\"map-canvas\")\\n\\n\\n@cl.action_callback(\"close_map\")\\nasync def on_test_action():\\n    await cl.ElementSidebar.set_elements([])\\n\\n\\n@cl.set_starters\\nasync def set_starters():\\n    return [\\n        cl.Starter(\\n            label=\"Paris\",\\n            message=\"Show me Paris.\",\\n        ),\\n        cl.Starter(\\n            label=\"NYC\",\\n            message=\"Show me NYC.\",\\n        ),\\n        cl.Starter(\\n            label=\"Tokyo\",\\n            message=\"Show me Tokyo.\",\\n        ),\\n    ]\\n\\n\\n@cl.on_chat_start\\nasync def on_start():\\n    cl.user_session.set(\"chat_messages\", [])\\n\\n    await open_map()\\n\\n\\n@cl.on_message\\nasync def on_message(msg: cl.Message):\\n    chat_messages = cl.user_session.get(\"chat_messages\")\\n    chat_messages.append({\"role\": \"user\", \"content\": msg.content})\\n    response = await call_claude(chat_messages)\\n\\n    while response.stop_reason == \"tool_use\":\\n        tool_use = next(block for block in response.content if block.type == \"tool_use\")\\n        tool_result = await call_tool(tool_use)\\n\\n        messages = [\\n            {\"role\": \"assistant\", \"content\": response.content},\\n            {\\n                \"role\": \"user\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"tool_result\",\\n                        \"tool_use_id\": tool_use.id,\\n                        \"content\": str(tool_result),\\n                    }\\n                ],\\n            },\\n        ]\\n\\n        chat_messages.extend(messages)\\n        response = await call_claude(chat_messages)\\n\\n    final_response = next(\\n        (block.text for block in response.content if hasattr(block, \"text\")),\\n        None,\\n    )\\n\\n    chat_messages = cl.user_session.get(\"chat_messages\")\\n    chat_messages.append({\"role\": \"assistant\", \"content\": final_response})\\n\\n\\n\\n================================================\\nFile: map-canvas/chainlit.md\\n================================================\\n# Welcome to Chainlit! 🚀🤖\\n\\nHi there, Developer! 👋 We\\'re excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\\n\\n## Useful Links 🔗\\n\\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) 📚\\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! 💬\\n\\nWe can\\'t wait to see what you create with Chainlit! Happy coding! 💻😊\\n\\n## Welcome screen\\n\\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.\\n\\n\\n\\n================================================\\nFile: map-canvas/requirements.txt\\n================================================\\nchainlit=>2.4.3\\n\\n\\n================================================\\nFile: map-canvas/public/elements/Map.jsx\\n================================================\\nimport React, { useEffect, useRef } from \"react\";\\nimport { useRecoilValue } from \"recoil\";\\nimport { callFnState } from \"@chainlit/react-client\";\\nimport { Button } from \"@/components/ui/button\";\\nimport { ArrowLeft } from \"lucide-react\";\\n\\nexport default function GoogleMap() {\\n  const mapRef = useRef(null);\\n  const mapInstanceRef = useRef(null);\\n  const callFn = useRecoilValue(callFnState);\\n\\n  useEffect(() => {\\n    // Check if API is already loaded\\n    if (window.google && window.google.maps) {\\n      // Maps API already loaded, initialize map directly\\n      initializeMap();\\n      return;\\n    }\\n\\n    // Check if the script is already in the process of loading\\n    const existingScript = document.querySelector(\\n      `script[src^=\"https://maps.googleapis.com/maps/api/js\"]`\\n    );\\n\\n    if (existingScript) {\\n      // Script is loading but not ready, wait for it\\n      const originalCallback = window.initMap;\\n      window.initMap = () => {\\n        if (originalCallback) originalCallback();\\n        initializeMap();\\n      };\\n      return;\\n    }\\n\\n    // Load the script only if it\\'s not already loaded or loading\\n    const script = document.createElement(\"script\");\\n    script.src = `https://maps.googleapis.com/maps/api/js?&callback=initMap`;\\n    script.async = true;\\n    script.defer = true;\\n\\n    // Define the callback function\\n    window.initMap = initializeMap;\\n\\n    document.head.appendChild(script);\\n\\n    // Clean up\\n    return () => {\\n      // Don\\'t remove the script as other components might be using it\\n      // Just clean up our callback\\n      if (window.initMap === initializeMap) {\\n        window.initMap = null;\\n      }\\n    };\\n  }, []);\\n\\n  useEffect(() => {\\n    if (callFn?.name === \"move-map\") {\\n      const { latitude, longitude } = callFn.args;\\n      moveMapTo(latitude, longitude);\\n      callFn.callback();\\n    }\\n  }, [callFn]);\\n\\n  const initializeMap = () => {\\n    if (mapRef.current && !mapInstanceRef.current) {\\n      mapInstanceRef.current = new window.google.maps.Map(mapRef.current, {\\n        center: { lat: props.latitude, lng: props.longitude },\\n        zoom: props.zoom,\\n      });\\n    }\\n  };\\n\\n  const moveMapTo = (newLat, newLng, newZoom = 12) => {\\n    if (!mapInstanceRef.current) return false;\\n\\n    // Create a new LatLng object\\n    const newPosition = new window.google.maps.LatLng(newLat, newLng);\\n\\n    // Pan the map to the new position\\n    mapInstanceRef.current.panTo(newPosition);\\n\\n    // Update zoom if provided\\n    if (newZoom !== null) {\\n      mapInstanceRef.current.setZoom(newZoom);\\n    }\\n\\n    return true;\\n  };\\n\\n  return (\\n    <div className=\"h-full w-full relative\">\\n      <Button\\n        className=\"absolute z-10\"\\n        style={{top: \".5rem\", left: \".5rem\"}}\\n        onClick={() => callAction({ name: \"close_map\", payload: {} })}\\n        size=\"icon\"\\n      >\\n        <ArrowLeft />\\n      </Button>\\n      <div ref={mapRef} className=\"h-full w-full\" />\\n    </div>\\n  );\\n}\\n\\n\\n\\n================================================\\nFile: realtime-assistant/README.md\\n================================================\\nTitle: Realtime Assistant\\nTags: [multimodal, audio]\\n\\n# Open AI realtime API with Chainlit\\n\\nThis cookbook demonstrates how to build realtime copilots with Chainlit.\\n\\n## Key Features\\n\\n- **Realtime Python Client**: Based off https://github.com/openai/openai-realtime-api-beta\\n- **Multimodal experience**: Speak and write to the assistant at the same time\\n- **Tool calling**: Ask the assistant to perform tasks and see their output in the UI\\n- **Visual Presence**: Visual cues indicating if the assistant is listening or speaking\\n\\n\\n\\n================================================\\nFile: realtime-assistant/app.py\\n================================================\\nimport os\\nimport asyncio\\nfrom openai import AsyncOpenAI\\n\\nimport chainlit as cl\\nfrom uuid import uuid4\\nfrom chainlit.logger import logger\\n\\nfrom realtime import RealtimeClient\\nfrom realtime.tools import tools\\n\\nclient = AsyncOpenAI()\\n\\n\\nasync def setup_openai_realtime():\\n    \"\"\"Instantiate and configure the OpenAI Realtime Client\"\"\"\\n    openai_realtime = RealtimeClient(api_key=os.getenv(\"OPENAI_API_KEY\"))\\n    cl.user_session.set(\"track_id\", str(uuid4()))\\n\\n    async def handle_conversation_updated(event):\\n        item = event.get(\"item\")\\n        delta = event.get(\"delta\")\\n        \"\"\"Currently used to stream audio back to the client.\"\"\"\\n        if delta:\\n            # Only one of the following will be populated for any given event\\n            if \"audio\" in delta:\\n                audio = delta[\"audio\"]  # Int16Array, audio added\\n                await cl.context.emitter.send_audio_chunk(\\n                    cl.OutputAudioChunk(\\n                        mimeType=\"pcm16\",\\n                        data=audio,\\n                        track=cl.user_session.get(\"track_id\"),\\n                    )\\n                )\\n            if \"transcript\" in delta:\\n                transcript = delta[\"transcript\"]  # string, transcript added\\n                pass\\n            if \"arguments\" in delta:\\n                arguments = delta[\"arguments\"]  # string, function arguments added\\n                pass\\n\\n    async def handle_item_completed(item):\\n        \"\"\"Used to populate the chat context with transcription once an item is completed.\"\"\"\\n        # print(item) # TODO\\n        pass\\n\\n    async def handle_conversation_interrupt(event):\\n        \"\"\"Used to cancel the client previous audio playback.\"\"\"\\n        cl.user_session.set(\"track_id\", str(uuid4()))\\n        await cl.context.emitter.send_audio_interrupt()\\n\\n    async def handle_error(event):\\n        logger.error(event)\\n\\n    openai_realtime.on(\"conversation.updated\", handle_conversation_updated)\\n    openai_realtime.on(\"conversation.item.completed\", handle_item_completed)\\n    openai_realtime.on(\"conversation.interrupted\", handle_conversation_interrupt)\\n    openai_realtime.on(\"error\", handle_error)\\n\\n    cl.user_session.set(\"openai_realtime\", openai_realtime)\\n    coros = [\\n        openai_realtime.add_tool(tool_def, tool_handler)\\n        for tool_def, tool_handler in tools\\n    ]\\n    await asyncio.gather(*coros)\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    await cl.Message(\\n        content=\"Welcome to the Chainlit x OpenAI realtime example. Press `P` to talk!\"\\n    ).send()\\n    await setup_openai_realtime()\\n\\n\\n@cl.on_message\\nasync def on_message(message: cl.Message):\\n    openai_realtime: RealtimeClient = cl.user_session.get(\"openai_realtime\")\\n    if openai_realtime and openai_realtime.is_connected():\\n        # TODO: Try image processing with message.elements\\n        await openai_realtime.send_user_message_content(\\n            [{\"type\": \"input_text\", \"text\": message.content}]\\n        )\\n    else:\\n        await cl.Message(\\n            content=\"Please activate voice mode before sending messages!\"\\n        ).send()\\n\\n\\n@cl.on_audio_start\\nasync def on_audio_start():\\n    try:\\n        openai_realtime: RealtimeClient = cl.user_session.get(\"openai_realtime\")\\n        await openai_realtime.connect()\\n        logger.info(\"Connected to OpenAI realtime\")\\n        # TODO: might want to recreate items to restore context\\n        # openai_realtime.create_conversation_item(item)\\n        return True\\n    except Exception as e:\\n        await cl.ErrorMessage(\\n            content=f\"Failed to connect to OpenAI realtime: {e}\"\\n        ).send()\\n        return False\\n\\n\\n@cl.on_audio_chunk\\nasync def on_audio_chunk(chunk: cl.InputAudioChunk):\\n    openai_realtime: RealtimeClient = cl.user_session.get(\"openai_realtime\")\\n    if openai_realtime.is_connected():\\n        await openai_realtime.append_input_audio(chunk.data)\\n    else:\\n        logger.info(\"RealtimeClient is not connected\")\\n\\n\\n@cl.on_audio_end\\n@cl.on_chat_end\\n@cl.on_stop\\nasync def on_end():\\n    openai_realtime: RealtimeClient = cl.user_session.get(\"openai_realtime\")\\n    if openai_realtime and openai_realtime.is_connected():\\n        await openai_realtime.disconnect()\\n\\n\\n\\n================================================\\nFile: realtime-assistant/chainlit.md\\n================================================\\n# Welcome to Chainlit! 🚀🤖\\n\\nHi there, Developer! 👋 We\\'re excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\\n\\n## Useful Links 🔗\\n\\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) 📚\\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! 💬\\n\\nWe can\\'t wait to see what you create with Chainlit! Happy coding! 💻😊\\n\\n## Welcome screen\\n\\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.\\n\\n\\n\\n================================================\\nFile: realtime-assistant/requirements.txt\\n================================================\\nchainlit\\nopenai\\nyfinance\\nplotly\\nwebsockets==14.1\\n\\n\\n\\n================================================\\nFile: realtime-assistant/.env.example\\n================================================\\nOPENAI_API_KEY=your-openai-api-key\\n\\n# Optional, enable multi modal observability on https://cloud.getliteral.ai\\nLITERAL_API_KEY=your-literal-api-key\\n\\n\\n\\n================================================\\nFile: realtime-assistant/realtime/__init__.py\\n================================================\\n# Derived from https://github.com/openai/openai-realtime-console. Will integrate with Chainlit when more mature.\\n\\nimport os\\nimport asyncio\\nimport inspect\\nimport numpy as np\\nimport json\\nimport websockets\\nfrom datetime import datetime\\nfrom collections import defaultdict\\nimport base64\\n\\nfrom chainlit.logger import logger\\nfrom chainlit.config import config\\n\\n\\ndef float_to_16bit_pcm(float32_array):\\n    \"\"\"\\n    Converts a numpy array of float32 amplitude data to a numpy array in int16 format.\\n    :param float32_array: numpy array of float32\\n    :return: numpy array of int16\\n    \"\"\"\\n    int16_array = np.clip(float32_array, -1, 1) * 32767\\n    return int16_array.astype(np.int16)\\n\\n\\ndef base64_to_array_buffer(base64_string):\\n    \"\"\"\\n    Converts a base64 string to a numpy array buffer.\\n    :param base64_string: base64 encoded string\\n    :return: numpy array of uint8\\n    \"\"\"\\n    binary_data = base64.b64decode(base64_string)\\n    return np.frombuffer(binary_data, dtype=np.uint8)\\n\\n\\ndef array_buffer_to_base64(array_buffer):\\n    \"\"\"\\n    Converts a numpy array buffer to a base64 string.\\n    :param array_buffer: numpy array\\n    :return: base64 encoded string\\n    \"\"\"\\n    if array_buffer.dtype == np.float32:\\n        array_buffer = float_to_16bit_pcm(array_buffer)\\n    elif array_buffer.dtype == np.int16:\\n        array_buffer = array_buffer.tobytes()\\n    else:\\n        array_buffer = array_buffer.tobytes()\\n\\n    return base64.b64encode(array_buffer).decode(\"utf-8\")\\n\\n\\nclass RealtimeEventHandler:\\n    def __init__(self):\\n        self.event_handlers = defaultdict(list)\\n\\n    def on(self, event_name, handler):\\n        self.event_handlers[event_name].append(handler)\\n\\n    def clear_event_handlers(self):\\n        self.event_handlers = defaultdict(list)\\n\\n    def dispatch(self, event_name, event):\\n        for handler in self.event_handlers[event_name]:\\n            if inspect.iscoroutinefunction(handler):\\n                asyncio.create_task(handler(event))\\n            else:\\n                handler(event)\\n\\n    async def wait_for_next(self, event_name):\\n        future = asyncio.Future()\\n\\n        def handler(event):\\n            if not future.done():\\n                future.set_result(event)\\n\\n        self.on(event_name, handler)\\n        return await future\\n\\n\\nclass RealtimeAPI(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None):\\n        super().__init__()\\n        self.default_url = \"wss://api.openai.com/v1/realtime\"\\n        self.url = url or self.default_url\\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\\n        self.ws = None\\n\\n    def is_connected(self):\\n        return self.ws is not None\\n\\n    def log(self, *args):\\n        logger.debug(f\"[Websocket/{datetime.utcnow().isoformat()}]\", *args)\\n\\n    async def connect(self, model=\"gpt-4o-realtime-preview-2024-10-01\"):\\n        if self.is_connected():\\n            raise Exception(\"Already connected\")\\n        self.ws = await websockets.connect(\\n            f\"{self.url}?model={model}\",\\n            extra_headers={\\n                \"Authorization\": f\"Bearer {self.api_key}\",\\n                \"OpenAI-Beta\": \"realtime=v1\",\\n            },\\n        )\\n        self.log(f\"Connected to {self.url}\")\\n        asyncio.create_task(self._receive_messages())\\n\\n    async def _receive_messages(self):\\n        async for message in self.ws:\\n            event = json.loads(message)\\n            if event[\"type\"] == \"error\":\\n                logger.error(\"ERROR\", event)\\n            self.log(\"received:\", event)\\n            self.dispatch(f\"server.{event[\\'type\\']}\", event)\\n            self.dispatch(\"server.*\", event)\\n\\n    async def send(self, event_name, data=None):\\n        if not self.is_connected():\\n            raise Exception(\"RealtimeAPI is not connected\")\\n        data = data or {}\\n        if not isinstance(data, dict):\\n            raise Exception(\"data must be a dictionary\")\\n        event = {\"event_id\": self._generate_id(\"evt_\"), \"type\": event_name, **data}\\n        self.dispatch(f\"client.{event_name}\", event)\\n        self.dispatch(\"client.*\", event)\\n        self.log(\"sent:\", event)\\n        await self.ws.send(json.dumps(event))\\n\\n    def _generate_id(self, prefix):\\n        return f\"{prefix}{int(datetime.utcnow().timestamp() * 1000)}\"\\n\\n    async def disconnect(self):\\n        if self.ws:\\n            await self.ws.close()\\n            self.ws = None\\n            self.log(f\"Disconnected from {self.url}\")\\n\\n\\nclass RealtimeConversation:\\n    default_frequency = config.features.audio.sample_rate\\n\\n    EventProcessors = {\\n        \"conversation.item.created\": lambda self, event: self._process_item_created(\\n            event\\n        ),\\n        \"conversation.item.truncated\": lambda self, event: self._process_item_truncated(\\n            event\\n        ),\\n        \"conversation.item.deleted\": lambda self, event: self._process_item_deleted(\\n            event\\n        ),\\n        \"conversation.item.input_audio_transcription.completed\": lambda self,\\n        event: self._process_input_audio_transcription_completed(event),\\n        \"input_audio_buffer.speech_started\": lambda self,\\n        event: self._process_speech_started(event),\\n        \"input_audio_buffer.speech_stopped\": lambda self,\\n        event,\\n        input_audio_buffer: self._process_speech_stopped(event, input_audio_buffer),\\n        \"response.created\": lambda self, event: self._process_response_created(event),\\n        \"response.output_item.added\": lambda self,\\n        event: self._process_output_item_added(event),\\n        \"response.output_item.done\": lambda self, event: self._process_output_item_done(\\n            event\\n        ),\\n        \"response.content_part.added\": lambda self,\\n        event: self._process_content_part_added(event),\\n        \"response.audio_transcript.delta\": lambda self,\\n        event: self._process_audio_transcript_delta(event),\\n        \"response.audio.delta\": lambda self, event: self._process_audio_delta(event),\\n        \"response.text.delta\": lambda self, event: self._process_text_delta(event),\\n        \"response.function_call_arguments.delta\": lambda self,\\n        event: self._process_function_call_arguments_delta(event),\\n    }\\n\\n    def __init__(self):\\n        self.clear()\\n\\n    def clear(self):\\n        self.item_lookup = {}\\n        self.items = []\\n        self.response_lookup = {}\\n        self.responses = []\\n        self.queued_speech_items = {}\\n        self.queued_transcript_items = {}\\n        self.queued_input_audio = None\\n\\n    def queue_input_audio(self, input_audio):\\n        self.queued_input_audio = input_audio\\n\\n    def process_event(self, event, *args):\\n        event_processor = self.EventProcessors.get(event[\"type\"])\\n        if not event_processor:\\n            raise Exception(f\"Missing conversation event processor for {event[\\'type\\']}\")\\n        return event_processor(self, event, *args)\\n\\n    def get_item(self, id):\\n        return self.item_lookup.get(id)\\n\\n    def get_items(self):\\n        return self.items[:]\\n\\n    def _process_item_created(self, event):\\n        item = event[\"item\"]\\n        new_item = item.copy()\\n        if new_item[\"id\"] not in self.item_lookup:\\n            self.item_lookup[new_item[\"id\"]] = new_item\\n            self.items.append(new_item)\\n        new_item[\"formatted\"] = {\"audio\": [], \"text\": \"\", \"transcript\": \"\"}\\n        if new_item[\"id\"] in self.queued_speech_items:\\n            new_item[\"formatted\"][\"audio\"] = self.queued_speech_items[new_item[\"id\"]][\\n                \"audio\"\\n            ]\\n            del self.queued_speech_items[new_item[\"id\"]]\\n        if \"content\" in new_item:\\n            text_content = [\\n                c for c in new_item[\"content\"] if c[\"type\"] in [\"text\", \"input_text\"]\\n            ]\\n            for content in text_content:\\n                new_item[\"formatted\"][\"text\"] += content[\"text\"]\\n        if new_item[\"id\"] in self.queued_transcript_items:\\n            new_item[\"formatted\"][\"transcript\"] = self.queued_transcript_items[\\n                new_item[\"id\"]\\n            ][\"transcript\"]\\n            del self.queued_transcript_items[new_item[\"id\"]]\\n        if new_item[\"type\"] == \"message\":\\n            if new_item[\"role\"] == \"user\":\\n                new_item[\"status\"] = \"completed\"\\n                if self.queued_input_audio:\\n                    new_item[\"formatted\"][\"audio\"] = self.queued_input_audio\\n                    self.queued_input_audio = None\\n            else:\\n                new_item[\"status\"] = \"in_progress\"\\n        elif new_item[\"type\"] == \"function_call\":\\n            new_item[\"formatted\"][\"tool\"] = {\\n                \"type\": \"function\",\\n                \"name\": new_item[\"name\"],\\n                \"call_id\": new_item[\"call_id\"],\\n                \"arguments\": \"\",\\n            }\\n            new_item[\"status\"] = \"in_progress\"\\n        elif new_item[\"type\"] == \"function_call_output\":\\n            new_item[\"status\"] = \"completed\"\\n            new_item[\"formatted\"][\"output\"] = new_item[\"output\"]\\n        return new_item, None\\n\\n    def _process_item_truncated(self, event):\\n        item_id = event[\"item_id\"]\\n        audio_end_ms = event[\"audio_end_ms\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(f\\'item.truncated: Item \"{item_id}\" not found\\')\\n        end_index = (audio_end_ms * self.default_frequency) // 1000\\n        item[\"formatted\"][\"transcript\"] = \"\"\\n        item[\"formatted\"][\"audio\"] = item[\"formatted\"][\"audio\"][:end_index]\\n        return item, None\\n\\n    def _process_item_deleted(self, event):\\n        item_id = event[\"item_id\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(f\\'item.deleted: Item \"{item_id}\" not found\\')\\n        del self.item_lookup[item[\"id\"]]\\n        self.items.remove(item)\\n        return item, None\\n\\n    def _process_input_audio_transcription_completed(self, event):\\n        item_id = event[\"item_id\"]\\n        content_index = event[\"content_index\"]\\n        transcript = event[\"transcript\"]\\n        formatted_transcript = transcript or \" \"\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            self.queued_transcript_items[item_id] = {\"transcript\": formatted_transcript}\\n            return None, None\\n        item[\"content\"][content_index][\"transcript\"] = transcript\\n        item[\"formatted\"][\"transcript\"] = formatted_transcript\\n        return item, {\"transcript\": transcript}\\n\\n    def _process_speech_started(self, event):\\n        item_id = event[\"item_id\"]\\n        audio_start_ms = event[\"audio_start_ms\"]\\n        self.queued_speech_items[item_id] = {\"audio_start_ms\": audio_start_ms}\\n        return None, None\\n\\n    def _process_speech_stopped(self, event, input_audio_buffer):\\n        item_id = event[\"item_id\"]\\n        audio_end_ms = event[\"audio_end_ms\"]\\n        speech = self.queued_speech_items[item_id]\\n        speech[\"audio_end_ms\"] = audio_end_ms\\n        if input_audio_buffer:\\n            start_index = (speech[\"audio_start_ms\"] * self.default_frequency) // 1000\\n            end_index = (speech[\"audio_end_ms\"] * self.default_frequency) // 1000\\n            speech[\"audio\"] = input_audio_buffer[start_index:end_index]\\n        return None, None\\n\\n    def _process_response_created(self, event):\\n        response = event[\"response\"]\\n        if response[\"id\"] not in self.response_lookup:\\n            self.response_lookup[response[\"id\"]] = response\\n            self.responses.append(response)\\n        return None, None\\n\\n    def _process_output_item_added(self, event):\\n        response_id = event[\"response_id\"]\\n        item = event[\"item\"]\\n        response = self.response_lookup.get(response_id)\\n        if not response:\\n            raise Exception(\\n                f\\'response.output_item.added: Response \"{response_id}\" not found\\'\\n            )\\n        response[\"output\"].append(item[\"id\"])\\n        return None, None\\n\\n    def _process_output_item_done(self, event):\\n        item = event[\"item\"]\\n        if not item:\\n            raise Exception(\\'response.output_item.done: Missing \"item\"\\')\\n        found_item = self.item_lookup.get(item[\"id\"])\\n        if not found_item:\\n            raise Exception(f\\'response.output_item.done: Item \"{item[\"id\"]}\" not found\\')\\n        found_item[\"status\"] = item[\"status\"]\\n        return found_item, None\\n\\n    def _process_content_part_added(self, event):\\n        item_id = event[\"item_id\"]\\n        part = event[\"part\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(f\\'response.content_part.added: Item \"{item_id}\" not found\\')\\n        item[\"content\"].append(part)\\n        return item, None\\n\\n    def _process_audio_transcript_delta(self, event):\\n        item_id = event[\"item_id\"]\\n        content_index = event[\"content_index\"]\\n        delta = event[\"delta\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(\\n                f\\'response.audio_transcript.delta: Item \"{item_id}\" not found\\'\\n            )\\n        item[\"content\"][content_index][\"transcript\"] += delta\\n        item[\"formatted\"][\"transcript\"] += delta\\n        return item, {\"transcript\": delta}\\n\\n    def _process_audio_delta(self, event):\\n        item_id = event[\"item_id\"]\\n        content_index = event[\"content_index\"]\\n        delta = event[\"delta\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            logger.debug(f\\'response.audio.delta: Item \"{item_id}\" not found\\')\\n            return None, None\\n        array_buffer = base64_to_array_buffer(delta)\\n        append_values = array_buffer.tobytes()\\n        item[\"formatted\"][\"audio\"] += [append_values]\\n        return item, {\"audio\": append_values}\\n\\n    def _process_text_delta(self, event):\\n        item_id = event[\"item_id\"]\\n        content_index = event[\"content_index\"]\\n        delta = event[\"delta\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(f\\'response.text.delta: Item \"{item_id}\" not found\\')\\n        item[\"content\"][content_index][\"text\"] += delta\\n        item[\"formatted\"][\"text\"] += delta\\n        return item, {\"text\": delta}\\n\\n    def _process_function_call_arguments_delta(self, event):\\n        item_id = event[\"item_id\"]\\n        delta = event[\"delta\"]\\n        item = self.item_lookup.get(item_id)\\n        if not item:\\n            raise Exception(\\n                f\\'response.function_call_arguments.delta: Item \"{item_id}\" not found\\'\\n            )\\n        item[\"arguments\"] += delta\\n        item[\"formatted\"][\"tool\"][\"arguments\"] += delta\\n        return item, {\"arguments\": delta}\\n\\n\\nclass RealtimeClient(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None):\\n        super().__init__()\\n        self.default_session_config = {\\n            \"modalities\": [\"text\", \"audio\"],\\n            \"instructions\": \"System settings:\\\\nTool use: enabled.\\\\n\\\\nInstructions:\\\\n- You are an artificial intelligence agent responsible for helping test realtime voice capabilities\\\\n- Please make sure to respond with a helpful voice via audio\\\\n- Be kind, helpful, and curteous\\\\n- It is okay to ask the user questions\\\\n- Use tools and functions you have available liberally, it is part of the training apparatus\\\\n- Be open to exploration and conversation\\\\n- Remember: this is just for fun and testing!\\\\n\\\\nPersonality:\\\\n- Be upbeat and genuine\\\\n- Try speaking quickly as if excited\\\\n\",\\n            \"voice\": \"shimmer\",\\n            \"input_audio_format\": \"pcm16\",\\n            \"output_audio_format\": \"pcm16\",\\n            \"input_audio_transcription\": {\"model\": \"whisper-1\"},\\n            \"turn_detection\": {\"type\": \"server_vad\"},\\n            \"tools\": [],\\n            \"tool_choice\": \"auto\",\\n            \"temperature\": 0.8,\\n            \"max_response_output_tokens\": 4096,\\n        }\\n        self.session_config = {}\\n        self.transcription_models = [{\"model\": \"whisper-1\"}]\\n        self.default_server_vad_config = {\\n            \"type\": \"server_vad\",\\n            \"threshold\": 0.5,\\n            \"prefix_padding_ms\": 300,\\n            \"silence_duration_ms\": 200,\\n        }\\n        self.realtime = RealtimeAPI(url, api_key)\\n        self.conversation = RealtimeConversation()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n\\n    def _reset_config(self):\\n        self.session_created = False\\n        self.tools = {}\\n        self.session_config = self.default_session_config.copy()\\n        self.input_audio_buffer = bytearray()\\n        return True\\n\\n    def _add_api_event_handlers(self):\\n        self.realtime.on(\"client.*\", self._log_event)\\n        self.realtime.on(\"server.*\", self._log_event)\\n        self.realtime.on(\"server.session.created\", self._on_session_created)\\n        self.realtime.on(\"server.response.created\", self._process_event)\\n        self.realtime.on(\"server.response.output_item.added\", self._process_event)\\n        self.realtime.on(\"server.response.content_part.added\", self._process_event)\\n        self.realtime.on(\\n            \"server.input_audio_buffer.speech_started\", self._on_speech_started\\n        )\\n        self.realtime.on(\\n            \"server.input_audio_buffer.speech_stopped\", self._on_speech_stopped\\n        )\\n        self.realtime.on(\"server.conversation.item.created\", self._on_item_created)\\n        self.realtime.on(\"server.conversation.item.truncated\", self._process_event)\\n        self.realtime.on(\"server.conversation.item.deleted\", self._process_event)\\n        self.realtime.on(\\n            \"server.conversation.item.input_audio_transcription.completed\",\\n            self._process_event,\\n        )\\n        self.realtime.on(\"server.response.audio_transcript.delta\", self._process_event)\\n        self.realtime.on(\"server.response.audio.delta\", self._process_event)\\n        self.realtime.on(\"server.response.text.delta\", self._process_event)\\n        self.realtime.on(\\n            \"server.response.function_call_arguments.delta\", self._process_event\\n        )\\n        self.realtime.on(\"server.response.output_item.done\", self._on_output_item_done)\\n\\n    def _log_event(self, event):\\n        realtime_event = {\\n            \"time\": datetime.utcnow().isoformat(),\\n            \"source\": \"client\" if event[\"type\"].startswith(\"client.\") else \"server\",\\n            \"event\": event,\\n        }\\n        self.dispatch(\"realtime.event\", realtime_event)\\n\\n    def _on_session_created(self, event):\\n        self.session_created = True\\n\\n    def _process_event(self, event, *args):\\n        item, delta = self.conversation.process_event(event, *args)\\n        if item:\\n            self.dispatch(\"conversation.updated\", {\"item\": item, \"delta\": delta})\\n        return item, delta\\n\\n    def _on_speech_started(self, event):\\n        self._process_event(event)\\n        self.dispatch(\"conversation.interrupted\", event)\\n\\n    def _on_speech_stopped(self, event):\\n        self._process_event(event, self.input_audio_buffer)\\n\\n    def _on_item_created(self, event):\\n        item, delta = self._process_event(event)\\n        self.dispatch(\"conversation.item.appended\", {\"item\": item})\\n        if item and item[\"status\"] == \"completed\":\\n            self.dispatch(\"conversation.item.completed\", {\"item\": item})\\n\\n    async def _on_output_item_done(self, event):\\n        item, delta = self._process_event(event)\\n        if item and item[\"status\"] == \"completed\":\\n            self.dispatch(\"conversation.item.completed\", {\"item\": item})\\n        if item and item.get(\"formatted\", {}).get(\"tool\"):\\n            await self._call_tool(item[\"formatted\"][\"tool\"])\\n\\n    async def _call_tool(self, tool):\\n        try:\\n            json_arguments = json.loads(tool[\"arguments\"])\\n            tool_config = self.tools.get(tool[\"name\"])\\n            if not tool_config:\\n                raise Exception(f\\'Tool \"{tool[\"name\"]}\" has not been added\\')\\n            result = await tool_config[\"handler\"](**json_arguments)\\n            await self.realtime.send(\\n                \"conversation.item.create\",\\n                {\\n                    \"item\": {\\n                        \"type\": \"function_call_output\",\\n                        \"call_id\": tool[\"call_id\"],\\n                        \"output\": json.dumps(result),\\n                    }\\n                },\\n            )\\n        except Exception as e:\\n            logger.error(f\"Tool call error: {json.dumps({\\'error\\': str(e)})}\")\\n            await self.realtime.send(\\n                \"conversation.item.create\",\\n                {\\n                    \"item\": {\\n                        \"type\": \"function_call_output\",\\n                        \"call_id\": tool[\"call_id\"],\\n                        \"output\": json.dumps({\"error\": str(e)}),\\n                    }\\n                },\\n            )\\n        await self.create_response()\\n\\n    def is_connected(self):\\n        return self.realtime.is_connected()\\n\\n    def reset(self):\\n        self.disconnect()\\n        self.realtime.clear_event_handlers()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n        return True\\n\\n    async def connect(self):\\n        if self.is_connected():\\n            raise Exception(\"Already connected, use .disconnect() first\")\\n        await self.realtime.connect()\\n        await self.update_session()\\n        return True\\n\\n    async def wait_for_session_created(self):\\n        if not self.is_connected():\\n            raise Exception(\"Not connected, use .connect() first\")\\n        while not self.session_created:\\n            await asyncio.sleep(0.001)\\n        return True\\n\\n    async def disconnect(self):\\n        self.session_created = False\\n        self.conversation.clear()\\n        if self.realtime.is_connected():\\n            await self.realtime.disconnect()\\n\\n    def get_turn_detection_type(self):\\n        return self.session_config.get(\"turn_detection\", {}).get(\"type\")\\n\\n    async def add_tool(self, definition, handler):\\n        if not definition.get(\"name\"):\\n            raise Exception(\"Missing tool name in definition\")\\n        name = definition[\"name\"]\\n        if name in self.tools:\\n            raise Exception(\\n                f\\'Tool \"{name}\" already added. Please use .removeTool(\"{name}\") before trying to add again.\\'\\n            )\\n        if not callable(handler):\\n            raise Exception(f\\'Tool \"{name}\" handler must be a function\\')\\n        self.tools[name] = {\"definition\": definition, \"handler\": handler}\\n        await self.update_session()\\n        return self.tools[name]\\n\\n    def remove_tool(self, name):\\n        if name not in self.tools:\\n            raise Exception(f\\'Tool \"{name}\" does not exist, can not be removed.\\')\\n        del self.tools[name]\\n        return True\\n\\n    async def delete_item(self, id):\\n        await self.realtime.send(\"conversation.item.delete\", {\"item_id\": id})\\n        return True\\n\\n    async def update_session(self, **kwargs):\\n        self.session_config.update(kwargs)\\n        use_tools = [\\n            {**tool_definition, \"type\": \"function\"}\\n            for tool_definition in self.session_config.get(\"tools\", [])\\n        ] + [\\n            {**self.tools[key][\"definition\"], \"type\": \"function\"} for key in self.tools\\n        ]\\n        session = {**self.session_config, \"tools\": use_tools}\\n        if self.realtime.is_connected():\\n            await self.realtime.send(\"session.update\", {\"session\": session})\\n        return True\\n\\n    async def create_conversation_item(self, item):\\n        await self.realtime.send(\"conversation.item.create\", {\"item\": item})\\n\\n    async def send_user_message_content(self, content=[]):\\n        if content:\\n            for c in content:\\n                if c[\"type\"] == \"input_audio\":\\n                    if isinstance(c[\"audio\"], (bytes, bytearray)):\\n                        c[\"audio\"] = array_buffer_to_base64(c[\"audio\"])\\n            await self.realtime.send(\\n                \"conversation.item.create\",\\n                {\\n                    \"item\": {\\n                        \"type\": \"message\",\\n                        \"role\": \"user\",\\n                        \"content\": content,\\n                    }\\n                },\\n            )\\n        await self.create_response()\\n        return True\\n\\n    async def append_input_audio(self, array_buffer):\\n        if len(array_buffer) > 0:\\n            await self.realtime.send(\\n                \"input_audio_buffer.append\",\\n                {\\n                    \"audio\": array_buffer_to_base64(np.array(array_buffer)),\\n                },\\n            )\\n            self.input_audio_buffer.extend(array_buffer)\\n        return True\\n\\n    async def create_response(self):\\n        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:\\n            await self.realtime.send(\"input_audio_buffer.commit\")\\n            self.conversation.queue_input_audio(self.input_audio_buffer)\\n            self.input_audio_buffer = bytearray()\\n        await self.realtime.send(\"response.create\")\\n        return True\\n\\n    async def cancel_response(self, id=None, sample_count=0):\\n        if not id:\\n            await self.realtime.send(\"response.cancel\")\\n            return {\"item\": None}\\n        else:\\n            item = self.conversation.get_item(id)\\n            if not item:\\n                raise Exception(f\\'Could not find item \"{id}\"\\')\\n            if item[\"type\"] != \"message\":\\n                raise Exception(\\'Can only cancelResponse messages with type \"message\"\\')\\n            if item[\"role\"] != \"assistant\":\\n                raise Exception(\\n                    \\'Can only cancelResponse messages with role \"assistant\"\\'\\n                )\\n            await self.realtime.send(\"response.cancel\")\\n            audio_index = next(\\n                (i for i, c in enumerate(item[\"content\"]) if c[\"type\"] == \"audio\"), -1\\n            )\\n            if audio_index == -1:\\n                raise Exception(\"Could not find audio on item to cancel\")\\n            await self.realtime.send(\\n                \"conversation.item.truncate\",\\n                {\\n                    \"item_id\": id,\\n                    \"content_index\": audio_index,\\n                    \"audio_end_ms\": int(\\n                        (sample_count / self.conversation.default_frequency) * 1000\\n                    ),\\n                },\\n            )\\n            return {\"item\": item}\\n\\n    async def wait_for_next_item(self):\\n        event = await self.wait_for_next(\"conversation.item.appended\")\\n        return {\"item\": event[\"item\"]}\\n\\n    async def wait_for_next_completed_item(self):\\n        event = await self.wait_for_next(\"conversation.item.completed\")\\n        return {\"item\": event[\"item\"]}\\n\\n\\n\\n================================================\\nFile: realtime-assistant/realtime/tools.py\\n================================================\\nimport yfinance as yf\\nimport chainlit as cl\\nimport plotly\\n\\nquery_stock_price_def = {\\n    \"name\": \"query_stock_price\",\\n    \"description\": \"Queries the latest stock price information for a given stock symbol.\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"symbol\": {\\n                \"type\": \"string\",\\n                \"description\": \"The stock symbol to query (e.g., \\'AAPL\\' for Apple Inc.)\",\\n            },\\n            \"period\": {\\n                \"type\": \"string\",\\n                \"description\": \"The time period for which to retrieve stock data (e.g., \\'1d\\' for one day, \\'1mo\\' for one month)\",\\n            },\\n        },\\n        \"required\": [\"symbol\", \"period\"],\\n    },\\n}\\n\\n\\nasync def query_stock_price_handler(symbol, period):\\n    \"\"\"\\n    Queries the latest stock price information for a given stock symbol.\\n    \"\"\"\\n    try:\\n        stock = yf.Ticker(symbol)\\n        hist = stock.history(period=period)\\n        if hist.empty:\\n            return {\"error\": \"No data found for the given symbol.\"}\\n        return hist.to_json()\\n\\n    except Exception as e:\\n        return {\"error\": str(e)}\\n\\n\\nquery_stock_price = (query_stock_price_def, query_stock_price_handler)\\n\\ndraw_plotly_chart_def = {\\n    \"name\": \"draw_plotly_chart\",\\n    \"description\": \"Draws a Plotly chart based on the provided JSON figure and displays it with an accompanying message.\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"message\": {\\n                \"type\": \"string\",\\n                \"description\": \"The message to display alongside the chart\",\\n            },\\n            \"plotly_json_fig\": {\\n                \"type\": \"string\",\\n                \"description\": \"A JSON string representing the Plotly figure to be drawn\",\\n            },\\n        },\\n        \"required\": [\"message\", \"plotly_json_fig\"],\\n    },\\n}\\n\\n\\nasync def draw_plotly_chart_handler(message: str, plotly_json_fig):\\n    fig = plotly.io.from_json(plotly_json_fig)\\n    elements = [cl.Plotly(name=\"chart\", figure=fig, display=\"inline\")]\\n\\n    await cl.Message(content=message, elements=elements).send()\\n\\n\\ndraw_plotly_chart = (draw_plotly_chart_def, draw_plotly_chart_handler)\\n\\n\\ntools = [query_stock_price, draw_plotly_chart]\\n\\n\\n\\n</cookbook>\\n\\n')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "완성된 시스템 프롬프트 길이: 364,823 문자\n"
          ]
        }
      ],
      "source": [
        "print(f\"완성된 시스템 프롬프트 길이: {len(formatted_system_prompt.text):,} 문자\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델: gemini-2.0-flash\n",
            "==================================================\n",
            "\n",
            " 전체 시스템 프롬프트: 100,081 토큰\n",
            "\n",
            "==================================================\n",
            "토큰 분석 요약:\n",
            "- 총합: 100,081 토큰\n"
          ]
        }
      ],
      "source": [
        "# 컨텍스트 토큰 수 계산\n",
        "if api_key:\n",
        "    model_name = rag_prompt['settings']['model']  # gemini-2.0-flash\n",
        "    \n",
        "    print(f\"모델: {model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 전체 시스템 프롬프트\n",
        "    total_tokens = client.models.count_tokens(model=model_name, contents=[formatted_system_prompt.text])\n",
        "    print(f\"\\n 전체 시스템 프롬프트: {total_tokens.total_tokens:,} 토큰\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"토큰 분석 요약:\")\n",
        "    print(f\"- 총합: {total_tokens.total_tokens:,} 토큰\")\n",
        "else:\n",
        "    print(\"API 키가 없어서 토큰 계산을 수행할 수 없습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
